{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation Contents Welcome to the comprehensive documentation section. Here you'll find detailed guides, tutorials, and best practices for various DevOps tools and technologies. CI/CD GitHub Actions with SonarQube & Harbor A comprehensive guide on setting up a modern CI/CD pipeline using GitHub Actions, integrated with SonarQube for code quality analysis and Harbor for container registry. Includes security best practices and troubleshooting tips. Infrastructure Ansible Guide Complete guide to Ansible automation, covering installation, configuration, and practical use cases. Learn how to automate server preparation, package management, and integrate with GitHub Actions for automated deployments. Networking Networking Overview Introduction to networking concepts and tools essential for DevOps practices. Covers basic network configuration, troubleshooting, and security considerations. Miscellaneous General Overview Additional resources and guides that don't fit into specific categories but are valuable for DevOps practices. Quick Links by Topic Version Control & CI/CD GitHub Actions Integration SonarQube Code Analysis Harbor Registry Setup Infrastructure Management Ansible Installation Server Preparation Package Management Environment Variables Security UFW Firewall Management SSL/TLS with Certbot Secrets Management Troubleshooting Ansible Debugging CI/CD Pipeline Issues Latest Updates Recently Added March 2025 : Added comprehensive Ansible guide with GitHub Actions integration March 2025 : Updated GitHub Actions guide with latest 2025 best practices March 2025 : Enhanced documentation with improved navigation and dark theme Contributing Found something that needs updating? All documentation pages have an \"Edit\" button at the top right. Feel free to submit improvements or report issues through our GitHub repository.","title":"Documentation Contents"},{"location":"#documentation-contents","text":"Welcome to the comprehensive documentation section. Here you'll find detailed guides, tutorials, and best practices for various DevOps tools and technologies.","title":"Documentation Contents"},{"location":"#cicd","text":"","title":"CI/CD"},{"location":"#github-actions-with-sonarqube-harbor","text":"A comprehensive guide on setting up a modern CI/CD pipeline using GitHub Actions, integrated with SonarQube for code quality analysis and Harbor for container registry. Includes security best practices and troubleshooting tips.","title":"GitHub Actions with SonarQube &amp; Harbor"},{"location":"#infrastructure","text":"","title":"Infrastructure"},{"location":"#ansible-guide","text":"Complete guide to Ansible automation, covering installation, configuration, and practical use cases. Learn how to automate server preparation, package management, and integrate with GitHub Actions for automated deployments.","title":"Ansible Guide"},{"location":"#networking","text":"","title":"Networking"},{"location":"#networking-overview","text":"Introduction to networking concepts and tools essential for DevOps practices. Covers basic network configuration, troubleshooting, and security considerations.","title":"Networking Overview"},{"location":"#miscellaneous","text":"","title":"Miscellaneous"},{"location":"#general-overview","text":"Additional resources and guides that don't fit into specific categories but are valuable for DevOps practices.","title":"General Overview"},{"location":"#quick-links-by-topic","text":"","title":"Quick Links by Topic"},{"location":"#version-control-cicd","text":"GitHub Actions Integration SonarQube Code Analysis Harbor Registry Setup","title":"Version Control &amp; CI/CD"},{"location":"#infrastructure-management","text":"Ansible Installation Server Preparation Package Management Environment Variables","title":"Infrastructure Management"},{"location":"#security","text":"UFW Firewall Management SSL/TLS with Certbot Secrets Management","title":"Security"},{"location":"#troubleshooting","text":"Ansible Debugging CI/CD Pipeline Issues","title":"Troubleshooting"},{"location":"#latest-updates","text":"Recently Added March 2025 : Added comprehensive Ansible guide with GitHub Actions integration March 2025 : Updated GitHub Actions guide with latest 2025 best practices March 2025 : Enhanced documentation with improved navigation and dark theme","title":"Latest Updates"},{"location":"#contributing","text":"Found something that needs updating? All documentation pages have an \"Edit\" button at the top right. Feel free to submit improvements or report issues through our GitHub repository.","title":"Contributing"},{"location":"about/","text":"About Me Welcome I'm a DevOps Architect and Systems Administrator based in New Delhi, India, with a strong background in Kubernetes, CI/CD, and infrastructure management. My expertise lies in building and maintaining scalable, secure, and automated cloud-native environments. This documentation site serves as both a personal knowledge base and a resource for others in the DevOps community. Professional Focus %%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '16px', 'fontFamily': 'arial' }, 'securityLevel': 'loose', 'zoom': 1.5 }}%% graph TD A[DevOps] --> B[Containers] A --> C[Infrastructure] A --> D[Automation] A --> E[Security] A --> F[Monitoring] B --> B1[Kubernetes] B --> B2[Docker] B --> B3[Container Security] C --> C1[High Availability] C --> C2[Distributed Storage] C --> C3[Load Balancing] D --> D1[CI/CD Pipelines] D --> D2[IaC] D --> D3[GitOps] E --> E1[SSO Integration] E --> E2[Certificate Management] E --> E3[Access Control] F --> F1[Prometheus] F --> F2[Grafana] F --> F3[UptimeKuma] style A fill:#f9f,stroke:#333,stroke-width:3px,text-align:center,width:200px,height:60px style B fill:#bbf,stroke:#333,stroke-width:2px,width:180px style C fill:#bbf,stroke:#333,stroke-width:2px,width:180px style D fill:#bbf,stroke:#333,stroke-width:2px,width:180px style E fill:#bbf,stroke:#333,stroke-width:2px,width:180px style F fill:#bbf,stroke:#333,stroke-width:2px,width:180px Core Competencies Infrastructure & Orchestration DevOps & Automation Security & Networking \u26f4\ufe0f Kubernetes Cluster Management High Availability setups Custom resource management Multi-environment deployments \ud83d\udd04 Container Orchestration Docker containerization Harbor private registry Image security scanning \ud83d\udcbe Storage Solutions Distributed storage (Longhorn) HA database clusters Backup management \ud83d\udd04 CI/CD Implementation GitHub Actions ArgoCD Automated deployments \ud83d\udee0\ufe0f Infrastructure as Code Ansible automation Configuration management Version control \ud83d\udd0d Monitoring & Logging Prometheus metrics Grafana dashboards Log aggregation \ud83d\udd12 Security Management SSL/TLS implementation SSO integration Access control \ud83c\udf10 Network Administration Traefik ingress control DNS management Cloudflare integration What I Do Infrastructure Automation : Expertise in tools like Terraform, Ansible, and CloudFormation Container Orchestration : Working with Docker and Kubernetes CI/CD Implementation : Setting up and optimizing deployment pipelines Cloud Architecture : Experience with major cloud providers (AWS, GCP, Azure) Current Projects HomeLab Environment A sophisticated home laboratory environment featuring: K3s Kubernetes cluster (1 master, 2 workers) Self-hosted services: Documentation server Nextcloud Bitwarden Jellyfin media server Secured with: Cloudflare tunnels SSO authentication Custom domain management Technical Stack Primary Technologies Category Technologies Container Platforms Kubernetes, Docker, Proxmox CI/CD Tools GitHub Actions, ArgoCD, Harbor Infrastructure Ansible, Terraform Databases PostgreSQL, MariaDB, Redis Monitoring Grafana, Prometheus, UptimeKuma Web Servers Nginx, Traefik Programming Python, Bash Version Control Git, GitHub Cloud & Hosting Experience Digital Ocean AWS Hetzner Traditional hosting (Plesk, cPanel) Areas of Interest I'm particularly passionate about: \ud83c\udfd7\ufe0f Building scalable infrastructure \ud83d\udd10 Implementing security best practices \ud83d\udcda Documentation and knowledge sharing \ud83e\udd1d Mentoring and team collaboration \ud83d\udd04 Process automation and optimization Purpose of This Site This documentation site aims to: Share knowledge and best practices in DevOps Provide practical guides and tutorials Document common solutions to technical challenges Create a reliable reference for DevOps tools and practices","title":"About"},{"location":"about/#about-me","text":"Welcome I'm a DevOps Architect and Systems Administrator based in New Delhi, India, with a strong background in Kubernetes, CI/CD, and infrastructure management. My expertise lies in building and maintaining scalable, secure, and automated cloud-native environments. This documentation site serves as both a personal knowledge base and a resource for others in the DevOps community.","title":"About Me"},{"location":"about/#professional-focus","text":"%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '16px', 'fontFamily': 'arial' }, 'securityLevel': 'loose', 'zoom': 1.5 }}%% graph TD A[DevOps] --> B[Containers] A --> C[Infrastructure] A --> D[Automation] A --> E[Security] A --> F[Monitoring] B --> B1[Kubernetes] B --> B2[Docker] B --> B3[Container Security] C --> C1[High Availability] C --> C2[Distributed Storage] C --> C3[Load Balancing] D --> D1[CI/CD Pipelines] D --> D2[IaC] D --> D3[GitOps] E --> E1[SSO Integration] E --> E2[Certificate Management] E --> E3[Access Control] F --> F1[Prometheus] F --> F2[Grafana] F --> F3[UptimeKuma] style A fill:#f9f,stroke:#333,stroke-width:3px,text-align:center,width:200px,height:60px style B fill:#bbf,stroke:#333,stroke-width:2px,width:180px style C fill:#bbf,stroke:#333,stroke-width:2px,width:180px style D fill:#bbf,stroke:#333,stroke-width:2px,width:180px style E fill:#bbf,stroke:#333,stroke-width:2px,width:180px style F fill:#bbf,stroke:#333,stroke-width:2px,width:180px","title":"Professional Focus"},{"location":"about/#core-competencies","text":"Infrastructure & Orchestration DevOps & Automation Security & Networking \u26f4\ufe0f Kubernetes Cluster Management High Availability setups Custom resource management Multi-environment deployments \ud83d\udd04 Container Orchestration Docker containerization Harbor private registry Image security scanning \ud83d\udcbe Storage Solutions Distributed storage (Longhorn) HA database clusters Backup management \ud83d\udd04 CI/CD Implementation GitHub Actions ArgoCD Automated deployments \ud83d\udee0\ufe0f Infrastructure as Code Ansible automation Configuration management Version control \ud83d\udd0d Monitoring & Logging Prometheus metrics Grafana dashboards Log aggregation \ud83d\udd12 Security Management SSL/TLS implementation SSO integration Access control \ud83c\udf10 Network Administration Traefik ingress control DNS management Cloudflare integration","title":"Core Competencies"},{"location":"about/#what-i-do","text":"Infrastructure Automation : Expertise in tools like Terraform, Ansible, and CloudFormation Container Orchestration : Working with Docker and Kubernetes CI/CD Implementation : Setting up and optimizing deployment pipelines Cloud Architecture : Experience with major cloud providers (AWS, GCP, Azure)","title":"What I Do"},{"location":"about/#current-projects","text":"HomeLab Environment A sophisticated home laboratory environment featuring: K3s Kubernetes cluster (1 master, 2 workers) Self-hosted services: Documentation server Nextcloud Bitwarden Jellyfin media server Secured with: Cloudflare tunnels SSO authentication Custom domain management","title":"Current Projects"},{"location":"about/#technical-stack","text":"","title":"Technical Stack"},{"location":"about/#primary-technologies","text":"Category Technologies Container Platforms Kubernetes, Docker, Proxmox CI/CD Tools GitHub Actions, ArgoCD, Harbor Infrastructure Ansible, Terraform Databases PostgreSQL, MariaDB, Redis Monitoring Grafana, Prometheus, UptimeKuma Web Servers Nginx, Traefik Programming Python, Bash Version Control Git, GitHub","title":"Primary Technologies"},{"location":"about/#cloud-hosting-experience","text":"Digital Ocean AWS Hetzner Traditional hosting (Plesk, cPanel)","title":"Cloud &amp; Hosting Experience"},{"location":"about/#areas-of-interest","text":"I'm particularly passionate about: \ud83c\udfd7\ufe0f Building scalable infrastructure \ud83d\udd10 Implementing security best practices \ud83d\udcda Documentation and knowledge sharing \ud83e\udd1d Mentoring and team collaboration \ud83d\udd04 Process automation and optimization","title":"Areas of Interest"},{"location":"about/#purpose-of-this-site","text":"This documentation site aims to: Share knowledge and best practices in DevOps Provide practical guides and tutorials Document common solutions to technical challenges Create a reliable reference for DevOps tools and practices","title":"Purpose of This Site"},{"location":"argocd-setup/","text":"ArgoCD Setup Guide for Kubernetes Overview Guide Information Difficulty : Intermediate Time Required : ~45 minutes Last Updated : March 2024 ArgoCD Version : v2.14.3 Kubernetes Compatibility : K3s v1.32.2+k3s1, K8s 1.24+ OS : Debian 12 This guide provides comprehensive instructions for setting up ArgoCD in a Kubernetes or K3s cluster, configuring Traefik ingress, and securing it with cert-manager for automatic SSL certificate renewal. What is ArgoCD? ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It automates the deployment of applications to Kubernetes clusters by monitoring Git repositories and applying changes when they occur. graph LR A[Git Repository] -->|Contains manifests| B[ArgoCD] B -->|Syncs to| C[Kubernetes Cluster] D[Developers] -->|Push changes| A E[ArgoCD UI/CLI] -->|Manage| B style B fill:#f9f,stroke:#333,stroke-width:2px Prerequisites Requirements A running Kubernetes or K3s cluster kubectl installed and configured helm v3.x installed A domain name for ArgoCD access DNS record pointing to your cluster's IP Administrative access to your cluster Installation Steps 1. Prepare Your Cluster For Standard Kubernetes For K3s # Verify cluster access kubectl cluster-info # Create namespace for ArgoCD kubectl create namespace argocd # Verify K3s is running sudo systemctl status k3s # Create namespace for ArgoCD kubectl create namespace argocd 2. Install ArgoCD There are two methods to install ArgoCD: using manifests directly or using Helm. Using Manifests Using Helm # Apply the ArgoCD installation manifest kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.14.3/manifests/install.yaml # Verify pods are running kubectl get pods -n argocd # Add ArgoCD Helm repository helm repo add argo https://argoproj.github.io/argo-helm helm repo update # Install ArgoCD helm install argocd argo/argo-cd \\ --namespace argocd \\ --create-namespace \\ --version 6 .7.0 \\ --set server.extraArgs = \"{--insecure}\" \\ --set controller.metrics.enabled = true \\ --set server.metrics.enabled = true Resource Requirements ArgoCD is relatively lightweight, but for production use, consider allocating: - At least 2 CPU cores and 4GB RAM for the cluster - 1GB RAM for the ArgoCD controller - 512MB RAM for the ArgoCD server 3. Install cert-manager cert-manager is required to automatically provision and manage TLS certificates. # Add Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io helm repo update # Install cert-manager with CRDs helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.13.2 \\ --set installCRDs = true # Verify cert-manager pods are running kubectl get pods -n cert-manager 4. Configure ClusterIssuer for Let's Encrypt Create a ClusterIssuer to obtain certificates from Let's Encrypt: letsencrypt-issuer.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : server : https://acme-v02.api.letsencrypt.org/directory email : your-email@example.com # Replace with your email privateKeySecretRef : name : letsencrypt-prod solvers : - http01 : ingress : class : traefik Apply the ClusterIssuer: # Apply the ClusterIssuer kubectl apply -f letsencrypt-issuer.yaml # Verify the ClusterIssuer is ready kubectl get clusterissuer letsencrypt-prod -o wide Rate Limits Let's Encrypt has rate limits: 50 certificates per domain per week. Use the staging server ( https://acme-staging-v02.api.letsencrypt.org/directory ) for testing. 5. Configure Traefik Ingress for ArgoCD Create an Ingress resource for ArgoCD: argocd-ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : argocd-server-ingress namespace : argocd annotations : cert-manager.io/cluster-issuer : \"letsencrypt-prod\" traefik.ingress.kubernetes.io/router.entrypoints : \"websecure\" traefik.ingress.kubernetes.io/router.tls : \"true\" traefik.ingress.kubernetes.io/router.middlewares : \"argocd-argocd-middleware@kubernetescrd\" spec : ingressClassName : traefik tls : - hosts : - argocd.example.com # Replace with your domain secretName : argocd-server-tls rules : - host : argocd.example.com # Replace with your domain http : paths : - path : / pathType : Prefix backend : service : name : argocd-server port : number : 80 Create a middleware to handle gRPC and HTTP traffic: argocd-middleware.yaml 1 2 3 4 5 6 7 8 9 apiVersion : traefik.containo.us/v1alpha1 kind : Middleware metadata : name : argocd-middleware namespace : argocd spec : headers : customRequestHeaders : X-Forwarded-Proto : \"https\" Apply the configurations: # Apply the middleware kubectl apply -f argocd-middleware.yaml # Apply the ingress kubectl apply -f argocd-ingress.yaml # Check the status of the ingress kubectl get ingress -n argocd Accessing ArgoCD Initial Login Once ArgoCD is installed and the ingress is configured, you can access it via your domain (e.g., https://argocd.example.com ). Get Initial Password Login via CLI # For manifest installation kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath = \"{.data.password}\" | base64 -d # For Helm installation (if using default values) kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath = \"{.data.password}\" | base64 -d # Install ArgoCD CLI curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/download/v2.14.3/argocd-linux-amd64 sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd rm argocd-linux-amd64 # Login using CLI argocd login argocd.example.com # Change the default password argocd account update-password Security Note Always change the default admin password immediately after the first login! Setting Up Your First Application After logging in, you can deploy your first application: Click on \"+ New App\" in the UI Fill in the application details: Name: example-app Project: default Sync Policy: Automatic Repository URL: Your Git repository URL Path: Path to your Kubernetes manifests Cluster: https://kubernetes.default.svc (for in-cluster deployment) Namespace: Your target namespace Security Hardening Security Best Practices RBAC Configuration : Limit access to ArgoCD SSO Integration : Connect to your identity provider Network Policies : Restrict pod communication Secrets Management : Use external secret stores Configure RBAC Create a custom RBAC policy: argocd-rbac-cm.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : ConfigMap metadata : name : argocd-rbac-cm namespace : argocd data : policy.csv : | p, role:readonly, applications, get, */*, allow p, role:readonly, clusters, get, *, allow p, role:developer, applications, create, */*, allow p, role:developer, applications, update, */*, allow p, role:developer, applications, delete, */*, allow g, developer@example.com, role:developer g, viewer@example.com, role:readonly policy.default : role:readonly Apply the ConfigMap: kubectl apply -f argocd-rbac-cm.yaml Configure SSO (GitHub Example) Update the ArgoCD ConfigMap: argocd-cm.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd data : url : https://argocd.example.com dex.config : | connectors: - type: github id: github name: GitHub config: clientID: your-github-client-id clientSecret: $dex.github.clientSecret orgs: - name: your-github-org Create a secret for GitHub OAuth: kubectl -n argocd create secret generic github-secret \\ --from-literal = clientSecret = your-github-client-secret Apply the ConfigMap: kubectl apply -f argocd-cm.yaml Network Policies Restrict network traffic to ArgoCD: argocd-network-policy.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : argocd-server-network-policy namespace : argocd spec : podSelector : matchLabels : app.kubernetes.io/name : argocd-server policyTypes : - Ingress ingress : - from : - namespaceSelector : {} ports : - protocol : TCP port : 80 - protocol : TCP port : 443 Apply the network policy: kubectl apply -f argocd-network-policy.yaml Troubleshooting Common Issues and Solutions Common Problems Ingress Not Working Certificate Issues ArgoCD Server Crashes Symptoms : Unable to access ArgoCD through the domain Solutions : 1. Check if the certificate is issued correctly: kubectl get certificate -n argocd 2. Verify Traefik is properly configured: kubectl get ingressroute -A 3. Check the Traefik logs: kubectl logs -n kube-system -l app.kubernetes.io/name = traefik Symptoms : SSL errors or certificate not issuing Solutions : 1. Check cert-manager logs: kubectl logs -n cert-manager -l app = cert-manager 2. Verify the ClusterIssuer status: kubectl describe clusterissuer letsencrypt-prod 3. Check certificate request status: kubectl get certificaterequest -n argocd Symptoms : ArgoCD UI unavailable, server pods restarting Solutions : 1. Check server logs: kubectl logs -n argocd -l app.kubernetes.io/name = argocd-server 2. Verify resource allocation: kubectl top pods -n argocd 3. Check for eviction events: kubectl get events -n argocd Diagnostic Commands Here are some useful commands for diagnosing issues: Diagnostic Commands # Check all ArgoCD components kubectl get pods -n argocd # Check ArgoCD server logs kubectl logs -n argocd -l app.kubernetes.io/name = argocd-server # Check ArgoCD application controller logs kubectl logs -n argocd -l app.kubernetes.io/name = argocd-application-controller # Check certificate status kubectl get certificate -n argocd # Check ingress status kubectl describe ingress argocd-server-ingress -n argocd Maintenance Upgrading ArgoCD Using Manifests Using Helm # Update to a new version kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.14.3/manifests/install.yaml # Update Helm repositories helm repo update # Upgrade ArgoCD helm upgrade argocd argo/argo-cd \\ --namespace argocd \\ --version 6 .7.0 Backup Before Upgrading Always backup your ArgoCD settings before upgrading: kubectl get -n argocd -o yaml configmap,secret,application > argocd-backup.yaml Monitoring ArgoCD ArgoCD exposes Prometheus metrics that can be scraped for monitoring: argocd-prometheus-servicemonitor.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-metrics namespace : monitoring spec : selector : matchLabels : app.kubernetes.io/name : argocd-metrics endpoints : - port : metrics Advanced Configuration GitOps Workflow Example sequenceDiagram participant Dev as Developer participant Git as Git Repository participant CI as CI Pipeline participant Argo as ArgoCD participant K8s as Kubernetes Dev->>Git: Push code changes Git->>CI: Trigger CI pipeline CI->>Git: Update manifests Git->>Argo: Detect changes Argo->>K8s: Apply changes K8s->>Argo: Report status Argo->>Git: Update deployment status Multi-Cluster Setup For managing multiple clusters with ArgoCD: Register the external cluster: argocd cluster add context-name Create applications targeting the external cluster: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : multi-cluster-app namespace : argocd spec : destination : namespace : default server : https://external-cluster-api-url project : default source : path : path/to/manifests repoURL : https://github.com/your-org/your-repo.git targetRevision : HEAD syncPolicy : automated : prune : true selfHeal : true Conclusion You now have a fully functional ArgoCD setup with: Secure access via HTTPS Automatic certificate management Traefik ingress integration Basic security hardening Next Steps Configure notifications Set up project templates Integrate with your CI pipeline Explore ApplicationSets for multi-cluster management References ArgoCD Documentation cert-manager Documentation Traefik Documentation Kubernetes Documentation","title":"ArgoCD Setup"},{"location":"argocd-setup/#argocd-setup-guide-for-kubernetes","text":"","title":"ArgoCD Setup Guide for Kubernetes"},{"location":"argocd-setup/#overview","text":"Guide Information Difficulty : Intermediate Time Required : ~45 minutes Last Updated : March 2024 ArgoCD Version : v2.14.3 Kubernetes Compatibility : K3s v1.32.2+k3s1, K8s 1.24+ OS : Debian 12 This guide provides comprehensive instructions for setting up ArgoCD in a Kubernetes or K3s cluster, configuring Traefik ingress, and securing it with cert-manager for automatic SSL certificate renewal.","title":"Overview"},{"location":"argocd-setup/#what-is-argocd","text":"ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It automates the deployment of applications to Kubernetes clusters by monitoring Git repositories and applying changes when they occur. graph LR A[Git Repository] -->|Contains manifests| B[ArgoCD] B -->|Syncs to| C[Kubernetes Cluster] D[Developers] -->|Push changes| A E[ArgoCD UI/CLI] -->|Manage| B style B fill:#f9f,stroke:#333,stroke-width:2px","title":"What is ArgoCD?"},{"location":"argocd-setup/#prerequisites","text":"Requirements A running Kubernetes or K3s cluster kubectl installed and configured helm v3.x installed A domain name for ArgoCD access DNS record pointing to your cluster's IP Administrative access to your cluster","title":"Prerequisites"},{"location":"argocd-setup/#installation-steps","text":"","title":"Installation Steps"},{"location":"argocd-setup/#1-prepare-your-cluster","text":"For Standard Kubernetes For K3s # Verify cluster access kubectl cluster-info # Create namespace for ArgoCD kubectl create namespace argocd # Verify K3s is running sudo systemctl status k3s # Create namespace for ArgoCD kubectl create namespace argocd","title":"1. Prepare Your Cluster"},{"location":"argocd-setup/#2-install-argocd","text":"There are two methods to install ArgoCD: using manifests directly or using Helm. Using Manifests Using Helm # Apply the ArgoCD installation manifest kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.14.3/manifests/install.yaml # Verify pods are running kubectl get pods -n argocd # Add ArgoCD Helm repository helm repo add argo https://argoproj.github.io/argo-helm helm repo update # Install ArgoCD helm install argocd argo/argo-cd \\ --namespace argocd \\ --create-namespace \\ --version 6 .7.0 \\ --set server.extraArgs = \"{--insecure}\" \\ --set controller.metrics.enabled = true \\ --set server.metrics.enabled = true Resource Requirements ArgoCD is relatively lightweight, but for production use, consider allocating: - At least 2 CPU cores and 4GB RAM for the cluster - 1GB RAM for the ArgoCD controller - 512MB RAM for the ArgoCD server","title":"2. Install ArgoCD"},{"location":"argocd-setup/#3-install-cert-manager","text":"cert-manager is required to automatically provision and manage TLS certificates. # Add Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io helm repo update # Install cert-manager with CRDs helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.13.2 \\ --set installCRDs = true # Verify cert-manager pods are running kubectl get pods -n cert-manager","title":"3. Install cert-manager"},{"location":"argocd-setup/#4-configure-clusterissuer-for-lets-encrypt","text":"Create a ClusterIssuer to obtain certificates from Let's Encrypt: letsencrypt-issuer.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : server : https://acme-v02.api.letsencrypt.org/directory email : your-email@example.com # Replace with your email privateKeySecretRef : name : letsencrypt-prod solvers : - http01 : ingress : class : traefik Apply the ClusterIssuer: # Apply the ClusterIssuer kubectl apply -f letsencrypt-issuer.yaml # Verify the ClusterIssuer is ready kubectl get clusterissuer letsencrypt-prod -o wide Rate Limits Let's Encrypt has rate limits: 50 certificates per domain per week. Use the staging server ( https://acme-staging-v02.api.letsencrypt.org/directory ) for testing.","title":"4. Configure ClusterIssuer for Let's Encrypt"},{"location":"argocd-setup/#5-configure-traefik-ingress-for-argocd","text":"Create an Ingress resource for ArgoCD: argocd-ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : argocd-server-ingress namespace : argocd annotations : cert-manager.io/cluster-issuer : \"letsencrypt-prod\" traefik.ingress.kubernetes.io/router.entrypoints : \"websecure\" traefik.ingress.kubernetes.io/router.tls : \"true\" traefik.ingress.kubernetes.io/router.middlewares : \"argocd-argocd-middleware@kubernetescrd\" spec : ingressClassName : traefik tls : - hosts : - argocd.example.com # Replace with your domain secretName : argocd-server-tls rules : - host : argocd.example.com # Replace with your domain http : paths : - path : / pathType : Prefix backend : service : name : argocd-server port : number : 80 Create a middleware to handle gRPC and HTTP traffic: argocd-middleware.yaml 1 2 3 4 5 6 7 8 9 apiVersion : traefik.containo.us/v1alpha1 kind : Middleware metadata : name : argocd-middleware namespace : argocd spec : headers : customRequestHeaders : X-Forwarded-Proto : \"https\" Apply the configurations: # Apply the middleware kubectl apply -f argocd-middleware.yaml # Apply the ingress kubectl apply -f argocd-ingress.yaml # Check the status of the ingress kubectl get ingress -n argocd","title":"5. Configure Traefik Ingress for ArgoCD"},{"location":"argocd-setup/#accessing-argocd","text":"","title":"Accessing ArgoCD"},{"location":"argocd-setup/#initial-login","text":"Once ArgoCD is installed and the ingress is configured, you can access it via your domain (e.g., https://argocd.example.com ). Get Initial Password Login via CLI # For manifest installation kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath = \"{.data.password}\" | base64 -d # For Helm installation (if using default values) kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath = \"{.data.password}\" | base64 -d # Install ArgoCD CLI curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/download/v2.14.3/argocd-linux-amd64 sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd rm argocd-linux-amd64 # Login using CLI argocd login argocd.example.com # Change the default password argocd account update-password Security Note Always change the default admin password immediately after the first login!","title":"Initial Login"},{"location":"argocd-setup/#setting-up-your-first-application","text":"After logging in, you can deploy your first application: Click on \"+ New App\" in the UI Fill in the application details: Name: example-app Project: default Sync Policy: Automatic Repository URL: Your Git repository URL Path: Path to your Kubernetes manifests Cluster: https://kubernetes.default.svc (for in-cluster deployment) Namespace: Your target namespace","title":"Setting Up Your First Application"},{"location":"argocd-setup/#security-hardening","text":"Security Best Practices RBAC Configuration : Limit access to ArgoCD SSO Integration : Connect to your identity provider Network Policies : Restrict pod communication Secrets Management : Use external secret stores","title":"Security Hardening"},{"location":"argocd-setup/#configure-rbac","text":"Create a custom RBAC policy: argocd-rbac-cm.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : ConfigMap metadata : name : argocd-rbac-cm namespace : argocd data : policy.csv : | p, role:readonly, applications, get, */*, allow p, role:readonly, clusters, get, *, allow p, role:developer, applications, create, */*, allow p, role:developer, applications, update, */*, allow p, role:developer, applications, delete, */*, allow g, developer@example.com, role:developer g, viewer@example.com, role:readonly policy.default : role:readonly Apply the ConfigMap: kubectl apply -f argocd-rbac-cm.yaml","title":"Configure RBAC"},{"location":"argocd-setup/#configure-sso-github-example","text":"Update the ArgoCD ConfigMap: argocd-cm.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd data : url : https://argocd.example.com dex.config : | connectors: - type: github id: github name: GitHub config: clientID: your-github-client-id clientSecret: $dex.github.clientSecret orgs: - name: your-github-org Create a secret for GitHub OAuth: kubectl -n argocd create secret generic github-secret \\ --from-literal = clientSecret = your-github-client-secret Apply the ConfigMap: kubectl apply -f argocd-cm.yaml","title":"Configure SSO (GitHub Example)"},{"location":"argocd-setup/#network-policies","text":"Restrict network traffic to ArgoCD: argocd-network-policy.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : argocd-server-network-policy namespace : argocd spec : podSelector : matchLabels : app.kubernetes.io/name : argocd-server policyTypes : - Ingress ingress : - from : - namespaceSelector : {} ports : - protocol : TCP port : 80 - protocol : TCP port : 443 Apply the network policy: kubectl apply -f argocd-network-policy.yaml","title":"Network Policies"},{"location":"argocd-setup/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"argocd-setup/#common-issues-and-solutions","text":"Common Problems Ingress Not Working Certificate Issues ArgoCD Server Crashes Symptoms : Unable to access ArgoCD through the domain Solutions : 1. Check if the certificate is issued correctly: kubectl get certificate -n argocd 2. Verify Traefik is properly configured: kubectl get ingressroute -A 3. Check the Traefik logs: kubectl logs -n kube-system -l app.kubernetes.io/name = traefik Symptoms : SSL errors or certificate not issuing Solutions : 1. Check cert-manager logs: kubectl logs -n cert-manager -l app = cert-manager 2. Verify the ClusterIssuer status: kubectl describe clusterissuer letsencrypt-prod 3. Check certificate request status: kubectl get certificaterequest -n argocd Symptoms : ArgoCD UI unavailable, server pods restarting Solutions : 1. Check server logs: kubectl logs -n argocd -l app.kubernetes.io/name = argocd-server 2. Verify resource allocation: kubectl top pods -n argocd 3. Check for eviction events: kubectl get events -n argocd","title":"Common Issues and Solutions"},{"location":"argocd-setup/#diagnostic-commands","text":"Here are some useful commands for diagnosing issues: Diagnostic Commands # Check all ArgoCD components kubectl get pods -n argocd # Check ArgoCD server logs kubectl logs -n argocd -l app.kubernetes.io/name = argocd-server # Check ArgoCD application controller logs kubectl logs -n argocd -l app.kubernetes.io/name = argocd-application-controller # Check certificate status kubectl get certificate -n argocd # Check ingress status kubectl describe ingress argocd-server-ingress -n argocd","title":"Diagnostic Commands"},{"location":"argocd-setup/#maintenance","text":"","title":"Maintenance"},{"location":"argocd-setup/#upgrading-argocd","text":"Using Manifests Using Helm # Update to a new version kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.14.3/manifests/install.yaml # Update Helm repositories helm repo update # Upgrade ArgoCD helm upgrade argocd argo/argo-cd \\ --namespace argocd \\ --version 6 .7.0 Backup Before Upgrading Always backup your ArgoCD settings before upgrading: kubectl get -n argocd -o yaml configmap,secret,application > argocd-backup.yaml","title":"Upgrading ArgoCD"},{"location":"argocd-setup/#monitoring-argocd","text":"ArgoCD exposes Prometheus metrics that can be scraped for monitoring: argocd-prometheus-servicemonitor.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-metrics namespace : monitoring spec : selector : matchLabels : app.kubernetes.io/name : argocd-metrics endpoints : - port : metrics","title":"Monitoring ArgoCD"},{"location":"argocd-setup/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"argocd-setup/#gitops-workflow-example","text":"sequenceDiagram participant Dev as Developer participant Git as Git Repository participant CI as CI Pipeline participant Argo as ArgoCD participant K8s as Kubernetes Dev->>Git: Push code changes Git->>CI: Trigger CI pipeline CI->>Git: Update manifests Git->>Argo: Detect changes Argo->>K8s: Apply changes K8s->>Argo: Report status Argo->>Git: Update deployment status","title":"GitOps Workflow Example"},{"location":"argocd-setup/#multi-cluster-setup","text":"For managing multiple clusters with ArgoCD: Register the external cluster: argocd cluster add context-name Create applications targeting the external cluster: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : multi-cluster-app namespace : argocd spec : destination : namespace : default server : https://external-cluster-api-url project : default source : path : path/to/manifests repoURL : https://github.com/your-org/your-repo.git targetRevision : HEAD syncPolicy : automated : prune : true selfHeal : true","title":"Multi-Cluster Setup"},{"location":"argocd-setup/#conclusion","text":"You now have a fully functional ArgoCD setup with: Secure access via HTTPS Automatic certificate management Traefik ingress integration Basic security hardening Next Steps Configure notifications Set up project templates Integrate with your CI pipeline Explore ApplicationSets for multi-cluster management","title":"Conclusion"},{"location":"argocd-setup/#references","text":"ArgoCD Documentation cert-manager Documentation Traefik Documentation Kubernetes Documentation","title":"References"},{"location":"contact/","text":"Get in Touch Connect With Me I'm always interested in connecting with fellow DevOps practitioners, developers, and technology enthusiasts. Feel free to reach out through any of the following channels: Social Media & Messaging GitHub : github.com/depgod Telegram : @yourusername Discord : Join our server Matrix : @yourusername:matrix.org XMPP : yourusername@xmpp.org Looking forward to connecting with you!","title":"Contact"},{"location":"contact/#get-in-touch","text":"","title":"Get in Touch"},{"location":"contact/#connect-with-me","text":"I'm always interested in connecting with fellow DevOps practitioners, developers, and technology enthusiasts. Feel free to reach out through any of the following channels:","title":"Connect With Me"},{"location":"contact/#social-media-messaging","text":"GitHub : github.com/depgod Telegram : @yourusername Discord : Join our server Matrix : @yourusername:matrix.org XMPP : yourusername@xmpp.org Looking forward to connecting with you!","title":"Social Media &amp; Messaging"},{"location":"k3s-ha-cluster/","text":"High Availability K3s Cluster Setup Guide Overview Guide Information Difficulty : Advanced Time Required : ~1 hour Last Updated : March 2024 K3s Version : v1.32.2+k3s1 Longhorn Version : v1.6.0 Architecture Overview graph TD subgraph \"Control Plane\" A[Master Node<br/>etcd + K3s Server] end subgraph \"Worker Nodes\" B[Worker Node 1<br/>K3s Agent] C[Worker Node 2<br/>K3s Agent] D[Worker Node 3<br/>K3s Agent] end subgraph \"Storage Layer\" E[Longhorn<br/>Distributed Block Storage] end A --> B A --> C A --> D B --> E C --> E D --> E style A fill:#f9f,stroke:#333 style E fill:#bbf,stroke:#333 Prerequisites System Requirements Master Node Worker Nodes Network Requirements 2 CPU cores 4GB RAM 40GB disk space Ubuntu 22.04 LTS Static IP address 2 CPU cores 8GB RAM 100GB disk space Ubuntu 22.04 LTS Static IP addresses All nodes must have: Unrestricted connectivity between nodes Internet access for package installation Firewall ports open: TCP/6443 (K3s API) TCP/2379-2380 (etcd) UDP/8472 (VXLAN) TCP/10250 (kubelet) Server Preparation Important Execute these steps on ALL nodes unless specified otherwise. System Updates # Update package list and upgrade system sudo apt update && sudo apt upgrade -y # Install required packages sudo apt install -y \\ curl \\ gnupg \\ nfs-common \\ open-iscsi \\ jq \\ logrotate Configure Log Rotation Create a logrotate configuration for K3s: /etc/logrotate.d/k3s 1 2 3 4 5 6 7 8 9 10 11 12 13 /var/log/k3s/*.log { daily rotate 7 compress delaycompress missingok notifempty create 0640 root root postrotate systemctl restart k3s-server 2 >/dev/null || true systemctl restart k3s-agent 2 >/dev/null || true endscript } System Configuration System Settings 1 2 3 4 5 6 7 8 9 10 11 # Enable and start open-iscsi for Longhorn sudo systemctl enable --now iscsid # Configure sysctl settings cat << EOF | sudo tee /etc/sysctl.d/k3s.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.max_map_count = 262144 EOF sudo sysctl --system K3s Installation Master Node Setup Master Node Only Run these commands ONLY on the master node. Install K3s Server # Download K3s installation script curl -sfL https://get.k3s.io > k3s-install.sh # Install K3s server with HA etcd and VXLAN sudo INSTALL_K3S_VERSION = \"v1.33.0+k3s1\" bash k3s-install.sh server \\ --cluster-init \\ --flannel-backend = vxlan \\ --disable traefik \\ --disable servicelb \\ --disable local-storage \\ --tls-san $( hostname -f ) \\ --write-kubeconfig-mode 644 # Get node token for workers sudo cat /var/lib/rancher/k3s/server/node-token Worker Nodes Setup Worker Nodes Only Replace MASTER_IP and NODE_TOKEN with your actual values. Install K3s Agent # Download K3s installation script curl -sfL https://get.k3s.io > k3s-install.sh # Install K3s agent sudo INSTALL_K3S_VERSION = \"v1.33.0+k3s1\" K3S_URL = \"https://MASTER_IP:6443\" \\ K3S_TOKEN = \"NODE_TOKEN\" bash k3s-install.sh agent Verify Cluster Status Check Nodes Check Pods Check etcd Health kubectl get nodes -o wide kubectl get pods -A kubectl -n kube-system exec -it etcd-master -- etcdctl endpoint health Longhorn Installation Prerequisites Check Run this on all nodes to verify Longhorn requirements: Verify Requirements curl -sSfL https://raw.githubusercontent.com/longhorn/longhorn/v1.6.0/scripts/environment_check.sh | bash Install Longhorn Installation Steps Execute these commands on the master node. Deploy Longhorn # Add Longhorn Helm repository helm repo add longhorn https://charts.longhorn.io helm repo update # Install Longhorn helm install longhorn longhorn/longhorn \\ --namespace longhorn-system \\ --create-namespace \\ --version 1 .6.0 \\ --set defaultSettings.defaultDataPath = \"/var/lib/longhorn\" \\ --set defaultSettings.guaranteedEngineManagerCPU = 5 \\ --set defaultSettings.guaranteedReplicaManagerCPU = 5 Verify Longhorn Installation Check Pods Check StorageClass Access Dashboard kubectl -n longhorn-system get pods kubectl get sc # Port forward Longhorn UI kubectl -n longhorn-system port-forward svc/longhorn-frontend 8000 :80 Access via: http://localhost:8000 Troubleshooting Common Issues Known Problems and Solutions Node Not Ready etcd Issues Longhorn Volume Issues Check K3s service status: sudo systemctl status k3s View K3s logs: sudo journalctl -u k3s Check etcd cluster health: sudo k3s etcd-snapshot ls Verify etcd member list: kubectl -n kube-system exec -it etcd-master -- etcdctl member list Check volume status: kubectl -n longhorn-system get volumes View instance manager logs: kubectl -n longhorn-system logs -l app = longhorn-manager Maintenance Backup Procedures etcd Backup Longhorn Backup # Create etcd snapshot sudo k3s etcd-snapshot save --name etcd-backup- $( date +%Y%m%d ) # Create backup settings kubectl -n longhorn-system apply -f - <<EOF apiVersion: longhorn.io/v1beta1 kind: BackupTarget metadata: name: default spec: backupTargetURL: s3://your-bucket-name@region/path credentialSecret: aws-secret EOF Monitoring Setup Monitoring Stack Consider installing: - Prometheus for metrics collection - Grafana for visualization - Alertmanager for notifications Security Recommendations Network Policies apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny-all spec : podSelector : {} policyTypes : - Ingress - Egress Pod Security Standards apiVersion : constraints.gatekeeper.sh/v1beta1 kind : K8sPSPPrivilegedContainer metadata : name : no-privileged-containers spec : enforcementAction : deny Next Steps [ ] Configure external load balancer [ ] Set up monitoring and logging [ ] Implement backup strategy [ ] Configure disaster recovery [ ] Set up CI/CD pipelines Need Help? If you encounter any issues: - Check the K3s documentation - Visit the Longhorn documentation - Join the K3s Slack channel","title":"HA K3s Cluster"},{"location":"k3s-ha-cluster/#high-availability-k3s-cluster-setup-guide","text":"","title":"High Availability K3s Cluster Setup Guide"},{"location":"k3s-ha-cluster/#overview","text":"Guide Information Difficulty : Advanced Time Required : ~1 hour Last Updated : March 2024 K3s Version : v1.32.2+k3s1 Longhorn Version : v1.6.0","title":"Overview"},{"location":"k3s-ha-cluster/#architecture-overview","text":"graph TD subgraph \"Control Plane\" A[Master Node<br/>etcd + K3s Server] end subgraph \"Worker Nodes\" B[Worker Node 1<br/>K3s Agent] C[Worker Node 2<br/>K3s Agent] D[Worker Node 3<br/>K3s Agent] end subgraph \"Storage Layer\" E[Longhorn<br/>Distributed Block Storage] end A --> B A --> C A --> D B --> E C --> E D --> E style A fill:#f9f,stroke:#333 style E fill:#bbf,stroke:#333","title":"Architecture Overview"},{"location":"k3s-ha-cluster/#prerequisites","text":"System Requirements Master Node Worker Nodes Network Requirements 2 CPU cores 4GB RAM 40GB disk space Ubuntu 22.04 LTS Static IP address 2 CPU cores 8GB RAM 100GB disk space Ubuntu 22.04 LTS Static IP addresses All nodes must have: Unrestricted connectivity between nodes Internet access for package installation Firewall ports open: TCP/6443 (K3s API) TCP/2379-2380 (etcd) UDP/8472 (VXLAN) TCP/10250 (kubelet)","title":"Prerequisites"},{"location":"k3s-ha-cluster/#server-preparation","text":"Important Execute these steps on ALL nodes unless specified otherwise.","title":"Server Preparation"},{"location":"k3s-ha-cluster/#system-updates","text":"# Update package list and upgrade system sudo apt update && sudo apt upgrade -y # Install required packages sudo apt install -y \\ curl \\ gnupg \\ nfs-common \\ open-iscsi \\ jq \\ logrotate","title":"System Updates"},{"location":"k3s-ha-cluster/#configure-log-rotation","text":"Create a logrotate configuration for K3s: /etc/logrotate.d/k3s 1 2 3 4 5 6 7 8 9 10 11 12 13 /var/log/k3s/*.log { daily rotate 7 compress delaycompress missingok notifempty create 0640 root root postrotate systemctl restart k3s-server 2 >/dev/null || true systemctl restart k3s-agent 2 >/dev/null || true endscript }","title":"Configure Log Rotation"},{"location":"k3s-ha-cluster/#system-configuration","text":"System Settings 1 2 3 4 5 6 7 8 9 10 11 # Enable and start open-iscsi for Longhorn sudo systemctl enable --now iscsid # Configure sysctl settings cat << EOF | sudo tee /etc/sysctl.d/k3s.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.max_map_count = 262144 EOF sudo sysctl --system","title":"System Configuration"},{"location":"k3s-ha-cluster/#k3s-installation","text":"","title":"K3s Installation"},{"location":"k3s-ha-cluster/#master-node-setup","text":"Master Node Only Run these commands ONLY on the master node. Install K3s Server # Download K3s installation script curl -sfL https://get.k3s.io > k3s-install.sh # Install K3s server with HA etcd and VXLAN sudo INSTALL_K3S_VERSION = \"v1.33.0+k3s1\" bash k3s-install.sh server \\ --cluster-init \\ --flannel-backend = vxlan \\ --disable traefik \\ --disable servicelb \\ --disable local-storage \\ --tls-san $( hostname -f ) \\ --write-kubeconfig-mode 644 # Get node token for workers sudo cat /var/lib/rancher/k3s/server/node-token","title":"Master Node Setup"},{"location":"k3s-ha-cluster/#worker-nodes-setup","text":"Worker Nodes Only Replace MASTER_IP and NODE_TOKEN with your actual values. Install K3s Agent # Download K3s installation script curl -sfL https://get.k3s.io > k3s-install.sh # Install K3s agent sudo INSTALL_K3S_VERSION = \"v1.33.0+k3s1\" K3S_URL = \"https://MASTER_IP:6443\" \\ K3S_TOKEN = \"NODE_TOKEN\" bash k3s-install.sh agent","title":"Worker Nodes Setup"},{"location":"k3s-ha-cluster/#verify-cluster-status","text":"Check Nodes Check Pods Check etcd Health kubectl get nodes -o wide kubectl get pods -A kubectl -n kube-system exec -it etcd-master -- etcdctl endpoint health","title":"Verify Cluster Status"},{"location":"k3s-ha-cluster/#longhorn-installation","text":"","title":"Longhorn Installation"},{"location":"k3s-ha-cluster/#prerequisites-check","text":"Run this on all nodes to verify Longhorn requirements: Verify Requirements curl -sSfL https://raw.githubusercontent.com/longhorn/longhorn/v1.6.0/scripts/environment_check.sh | bash","title":"Prerequisites Check"},{"location":"k3s-ha-cluster/#install-longhorn","text":"Installation Steps Execute these commands on the master node. Deploy Longhorn # Add Longhorn Helm repository helm repo add longhorn https://charts.longhorn.io helm repo update # Install Longhorn helm install longhorn longhorn/longhorn \\ --namespace longhorn-system \\ --create-namespace \\ --version 1 .6.0 \\ --set defaultSettings.defaultDataPath = \"/var/lib/longhorn\" \\ --set defaultSettings.guaranteedEngineManagerCPU = 5 \\ --set defaultSettings.guaranteedReplicaManagerCPU = 5","title":"Install Longhorn"},{"location":"k3s-ha-cluster/#verify-longhorn-installation","text":"Check Pods Check StorageClass Access Dashboard kubectl -n longhorn-system get pods kubectl get sc # Port forward Longhorn UI kubectl -n longhorn-system port-forward svc/longhorn-frontend 8000 :80 Access via: http://localhost:8000","title":"Verify Longhorn Installation"},{"location":"k3s-ha-cluster/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"k3s-ha-cluster/#common-issues","text":"Known Problems and Solutions Node Not Ready etcd Issues Longhorn Volume Issues Check K3s service status: sudo systemctl status k3s View K3s logs: sudo journalctl -u k3s Check etcd cluster health: sudo k3s etcd-snapshot ls Verify etcd member list: kubectl -n kube-system exec -it etcd-master -- etcdctl member list Check volume status: kubectl -n longhorn-system get volumes View instance manager logs: kubectl -n longhorn-system logs -l app = longhorn-manager","title":"Common Issues"},{"location":"k3s-ha-cluster/#maintenance","text":"","title":"Maintenance"},{"location":"k3s-ha-cluster/#backup-procedures","text":"etcd Backup Longhorn Backup # Create etcd snapshot sudo k3s etcd-snapshot save --name etcd-backup- $( date +%Y%m%d ) # Create backup settings kubectl -n longhorn-system apply -f - <<EOF apiVersion: longhorn.io/v1beta1 kind: BackupTarget metadata: name: default spec: backupTargetURL: s3://your-bucket-name@region/path credentialSecret: aws-secret EOF","title":"Backup Procedures"},{"location":"k3s-ha-cluster/#monitoring-setup","text":"Monitoring Stack Consider installing: - Prometheus for metrics collection - Grafana for visualization - Alertmanager for notifications","title":"Monitoring Setup"},{"location":"k3s-ha-cluster/#security-recommendations","text":"Network Policies apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny-all spec : podSelector : {} policyTypes : - Ingress - Egress Pod Security Standards apiVersion : constraints.gatekeeper.sh/v1beta1 kind : K8sPSPPrivilegedContainer metadata : name : no-privileged-containers spec : enforcementAction : deny","title":"Security Recommendations"},{"location":"k3s-ha-cluster/#next-steps","text":"[ ] Configure external load balancer [ ] Set up monitoring and logging [ ] Implement backup strategy [ ] Configure disaster recovery [ ] Set up CI/CD pipelines Need Help? If you encounter any issues: - Check the K3s documentation - Visit the Longhorn documentation - Join the K3s Slack channel","title":"Next Steps"},{"location":"markdown_basics/","text":"Markdown Guide This comprehensive guide will help you master Markdown syntax for creating well-formatted documentation. Each section includes both the Markdown syntax and its rendered output. Basic Syntax 1. Headings Markdown provides six levels of headings, using # symbols: # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 The rendered output looks like this: Heading 1 Heading 2 Heading 3 Heading 4 Heading 5 Heading 6 2. Text Formatting Bold Text **Bold text** or __Bold text__ Bold text or Bold text Italic Text *Italic text* or _Italic text_ Italic text or Italic text Bold and Italic ***Bold and italic*** or ___Bold and italic___ Bold and italic or Bold and italic Strikethrough ~~Strikethrough text~~ ~~Strikethrough text~~ 3. Lists Unordered Lists - First item - Second item - Indented item - Another indented item - Third item First item Second item Indented item Another indented item Third item Ordered Lists 1. First item 2. Second item 1. Indented item 2. Another indented item 3. Third item First item Second item Indented item Another indented item Third item 4. Links Basic Links [ Visit GitHub ]( https://github.com ) Visit GitHub Links with Titles [ GitHub ]( https://github.com \"GitHub's Homepage\" ) GitHub Reference-style Links [ GitHub ][ 1 ] [ DevOps ][ 2 ] [ 1 ]: https://github.com [ 2 ]: https://en.wikipedia.org/wiki/DevOps GitHub DevOps 5. Images Basic Image ![ Alt text ]( https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png ) Image with Title ![ GitHub Logo ]( https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png \"GitHub Logo\" ) 6. Code Inline Code Use `git status` to list all changed files. Use git status to list all changed files. Code Blocks ```python def hello_world (): print ( \"Hello, World!\" ) ``` def hello_world (): print ( \"Hello, World!\" ) Syntax Highlighting ```javascript function greet ( name ) { console . log ( `Hello, ${ name } !` ); } ``` function greet ( name ) { console . log ( `Hello, ${ name } !` ); } 7. Tables | Left-aligned | Center-aligned | Right-aligned | |:-------------|:-------------:|-------------:| | Content | Content | Content | | Left | Center | Right | Left-aligned Center-aligned Right-aligned Content Content Content Left Center Right 8. Blockquotes Simple Blockquote > This is a blockquote This is a blockquote Nested Blockquotes > First level >> Second level >>> Third level First level Second level Third level 9. Task Lists - [x] Completed task - [ ] Incomplete task - [x] Completed subtask - [ ] Incomplete subtask [x] Completed task [ ] Incomplete task [x] Completed subtask [ ] Incomplete subtask 10. Horizontal Rules Any of these will create a horizontal rule: --- *** ___ 11. Escaping Characters Use backslash to escape special characters: \\* Not italic \\* \\` Not code \\` \\[ Not a link \\] * Not italic * ` Not code ` [ Not a link ] 12. Extended Syntax (with Material for MkDocs) Highlighting Text ==Highlighted text== ==Highlighted text== Footnotes Here's a sentence with a footnote[^1]. [ ^1 ]: This is the footnote. Here's a sentence with a footnote[^1]. [^1]: This is the footnote. Definition Lists term : definition term : definition Emoji :smile: :heart: :thumbsup: :smile: :heart: :thumbsup: Best Practices Consistency : Use consistent formatting throughout your document Spacing : Add blank lines before and after headings Headers : Use proper header hierarchy (don't skip levels) Lists : Keep them simple and nested no more than three levels Code Blocks : Always specify the language for syntax highlighting Links : Use descriptive text rather than \"click here\" Images : Always include alt text for accessibility Common Pitfalls to Avoid Forgetting to add two spaces for line breaks Incorrect nesting of lists Missing blank lines before and after lists and code blocks Improper escaping of special characters Inconsistent heading hierarchy Tools and Resources Markdown Editors : Visual Studio Code with Markdown extensions Typora StackEdit (web-based) Online Validators : MarkdownLint Dillinger Cheat Sheets : GitHub Markdown Guide Markdown Guide","title":"Markdown Guide"},{"location":"markdown_basics/#markdown-guide","text":"This comprehensive guide will help you master Markdown syntax for creating well-formatted documentation. Each section includes both the Markdown syntax and its rendered output.","title":"Markdown Guide"},{"location":"markdown_basics/#basic-syntax","text":"","title":"Basic Syntax"},{"location":"markdown_basics/#1-headings","text":"Markdown provides six levels of headings, using # symbols: # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 The rendered output looks like this:","title":"1. Headings"},{"location":"markdown_basics/#heading-1","text":"","title":"Heading 1"},{"location":"markdown_basics/#heading-2","text":"","title":"Heading 2"},{"location":"markdown_basics/#heading-3","text":"","title":"Heading 3"},{"location":"markdown_basics/#heading-4","text":"","title":"Heading 4"},{"location":"markdown_basics/#heading-5","text":"","title":"Heading 5"},{"location":"markdown_basics/#heading-6","text":"","title":"Heading 6"},{"location":"markdown_basics/#2-text-formatting","text":"","title":"2. Text Formatting"},{"location":"markdown_basics/#bold-text","text":"**Bold text** or __Bold text__ Bold text or Bold text","title":"Bold Text"},{"location":"markdown_basics/#italic-text","text":"*Italic text* or _Italic text_ Italic text or Italic text","title":"Italic Text"},{"location":"markdown_basics/#bold-and-italic","text":"***Bold and italic*** or ___Bold and italic___ Bold and italic or Bold and italic","title":"Bold and Italic"},{"location":"markdown_basics/#strikethrough","text":"~~Strikethrough text~~ ~~Strikethrough text~~","title":"Strikethrough"},{"location":"markdown_basics/#3-lists","text":"","title":"3. Lists"},{"location":"markdown_basics/#unordered-lists","text":"- First item - Second item - Indented item - Another indented item - Third item First item Second item Indented item Another indented item Third item","title":"Unordered Lists"},{"location":"markdown_basics/#ordered-lists","text":"1. First item 2. Second item 1. Indented item 2. Another indented item 3. Third item First item Second item Indented item Another indented item Third item","title":"Ordered Lists"},{"location":"markdown_basics/#4-links","text":"","title":"4. Links"},{"location":"markdown_basics/#basic-links","text":"[ Visit GitHub ]( https://github.com ) Visit GitHub","title":"Basic Links"},{"location":"markdown_basics/#links-with-titles","text":"[ GitHub ]( https://github.com \"GitHub's Homepage\" ) GitHub","title":"Links with Titles"},{"location":"markdown_basics/#reference-style-links","text":"[ GitHub ][ 1 ] [ DevOps ][ 2 ] [ 1 ]: https://github.com [ 2 ]: https://en.wikipedia.org/wiki/DevOps GitHub DevOps","title":"Reference-style Links"},{"location":"markdown_basics/#5-images","text":"","title":"5. Images"},{"location":"markdown_basics/#basic-image","text":"![ Alt text ]( https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png )","title":"Basic Image"},{"location":"markdown_basics/#image-with-title","text":"![ GitHub Logo ]( https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png \"GitHub Logo\" )","title":"Image with Title"},{"location":"markdown_basics/#6-code","text":"","title":"6. Code"},{"location":"markdown_basics/#inline-code","text":"Use `git status` to list all changed files. Use git status to list all changed files.","title":"Inline Code"},{"location":"markdown_basics/#code-blocks","text":"```python def hello_world (): print ( \"Hello, World!\" ) ``` def hello_world (): print ( \"Hello, World!\" )","title":"Code Blocks"},{"location":"markdown_basics/#syntax-highlighting","text":"```javascript function greet ( name ) { console . log ( `Hello, ${ name } !` ); } ``` function greet ( name ) { console . log ( `Hello, ${ name } !` ); }","title":"Syntax Highlighting"},{"location":"markdown_basics/#7-tables","text":"| Left-aligned | Center-aligned | Right-aligned | |:-------------|:-------------:|-------------:| | Content | Content | Content | | Left | Center | Right | Left-aligned Center-aligned Right-aligned Content Content Content Left Center Right","title":"7. Tables"},{"location":"markdown_basics/#8-blockquotes","text":"","title":"8. Blockquotes"},{"location":"markdown_basics/#simple-blockquote","text":"> This is a blockquote This is a blockquote","title":"Simple Blockquote"},{"location":"markdown_basics/#nested-blockquotes","text":"> First level >> Second level >>> Third level First level Second level Third level","title":"Nested Blockquotes"},{"location":"markdown_basics/#9-task-lists","text":"- [x] Completed task - [ ] Incomplete task - [x] Completed subtask - [ ] Incomplete subtask [x] Completed task [ ] Incomplete task [x] Completed subtask [ ] Incomplete subtask","title":"9. Task Lists"},{"location":"markdown_basics/#10-horizontal-rules","text":"Any of these will create a horizontal rule: --- *** ___","title":"10. Horizontal Rules"},{"location":"markdown_basics/#11-escaping-characters","text":"Use backslash to escape special characters: \\* Not italic \\* \\` Not code \\` \\[ Not a link \\] * Not italic * ` Not code ` [ Not a link ]","title":"11. Escaping Characters"},{"location":"markdown_basics/#12-extended-syntax-with-material-for-mkdocs","text":"","title":"12. Extended Syntax (with Material for MkDocs)"},{"location":"markdown_basics/#highlighting-text","text":"==Highlighted text== ==Highlighted text==","title":"Highlighting Text"},{"location":"markdown_basics/#footnotes","text":"Here's a sentence with a footnote[^1]. [ ^1 ]: This is the footnote. Here's a sentence with a footnote[^1]. [^1]: This is the footnote.","title":"Footnotes"},{"location":"markdown_basics/#definition-lists","text":"term : definition term : definition","title":"Definition Lists"},{"location":"markdown_basics/#emoji","text":":smile: :heart: :thumbsup: :smile: :heart: :thumbsup:","title":"Emoji"},{"location":"markdown_basics/#best-practices","text":"Consistency : Use consistent formatting throughout your document Spacing : Add blank lines before and after headings Headers : Use proper header hierarchy (don't skip levels) Lists : Keep them simple and nested no more than three levels Code Blocks : Always specify the language for syntax highlighting Links : Use descriptive text rather than \"click here\" Images : Always include alt text for accessibility","title":"Best Practices"},{"location":"markdown_basics/#common-pitfalls-to-avoid","text":"Forgetting to add two spaces for line breaks Incorrect nesting of lists Missing blank lines before and after lists and code blocks Improper escaping of special characters Inconsistent heading hierarchy","title":"Common Pitfalls to Avoid"},{"location":"markdown_basics/#tools-and-resources","text":"Markdown Editors : Visual Studio Code with Markdown extensions Typora StackEdit (web-based) Online Validators : MarkdownLint Dillinger Cheat Sheets : GitHub Markdown Guide Markdown Guide","title":"Tools and Resources"},{"location":"multipass-k8s-cluster/","text":"Local Kubernetes Cluster with Multipass Overview Guide Information Difficulty : Intermediate Time Required : ~30 minutes Last Updated : March 2024 Table of Contents Setup Multipass Provision Virtual Machines Server Preparation Configure System Settings Install Kubernetes Components Initialize Cluster Setup Network Interface Join Worker Nodes Verify Cluster Troubleshooting Cluster Maintenance Architecture graph TD A[Host Ubuntu 24.04] --> B[Multipass] B --> C[Master Node<br/>4GB RAM, 2 CPU] B --> D[Worker1<br/>4GB RAM, 2 CPU] B --> E[Worker2<br/>4GB RAM, 2 CPU] C --> F[Control Plane] F --> G[API Server] F --> H[etcd] F --> I[Controller Manager] F --> J[Scheduler] D --> K[kubelet] D --> L[containerd] E --> M[kubelet] E --> N[containerd] Setup Multipass Quick Setup Multipass provides a fast way to spin up Ubuntu VMs. It's lightweight and perfect for local Kubernetes clusters. Install Multipass Verify Installation List Available Images sudo snap install multipass multipass version multipass find Provision Virtual Machines Resource Allocation We'll create one master node and two worker nodes. Adjust the resources based on your system capabilities. Create Cluster Nodes # Create master node multipass launch --name master --cpus 2 --mem 4G --disk 20G # Create worker nodes multipass launch --name worker1 --cpus 2 --mem 4G --disk 20G multipass launch --name worker2 --cpus 2 --mem 4G --disk 20G Expected Output Launched: master Launched: worker1 Launched: worker2 Access Nodes Master Node Worker Node 1 Worker Node 2 multipass shell master multipass shell worker1 multipass shell worker2 Get Node IPs multipass list Server Preparation Important Run these commands on ALL nodes (master and workers). Update System Install Dependencies Setup Docker Repository sudo apt update && sudo apt upgrade -y sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo tee /etc/apt/keyrings/docker.asc > /dev/null echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install Containerd Install and Configure Containerd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Install containerd sudo apt update sudo apt install -y containerd.io # Enable and start containerd sudo systemctl enable containerd sudo systemctl start containerd # Generate default config sudo mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml # Modify containerd configuration sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml sudo sed -i 's/^disabled_plugins = \\[\"cri\"\\]/#disabled_plugins = \\[\"cri\"\\]/' /etc/containerd/config.toml # Restart containerd sudo systemctl restart containerd Configure System Settings Critical Step Skipping these configurations may result in cluster initialization failures. Disable Swap Load Kernel Modules Configure Sysctl sudo swapoff -a sudo sed -i '/swap/d' /etc/fstab cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF sudo sysctl --system Install Kubernetes Components Version Information This guide uses Kubernetes v1.32. Adjust version numbers as needed. Install Kubernetes Tools # Add Kubernetes repository curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list # Install required packages sudo apt update sudo apt install -y kubelet kubeadm kubectl # Prevent accidental upgrades sudo apt-mark hold kubelet kubeadm kubectl # Enable kubelet sudo systemctl enable kubelet sudo systemctl start kubelet Initialize Cluster Master Node Only Run these commands ONLY on the master node. Initialize Kubernetes Cluster # Initialize cluster sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 # Setup kubeconfig mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config Save the Join Command The initialization will output a kubeadm join command. Save this for joining worker nodes. Setup Network Interface Install Flannel CNI Verify Installation kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml kubectl get pods -n kube-flannel Join Worker Nodes Worker Nodes Only Run these commands on each worker node. Get Join Command (Master) Join Cluster (Workers) kubeadm token create --print-join-command sudo kubeadm join <master-ip>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash> Verify Cluster Check Node Status Check Pods kubectl get nodes kubectl get pods -A Troubleshooting Common Issues Node Not Ready Join Command Issues Pod Network Issues Check CNI pods: kubectl get pods -n kube-system Check kubelet status: systemctl status kubelet View kubelet logs: journalctl -xeu kubelet Generate new token: kubeadm token create Get discovery token CA cert hash: openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | \\ openssl rsa -pubin -outform der 2 >/dev/null | \\ openssl dgst -sha256 -hex | sed 's/^.* //' Check flannel pods: kubectl get pods -n kube-flannel Check flannel logs: kubectl logs -n kube-flannel <pod-name> Cluster Maintenance Backup Procedures Backup etcd sudo apt install etcd-client ETCDCTL_API = 3 etcdctl --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ snapshot save snapshot.db Scaling the Cluster To add more worker nodes: Create new VM using multipass Follow server preparation steps Join the cluster using the join command Cleanup Delete Node Delete VM # On master node kubectl drain <node-name> --ignore-daemonsets kubectl delete node <node-name> # On worker node sudo kubeadm reset multipass delete <vm-name> multipass purge Security Best Practices Keep Kubernetes version updated Use Network Policies Enable RBAC Regularly rotate certificates Monitor cluster with security tools Next Steps Deploy sample applications Setup monitoring with Prometheus and Grafana Configure persistent storage Implement high availability Need Help? If you encounter any issues, check the official Kubernetes documentation or open an issue in the repository.","title":"Multipass Local Cluster"},{"location":"multipass-k8s-cluster/#local-kubernetes-cluster-with-multipass","text":"","title":"Local Kubernetes Cluster with Multipass"},{"location":"multipass-k8s-cluster/#overview","text":"Guide Information Difficulty : Intermediate Time Required : ~30 minutes Last Updated : March 2024","title":"Overview"},{"location":"multipass-k8s-cluster/#table-of-contents","text":"Setup Multipass Provision Virtual Machines Server Preparation Configure System Settings Install Kubernetes Components Initialize Cluster Setup Network Interface Join Worker Nodes Verify Cluster Troubleshooting Cluster Maintenance","title":"Table of Contents"},{"location":"multipass-k8s-cluster/#architecture","text":"graph TD A[Host Ubuntu 24.04] --> B[Multipass] B --> C[Master Node<br/>4GB RAM, 2 CPU] B --> D[Worker1<br/>4GB RAM, 2 CPU] B --> E[Worker2<br/>4GB RAM, 2 CPU] C --> F[Control Plane] F --> G[API Server] F --> H[etcd] F --> I[Controller Manager] F --> J[Scheduler] D --> K[kubelet] D --> L[containerd] E --> M[kubelet] E --> N[containerd]","title":"Architecture"},{"location":"multipass-k8s-cluster/#setup-multipass","text":"Quick Setup Multipass provides a fast way to spin up Ubuntu VMs. It's lightweight and perfect for local Kubernetes clusters. Install Multipass Verify Installation List Available Images sudo snap install multipass multipass version multipass find","title":"Setup Multipass"},{"location":"multipass-k8s-cluster/#provision-virtual-machines","text":"Resource Allocation We'll create one master node and two worker nodes. Adjust the resources based on your system capabilities. Create Cluster Nodes # Create master node multipass launch --name master --cpus 2 --mem 4G --disk 20G # Create worker nodes multipass launch --name worker1 --cpus 2 --mem 4G --disk 20G multipass launch --name worker2 --cpus 2 --mem 4G --disk 20G Expected Output Launched: master Launched: worker1 Launched: worker2","title":"Provision Virtual Machines"},{"location":"multipass-k8s-cluster/#access-nodes","text":"Master Node Worker Node 1 Worker Node 2 multipass shell master multipass shell worker1 multipass shell worker2","title":"Access Nodes"},{"location":"multipass-k8s-cluster/#get-node-ips","text":"multipass list","title":"Get Node IPs"},{"location":"multipass-k8s-cluster/#server-preparation","text":"Important Run these commands on ALL nodes (master and workers). Update System Install Dependencies Setup Docker Repository sudo apt update && sudo apt upgrade -y sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo tee /etc/apt/keyrings/docker.asc > /dev/null echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null","title":"Server Preparation"},{"location":"multipass-k8s-cluster/#install-containerd","text":"Install and Configure Containerd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Install containerd sudo apt update sudo apt install -y containerd.io # Enable and start containerd sudo systemctl enable containerd sudo systemctl start containerd # Generate default config sudo mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml # Modify containerd configuration sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml sudo sed -i 's/^disabled_plugins = \\[\"cri\"\\]/#disabled_plugins = \\[\"cri\"\\]/' /etc/containerd/config.toml # Restart containerd sudo systemctl restart containerd","title":"Install Containerd"},{"location":"multipass-k8s-cluster/#configure-system-settings","text":"Critical Step Skipping these configurations may result in cluster initialization failures. Disable Swap Load Kernel Modules Configure Sysctl sudo swapoff -a sudo sed -i '/swap/d' /etc/fstab cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF sudo sysctl --system","title":"Configure System Settings"},{"location":"multipass-k8s-cluster/#install-kubernetes-components","text":"Version Information This guide uses Kubernetes v1.32. Adjust version numbers as needed. Install Kubernetes Tools # Add Kubernetes repository curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list # Install required packages sudo apt update sudo apt install -y kubelet kubeadm kubectl # Prevent accidental upgrades sudo apt-mark hold kubelet kubeadm kubectl # Enable kubelet sudo systemctl enable kubelet sudo systemctl start kubelet","title":"Install Kubernetes Components"},{"location":"multipass-k8s-cluster/#initialize-cluster","text":"Master Node Only Run these commands ONLY on the master node. Initialize Kubernetes Cluster # Initialize cluster sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 # Setup kubeconfig mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config Save the Join Command The initialization will output a kubeadm join command. Save this for joining worker nodes.","title":"Initialize Cluster"},{"location":"multipass-k8s-cluster/#setup-network-interface","text":"Install Flannel CNI Verify Installation kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml kubectl get pods -n kube-flannel","title":"Setup Network Interface"},{"location":"multipass-k8s-cluster/#join-worker-nodes","text":"Worker Nodes Only Run these commands on each worker node. Get Join Command (Master) Join Cluster (Workers) kubeadm token create --print-join-command sudo kubeadm join <master-ip>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>","title":"Join Worker Nodes"},{"location":"multipass-k8s-cluster/#verify-cluster","text":"Check Node Status Check Pods kubectl get nodes kubectl get pods -A","title":"Verify Cluster"},{"location":"multipass-k8s-cluster/#troubleshooting","text":"Common Issues Node Not Ready Join Command Issues Pod Network Issues Check CNI pods: kubectl get pods -n kube-system Check kubelet status: systemctl status kubelet View kubelet logs: journalctl -xeu kubelet Generate new token: kubeadm token create Get discovery token CA cert hash: openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | \\ openssl rsa -pubin -outform der 2 >/dev/null | \\ openssl dgst -sha256 -hex | sed 's/^.* //' Check flannel pods: kubectl get pods -n kube-flannel Check flannel logs: kubectl logs -n kube-flannel <pod-name>","title":"Troubleshooting"},{"location":"multipass-k8s-cluster/#cluster-maintenance","text":"","title":"Cluster Maintenance"},{"location":"multipass-k8s-cluster/#backup-procedures","text":"Backup etcd sudo apt install etcd-client ETCDCTL_API = 3 etcdctl --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ snapshot save snapshot.db","title":"Backup Procedures"},{"location":"multipass-k8s-cluster/#scaling-the-cluster","text":"To add more worker nodes: Create new VM using multipass Follow server preparation steps Join the cluster using the join command","title":"Scaling the Cluster"},{"location":"multipass-k8s-cluster/#cleanup","text":"Delete Node Delete VM # On master node kubectl drain <node-name> --ignore-daemonsets kubectl delete node <node-name> # On worker node sudo kubeadm reset multipass delete <vm-name> multipass purge","title":"Cleanup"},{"location":"multipass-k8s-cluster/#security-best-practices","text":"Keep Kubernetes version updated Use Network Policies Enable RBAC Regularly rotate certificates Monitor cluster with security tools","title":"Security Best Practices"},{"location":"multipass-k8s-cluster/#next-steps","text":"Deploy sample applications Setup monitoring with Prometheus and Grafana Configure persistent storage Implement high availability Need Help? If you encounter any issues, check the official Kubernetes documentation or open an issue in the repository.","title":"Next Steps"},{"location":"versioning/","text":"1. Git SHA (Commit Hash) Format : image:git-sha Example : myapp:a1b2c3d Pros: Precise tracking to source code Immutable and unique Easy to debug and rollback Perfect for development environments Cons: Not human-readable Difficult to determine version order No immediate indication of stability level Detailed Explanation: Git SHA versioning uses the unique hash identifier that Git generates for each commit in your repository. When you build a Docker image using this approach, you take the first few characters (usually 7-8) of the commit hash and use it as your image tag. This method creates an unbreakable link between your source code and the Docker image, making it extremely useful for debugging and traceability. For instance, if you discover an issue in production, you can immediately identify the exact code commit that produced that image. However, these hashes are not human-friendly - you can't tell at a glance which version came first or what changes it contains. This makes it less ideal for release management but perfect for development and testing environments where precise code tracking is crucial. 2. Semantic Versioning Format : image:MAJOR.MINOR.PATCH Example : myapp:1.2.3 Pros: Clear indication of change magnitude Well understood by developers Good for stable releases Easy to automate with conventional commits Cons: Can be subjective (what constitutes a breaking change?) Multiple tags might point to same image Detailed Explanation: Semantic Versioning follows a structured numbering system with three components: MAJOR.MINOR.PATCH. Each component has a specific meaning - MAJOR versions indicate breaking changes that might require users to modify their code, MINOR versions add new features while maintaining backward compatibility, and PATCH versions represent bug fixes. This system is particularly valuable when your Docker image contains an application or service that other systems depend on. Users can quickly understand the impact of upgrading to a new version. For example, if you're currently using version 1.2.3 and see version 1.2.4, you know it's safe to upgrade since it's just a patch. However, if you see version 2.0.0, you know to carefully review the changes as it contains breaking changes. The main challenge with SemVer is maintaining discipline in version number assignment - teams need to consistently agree on what constitutes a breaking change versus a minor feature addition. 3. Git Tag Based Format : image:v1.2.3 Example : myapp:v1.2.3 Pros: Direct correlation with Git releases Good for release automation Clear release history Cons: Requires disciplined tag management May need additional CI/CD configuration Can be confusing with multiple release branches Detailed Explanation: Git tag based versioning aligns your Docker image versions with your Git repository's release tags. This approach creates a natural workflow where creating a Git tag automatically triggers a new Docker image build with the same version. It's particularly powerful when combined with semantic versioning - for example, tagging a release as v1.2.3 in Git automatically produces a Docker image tagged 1.2.3. This method works exceptionally well with automated release processes and provides clear documentation of your release history. The challenge comes when managing multiple release branches or when hotfixes need to be applied to older versions. You need robust processes to handle these scenarios and ensure tags are created consistently across branches. 4. Environment Based Format : image:env-timestamp Example : myapp:prod-20250302 Pros: Clear deployment target Easy to track when image was built Good for environment-specific configurations Cons: Less precise source tracking Potential confusion with multiple deployments per day Additional storage overhead Detailed Explanation: Environment based versioning adds context about where and when an image is intended to be used. This approach often combines an environment identifier with a timestamp or build number, such as prod-20250302 or staging-build123. This strategy is particularly useful in organizations with complex deployment pipelines involving multiple environments (development, staging, QA, production). It makes it immediately clear which images are approved for which environments and when they were built. The timestamp component helps track the age of deployments and can be crucial for compliance requirements. However, this approach can lead to image proliferation and doesn't inherently track the relationship between images across environments. You might need additional tooling to know that prod-20250302 and dev-20250301 contain the same code. 5. Latest Tag Format : image:latest Example : myapp:latest Pros: Simple to use Always points to newest version Good for development Cons: Unreliable for production Can lead to inconsistent deployments Hard to track actual version deployed Detailed Explanation: The 'latest' tag is a special convention in Docker that typically points to the most recent version of an image. While simple and convenient, especially during development, it's considered an anti-pattern for production use. The main issue is its mutability - the 'latest' tag can point to different images at different times, making it impossible to guarantee consistency across deployments. For example, if two developers pull 'latest' at different times, they might get different versions. This can lead to the \"it works on my machine\" problem and make debugging extremely difficult. Latest tags are best reserved for development environments or automated testing where having the newest version is more important than version stability. Best Practices For CI/CD Pipeline: Always use immutable tags Include build metadata (e.g., myapp:1.2.3-a1b2c3d ) Implement automated version bumping Use multi-stage builds to reduce image size For Production: Never use :latest tag Always specify exact version Implement image scanning Keep image history for rollbacks For ArgoCD: Use specific versions in manifests Implement automatic image updater Configure image pull policies Set up proper RBAC for image repositories Recommended Strategy For a robust production setup, combine multiple approaches: myapp:1.2.3-a1b2c3d-20250302 \u2502 \u2502 \u2502 \u2514\u2500 Build timestamp \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Git SHA (short) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Semantic version \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Image name This provides: - Clear version tracking - Easy rollbacks - Build traceability - Deployment history Combined Strategy Explanation: Many organizations adopt a hybrid approach that combines multiple versioning strategies to get the best of each. A common pattern is to tag each image with both a semantic version and a Git SHA (e.g., myapp:1.2.3-a1b2c3d). This provides both human-readable version information and precise code traceability. Some teams also add build metadata like timestamps or CI build numbers. While this approach provides comprehensive information, it requires more sophisticated build and deployment automation to manage the multiple tags correctly. Automation Tips Use GitHub Actions to automatically: Generate versions based on conventional commits Tag Docker images Push to registry Update deployment manifests Use ArgoCD to: Monitor image repositories Auto-sync new versions Maintain deployment history Enable easy rollbacks Key Stakeholder Considerations The key to successful Docker image versioning is choosing a strategy that balances the needs of different stakeholders: - Developers need to quickly identify and debug issues - Operations teams need stable, traceable deployments - Release managers need clear version progression - Security teams need audit capabilities - End users need clear upgrade paths","title":"Versioning Guide"},{"location":"versioning/#1-git-sha-commit-hash","text":"Format : image:git-sha Example : myapp:a1b2c3d","title":"1. Git SHA (Commit Hash)"},{"location":"versioning/#pros","text":"Precise tracking to source code Immutable and unique Easy to debug and rollback Perfect for development environments","title":"Pros:"},{"location":"versioning/#cons","text":"Not human-readable Difficult to determine version order No immediate indication of stability level","title":"Cons:"},{"location":"versioning/#detailed-explanation","text":"Git SHA versioning uses the unique hash identifier that Git generates for each commit in your repository. When you build a Docker image using this approach, you take the first few characters (usually 7-8) of the commit hash and use it as your image tag. This method creates an unbreakable link between your source code and the Docker image, making it extremely useful for debugging and traceability. For instance, if you discover an issue in production, you can immediately identify the exact code commit that produced that image. However, these hashes are not human-friendly - you can't tell at a glance which version came first or what changes it contains. This makes it less ideal for release management but perfect for development and testing environments where precise code tracking is crucial.","title":"Detailed Explanation:"},{"location":"versioning/#2-semantic-versioning","text":"Format : image:MAJOR.MINOR.PATCH Example : myapp:1.2.3","title":"2. Semantic Versioning"},{"location":"versioning/#pros_1","text":"Clear indication of change magnitude Well understood by developers Good for stable releases Easy to automate with conventional commits","title":"Pros:"},{"location":"versioning/#cons_1","text":"Can be subjective (what constitutes a breaking change?) Multiple tags might point to same image","title":"Cons:"},{"location":"versioning/#detailed-explanation_1","text":"Semantic Versioning follows a structured numbering system with three components: MAJOR.MINOR.PATCH. Each component has a specific meaning - MAJOR versions indicate breaking changes that might require users to modify their code, MINOR versions add new features while maintaining backward compatibility, and PATCH versions represent bug fixes. This system is particularly valuable when your Docker image contains an application or service that other systems depend on. Users can quickly understand the impact of upgrading to a new version. For example, if you're currently using version 1.2.3 and see version 1.2.4, you know it's safe to upgrade since it's just a patch. However, if you see version 2.0.0, you know to carefully review the changes as it contains breaking changes. The main challenge with SemVer is maintaining discipline in version number assignment - teams need to consistently agree on what constitutes a breaking change versus a minor feature addition.","title":"Detailed Explanation:"},{"location":"versioning/#3-git-tag-based","text":"Format : image:v1.2.3 Example : myapp:v1.2.3","title":"3. Git Tag Based"},{"location":"versioning/#pros_2","text":"Direct correlation with Git releases Good for release automation Clear release history","title":"Pros:"},{"location":"versioning/#cons_2","text":"Requires disciplined tag management May need additional CI/CD configuration Can be confusing with multiple release branches","title":"Cons:"},{"location":"versioning/#detailed-explanation_2","text":"Git tag based versioning aligns your Docker image versions with your Git repository's release tags. This approach creates a natural workflow where creating a Git tag automatically triggers a new Docker image build with the same version. It's particularly powerful when combined with semantic versioning - for example, tagging a release as v1.2.3 in Git automatically produces a Docker image tagged 1.2.3. This method works exceptionally well with automated release processes and provides clear documentation of your release history. The challenge comes when managing multiple release branches or when hotfixes need to be applied to older versions. You need robust processes to handle these scenarios and ensure tags are created consistently across branches.","title":"Detailed Explanation:"},{"location":"versioning/#4-environment-based","text":"Format : image:env-timestamp Example : myapp:prod-20250302","title":"4. Environment Based"},{"location":"versioning/#pros_3","text":"Clear deployment target Easy to track when image was built Good for environment-specific configurations","title":"Pros:"},{"location":"versioning/#cons_3","text":"Less precise source tracking Potential confusion with multiple deployments per day Additional storage overhead","title":"Cons:"},{"location":"versioning/#detailed-explanation_3","text":"Environment based versioning adds context about where and when an image is intended to be used. This approach often combines an environment identifier with a timestamp or build number, such as prod-20250302 or staging-build123. This strategy is particularly useful in organizations with complex deployment pipelines involving multiple environments (development, staging, QA, production). It makes it immediately clear which images are approved for which environments and when they were built. The timestamp component helps track the age of deployments and can be crucial for compliance requirements. However, this approach can lead to image proliferation and doesn't inherently track the relationship between images across environments. You might need additional tooling to know that prod-20250302 and dev-20250301 contain the same code.","title":"Detailed Explanation:"},{"location":"versioning/#5-latest-tag","text":"Format : image:latest Example : myapp:latest","title":"5. Latest Tag"},{"location":"versioning/#pros_4","text":"Simple to use Always points to newest version Good for development","title":"Pros:"},{"location":"versioning/#cons_4","text":"Unreliable for production Can lead to inconsistent deployments Hard to track actual version deployed","title":"Cons:"},{"location":"versioning/#detailed-explanation_4","text":"The 'latest' tag is a special convention in Docker that typically points to the most recent version of an image. While simple and convenient, especially during development, it's considered an anti-pattern for production use. The main issue is its mutability - the 'latest' tag can point to different images at different times, making it impossible to guarantee consistency across deployments. For example, if two developers pull 'latest' at different times, they might get different versions. This can lead to the \"it works on my machine\" problem and make debugging extremely difficult. Latest tags are best reserved for development environments or automated testing where having the newest version is more important than version stability.","title":"Detailed Explanation:"},{"location":"versioning/#best-practices","text":"","title":"Best Practices"},{"location":"versioning/#for-cicd-pipeline","text":"Always use immutable tags Include build metadata (e.g., myapp:1.2.3-a1b2c3d ) Implement automated version bumping Use multi-stage builds to reduce image size","title":"For CI/CD Pipeline:"},{"location":"versioning/#for-production","text":"Never use :latest tag Always specify exact version Implement image scanning Keep image history for rollbacks","title":"For Production:"},{"location":"versioning/#for-argocd","text":"Use specific versions in manifests Implement automatic image updater Configure image pull policies Set up proper RBAC for image repositories","title":"For ArgoCD:"},{"location":"versioning/#recommended-strategy","text":"For a robust production setup, combine multiple approaches: myapp:1.2.3-a1b2c3d-20250302 \u2502 \u2502 \u2502 \u2514\u2500 Build timestamp \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Git SHA (short) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Semantic version \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Image name This provides: - Clear version tracking - Easy rollbacks - Build traceability - Deployment history","title":"Recommended Strategy"},{"location":"versioning/#combined-strategy-explanation","text":"Many organizations adopt a hybrid approach that combines multiple versioning strategies to get the best of each. A common pattern is to tag each image with both a semantic version and a Git SHA (e.g., myapp:1.2.3-a1b2c3d). This provides both human-readable version information and precise code traceability. Some teams also add build metadata like timestamps or CI build numbers. While this approach provides comprehensive information, it requires more sophisticated build and deployment automation to manage the multiple tags correctly.","title":"Combined Strategy Explanation:"},{"location":"versioning/#automation-tips","text":"Use GitHub Actions to automatically: Generate versions based on conventional commits Tag Docker images Push to registry Update deployment manifests Use ArgoCD to: Monitor image repositories Auto-sync new versions Maintain deployment history Enable easy rollbacks","title":"Automation Tips"},{"location":"versioning/#key-stakeholder-considerations","text":"The key to successful Docker image versioning is choosing a strategy that balances the needs of different stakeholders: - Developers need to quickly identify and debug issues - Operations teams need stable, traceable deployments - Release managers need clear version progression - Security teams need audit capabilities - End users need clear upgrade paths","title":"Key Stakeholder Considerations"},{"location":"documentation/","text":"Documentation Welcome to the documentation section! Here you'll find detailed guides and tutorials on various DevOps topics. Available Guides Markdown Guide : Learn markdown syntax for better documentation Versioning Guide : Understand different versioning strategies and best practices More guides coming soon!","title":"Overview"},{"location":"documentation/#documentation","text":"Welcome to the documentation section! Here you'll find detailed guides and tutorials on various DevOps topics.","title":"Documentation"},{"location":"documentation/#available-guides","text":"Markdown Guide : Learn markdown syntax for better documentation Versioning Guide : Understand different versioning strategies and best practices More guides coming soon!","title":"Available Guides"},{"location":"documentation/contents/","text":"Documentation Contents Welcome to the comprehensive documentation section. Here you'll find detailed guides, tutorials, and best practices for various DevOps tools and technologies. CI/CD GitHub Actions with SonarQube & Harbor A comprehensive guide on setting up a modern CI/CD pipeline using GitHub Actions, integrated with SonarQube for code quality analysis and Harbor for container registry. Includes security best practices and troubleshooting tips. Infrastructure Ansible Guide Complete guide to Ansible automation, covering installation, configuration, and practical use cases. Learn how to automate server preparation, package management, and integrate with GitHub Actions for automated deployments. Networking Networking Overview Introduction to networking concepts and tools essential for DevOps practices. Covers basic network configuration, troubleshooting, and security considerations. Miscellaneous General Overview Additional resources and guides that don't fit into specific categories but are valuable for DevOps practices. Quick Links by Topic Version Control & CI/CD GitHub Actions Integration SonarQube Code Analysis Harbor Registry Setup Infrastructure Management Ansible Installation Server Preparation Package Management Environment Variables Security UFW Firewall Management SSL/TLS with Certbot Secrets Management Troubleshooting Ansible Debugging CI/CD Pipeline Issues Latest Updates Recently Added March 2025 : Added comprehensive Ansible guide with GitHub Actions integration March 2025 : Updated GitHub Actions guide with latest 2025 best practices March 2025 : Enhanced documentation with improved navigation and dark theme Contributing Found something that needs updating? All documentation pages have an \"Edit\" button at the top right. Feel free to submit improvements or report issues through our GitHub repository.","title":"Contents"},{"location":"documentation/contents/#documentation-contents","text":"Welcome to the comprehensive documentation section. Here you'll find detailed guides, tutorials, and best practices for various DevOps tools and technologies.","title":"Documentation Contents"},{"location":"documentation/contents/#cicd","text":"","title":"CI/CD"},{"location":"documentation/contents/#github-actions-with-sonarqube-harbor","text":"A comprehensive guide on setting up a modern CI/CD pipeline using GitHub Actions, integrated with SonarQube for code quality analysis and Harbor for container registry. Includes security best practices and troubleshooting tips.","title":"GitHub Actions with SonarQube &amp; Harbor"},{"location":"documentation/contents/#infrastructure","text":"","title":"Infrastructure"},{"location":"documentation/contents/#ansible-guide","text":"Complete guide to Ansible automation, covering installation, configuration, and practical use cases. Learn how to automate server preparation, package management, and integrate with GitHub Actions for automated deployments.","title":"Ansible Guide"},{"location":"documentation/contents/#networking","text":"","title":"Networking"},{"location":"documentation/contents/#networking-overview","text":"Introduction to networking concepts and tools essential for DevOps practices. Covers basic network configuration, troubleshooting, and security considerations.","title":"Networking Overview"},{"location":"documentation/contents/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"documentation/contents/#general-overview","text":"Additional resources and guides that don't fit into specific categories but are valuable for DevOps practices.","title":"General Overview"},{"location":"documentation/contents/#quick-links-by-topic","text":"","title":"Quick Links by Topic"},{"location":"documentation/contents/#version-control-cicd","text":"GitHub Actions Integration SonarQube Code Analysis Harbor Registry Setup","title":"Version Control &amp; CI/CD"},{"location":"documentation/contents/#infrastructure-management","text":"Ansible Installation Server Preparation Package Management Environment Variables","title":"Infrastructure Management"},{"location":"documentation/contents/#security","text":"UFW Firewall Management SSL/TLS with Certbot Secrets Management","title":"Security"},{"location":"documentation/contents/#troubleshooting","text":"Ansible Debugging CI/CD Pipeline Issues","title":"Troubleshooting"},{"location":"documentation/contents/#latest-updates","text":"Recently Added March 2025 : Added comprehensive Ansible guide with GitHub Actions integration March 2025 : Updated GitHub Actions guide with latest 2025 best practices March 2025 : Enhanced documentation with improved navigation and dark theme","title":"Latest Updates"},{"location":"documentation/contents/#contributing","text":"Found something that needs updating? All documentation pages have an \"Edit\" button at the top right. Feel free to submit improvements or report issues through our GitHub repository.","title":"Contributing"},{"location":"documentation/cicd/","text":"CI/CD Documentation This section contains guides and documentation related to Continuous Integration and Continuous Deployment practices and tools. Available Guides GitHub Actions: SonarQube Scanning & Docker Image Deployment to Harbor - Learn how to set up a GitHub Actions workflow that scans your code with SonarQube and builds/pushes Docker images to Harbor. Coming Soon Jenkins Pipeline Setup GitLab CI/CD Configuration Tekton Pipelines on Kubernetes","title":"Overview"},{"location":"documentation/cicd/#cicd-documentation","text":"This section contains guides and documentation related to Continuous Integration and Continuous Deployment practices and tools.","title":"CI/CD Documentation"},{"location":"documentation/cicd/#available-guides","text":"GitHub Actions: SonarQube Scanning & Docker Image Deployment to Harbor - Learn how to set up a GitHub Actions workflow that scans your code with SonarQube and builds/pushes Docker images to Harbor.","title":"Available Guides"},{"location":"documentation/cicd/#coming-soon","text":"Jenkins Pipeline Setup GitLab CI/CD Configuration Tekton Pipelines on Kubernetes","title":"Coming Soon"},{"location":"documentation/cicd/github-actions-sonarqube-docker/","text":"GitHub Actions: SonarQube Scanning & Docker Image Deployment to Harbor Overview Guide Information Difficulty : Intermediate Time Required : ~30 minutes for setup Last Updated : March 2025 GitHub Actions Version : v4.2 (2025) SonarQube Version : 11.2 LTS Harbor Version : 3.2 Docker Version : 26.0 This guide provides comprehensive instructions for setting up a GitHub Actions workflow that: Scans your code using SonarQube for quality and security issues Builds a Docker image from your codebase Pushes the image to a private Harbor registry Workflow Architecture graph TD A[Git Push] -->|Trigger| B[GitHub Actions] B --> C{Run Jobs} C -->|Job 1| D[Code Quality] C -->|Job 2| E[Build & Push] D --> D1[Checkout Code] D --> D2[SonarQube Scan] D --> D3[Quality Gate] E --> E1[Checkout Code] E --> E2[Docker Build] E --> E3[Security Scan] E --> E4[Push to Harbor] D3 -->|Pass| E D3 -->|Fail| F[Notify Team] E4 --> G[Deployment Ready] style A fill:#f9f,stroke:#333,stroke-width:2px style B fill:#bbf,stroke:#333,stroke-width:2px style D fill:#dfd,stroke:#333,stroke-width:2px style E fill:#dfd,stroke:#333,stroke-width:2px style F fill:#fdd,stroke:#333,stroke-width:2px style G fill:#dfd,stroke:#333,stroke-width:2px CI/CD Process Flow sequenceDiagram participant Dev as Developer participant GH as GitHub participant GA as GitHub Actions participant SQ as SonarQube participant DR as Docker Registry participant HR as Harbor Registry Dev->>GH: Push Code GH->>GA: Trigger Workflow GA->>SQ: Run Code Analysis SQ-->>GA: Quality Report alt Quality Gate Passed GA->>DR: Pull Base Image GA->>GA: Build Docker Image GA->>GA: Run Security Scan GA->>HR: Push Image HR-->>GA: Confirm Push GA-->>GH: Update Status else Quality Gate Failed GA-->>GH: Mark Check as Failed GH-->>Dev: Notify Failure end Prerequisites Before you begin, ensure you have: A GitHub account with a private repository containing your application code Admin access to a SonarQube instance (self-hosted or SonarCloud) Access to a Harbor registry instance A Dockerfile in your repository that defines your application image Basic understanding of YAML and GitHub Actions concepts Repository Structure graph TD A[Repository Root] --> B[.github] B --> C[workflows] C --> D[build-scan-deploy.yml] A --> E[src] E --> E1[application code] A --> F[tests] F --> F1[test files] A --> G[Dockerfile] A --> H[sonar-project.properties] A --> I[README.md] style A fill:#f9f,stroke:#333,stroke-width:2px style D fill:#bbf,stroke:#333,stroke-width:2px style G fill:#bfb,stroke:#333,stroke-width:2px style H fill:#bfb,stroke:#333,stroke-width:2px Step-by-Step Implementation 1. Create GitHub Secrets First, you'll need to add the following secrets to your GitHub repository: Navigate to your GitHub repository Go to Settings > Secrets and variables > Actions Click New repository secret and add the following: Secret Name Description SONAR_TOKEN Authentication token for SonarQube SONAR_HOST_URL URL of your SonarQube instance HARBOR_USERNAME Username for Harbor registry HARBOR_PASSWORD Password or access token for Harbor registry HARBOR_URL URL of your Harbor registry HARBOR_PROJECT Project name in Harbor IMAGE_NAME Name of the Docker image 2. Create the GitHub Actions Workflow File Create a new file at .github/workflows/build-scan-deploy.yml : name : Build, Scan and Deploy on : push : branches : [ main , develop ] pull_request : branches : [ main , develop ] workflow_dispatch : env : IMAGE_TAG : ${{ github.sha }} jobs : code-quality : name : Code Quality Scan runs-on : ubuntu-latest permissions : security-events : write actions : read contents : read steps : - name : Checkout code uses : actions/checkout@v5 with : fetch-depth : 0 - name : SonarQube Scan uses : SonarSource/sonarqube-scan-action@v4 env : SONAR_TOKEN : ${{ secrets.SONAR_TOKEN }} SONAR_HOST_URL : ${{ secrets.SONAR_HOST_URL }} with : args : > -Dsonar.projectKey=${{ github.repository_owner }}_${{ github.event.repository.name }} -Dsonar.projectName=${{ github.repository_owner }}_${{ github.event.repository.name }} -Dsonar.python.version=3.12 -Dsonar.qualitygate.wait=true - name : SonarQube Quality Gate uses : SonarSource/sonarqube-quality-gate-action@v3 timeout-minutes : 5 env : SONAR_TOKEN : ${{ secrets.SONAR_TOKEN }} SONAR_HOST_URL : ${{ secrets.SONAR_HOST_URL }} build-and-push : name : Build and Push Docker Image needs : code-quality runs-on : ubuntu-latest permissions : packages : write contents : read steps : - name : Checkout code uses : actions/checkout@v5 - name : Set up Docker Buildx uses : docker/setup-buildx-action@v3 with : version : v0.12.0 buildkitd-flags : --debug - name : Login to Harbor uses : docker/login-action@v3 with : registry : ${{ secrets.HARBOR_URL }} username : ${{ secrets.HARBOR_USERNAME }} password : ${{ secrets.HARBOR_PASSWORD }} - name : Generate Image Tags id : meta uses : docker/metadata-action@v5 with : images : ${{ secrets.HARBOR_URL }}/${{ secrets.HARBOR_PROJECT }}/${{ secrets.IMAGE_NAME }} tags : | type=raw,value=latest,enable=${{ github.ref == 'refs/heads/main' }} type=sha,format=short type=ref,event=branch type=semver,pattern={{version}} type=semver,pattern={{major}}.{{minor}} - name : Build and Push Docker Image uses : docker/build-push-action@v5 with : context : . push : true tags : ${{ steps.meta.outputs.tags }} labels : ${{ steps.meta.outputs.labels }} cache-from : type=registry,ref=${{ secrets.HARBOR_URL }}/${{ secrets.HARBOR_PROJECT }}/${{ secrets.IMAGE_NAME }}:buildcache cache-to : type=registry,ref=${{ secrets.HARBOR_URL }}/${{ secrets.HARBOR_PROJECT }}/${{ secrets.IMAGE_NAME }}:buildcache,mode=max platforms : linux/amd64,linux/arm64 build-args : | BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ') VCS_REF=${{ github.sha }} VERSION=${{ steps.meta.outputs.version }} - name : Run Trivy vulnerability scanner uses : aquasecurity/trivy-action@master with : image-ref : ${{ secrets.HARBOR_URL }}/${{ secrets.HARBOR_PROJECT }}/${{ secrets.IMAGE_NAME }}:${{ github.sha }} format : 'sarif' output : 'trivy-results.sarif' severity : 'CRITICAL,HIGH' timeout : '10m0s' - name : Upload Trivy scan results to GitHub Security tab uses : github/codeql-action/upload-sarif@v3 if : always() with : sarif_file : 'trivy-results.sarif' 3. Configure SonarQube Properties Create a sonar-project.properties file: sonar.projectKey = ${PROJECT_KEY} sonar.projectName = ${PROJECT_NAME} sonar.projectVersion = 1.0 sonar.sources = src sonar.tests = tests sonar.sourceEncoding = UTF-8 sonar.exclusions = **/node_modules/**,**/*.spec.ts,**/dist/**,**/coverage/** sonar.javascript.lcov.reportPaths = coverage/lcov.info sonar.python.version = 3.12 sonar.python.coverage.reportPaths = coverage.xml sonar.python.flake8.reportPaths = flake8-report.txt sonar.security.config.path = .sonarqube/security-config.json 4. Modern Dockerfile Example # Syntax version for better caching and security # syntax=docker/dockerfile:1.6 # Build stage FROM node:20-alpine AS builder # Use non-root user for security USER node WORKDIR /app # Copy package files COPY --chown = node:node package*.json ./ # Install dependencies with exact versions RUN npm ci --only = production # Copy source code COPY --chown = node:node . . # Build application RUN npm run build # Production stage FROM node:20-alpine AS production # Set security headers LABEL org.opencontainers.image.security.policy = \"policy.json\" # Use non-root user USER node WORKDIR /app # Copy only production files COPY --from = builder --chown = node:node /app/package*.json ./ COPY --from = builder --chown = node:node /app/node_modules ./node_modules COPY --from = builder --chown = node:node /app/dist ./dist # Set environment ENV NODE_ENV = production \\ PORT = 3000 # Expose port EXPOSE 3000 # Health check HEALTHCHECK --interval = 30s --timeout = 3s \\ CMD wget --no-verbose --tries = 1 --spider http://localhost:3000/health || exit 1 # Start application CMD [ \"node\" , \"dist/main.js\" ] Security Best Practices %%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '16px', 'fontFamily': 'arial' }, 'securityLevel': 'loose', 'zoom': 1.5 }}%% graph TD A[Security Best Practices] --> B[Secrets Management] A --> C[Container Security] A --> D[Code Security] A --> E[Access Control] A --> F[Image Security] B --> B1[Use GitHub Secrets] B --> B2[Rotate Regularly] B --> B3[Limit Access Scope] C --> C1[Non-root User] C --> C2[Multi-stage Builds] C --> C3[Minimal Base Images] C --> C4[Regular Updates] D --> D1[SonarQube Scanning] D --> D2[Dependency Scanning] D --> D3[SAST/DAST] D --> D4[Quality Gates] E --> E1[RBAC] E --> E2[Least Privilege] E --> E3[Token-based Auth] E --> E4[Regular Audits] F --> F1[Version Tagging] F --> F2[Image Signing] F --> F3[Vulnerability Scanning] F --> F4[Harbor Security] style A fill:#f9f,stroke:#333,stroke-width:3px,text-align:center,width:200px,height:60px style B fill:#bbf,stroke:#333,stroke-width:2px,width:180px style C fill:#bbf,stroke:#333,stroke-width:2px,width:180px style D fill:#bbf,stroke:#333,stroke-width:2px,width:180px style E fill:#bbf,stroke:#333,stroke-width:2px,width:180px style F fill:#bbf,stroke:#333,stroke-width:2px,width:180px Troubleshooting Guide flowchart TD A[Issue Detected] --> B{Error Type} B -->|Build Error| C[Check Build Logs] B -->|Push Error| D[Check Registry] B -->|Scan Error| E[Check SonarQube] C --> C1[Check Dockerfile] C --> C2[Check Dependencies] C --> C3[Check Build Context] D --> D1[Check Credentials] D --> D2[Check Network] D --> D3[Check Permissions] E --> E1[Check Token] E --> E2[Check Config] E --> E3[Check Coverage] style A fill:#f96,stroke:#333,stroke-width:2px style B fill:#69f,stroke:#333,stroke-width:2px style C fill:#9f6,stroke:#333,stroke-width:2px style D fill:#9f6,stroke:#333,stroke-width:2px style E fill:#9f6,stroke:#333,stroke-width:2px Common Issues and Solutions SonarQube Analysis Fails Issue : SonarQube scan fails with authentication errors. Solution : - Verify SONAR_TOKEN is valid and not expired - Check SonarQube instance accessibility - Ensure correct project permissions Docker Build Fails Issue : Docker build fails with errors. Solution : - Check Dockerfile syntax - Verify build context - Check base image availability - Validate multi-stage build steps Harbor Push Fails Issue : Cannot push to Harbor registry. Solution : - Verify Harbor credentials - Check project exists and permissions - Validate network connectivity - Ensure image tag format is correct Monitoring and Alerts graph LR A[GitHub Actions] -->|Status| B[GitHub Status Checks] A -->|Logs| C[GitHub Actions Logs] A -->|Alerts| D[GitHub Notifications] B -->|Success/Failure| E[Team Notification] C -->|Error Analysis| F[Troubleshooting] D -->|Security Issues| G[Security Team] style A fill:#f9f,stroke:#333,stroke-width:2px style E fill:#9f9,stroke:#333,stroke-width:2px style G fill:#f99,stroke:#333,stroke-width:2px Conclusion This GitHub Actions workflow provides a modern, secure CI/CD pipeline that ensures code quality through SonarQube scanning and delivers secure, versioned Docker images to your Harbor registry. The workflow includes best practices for 2025, including: Multi-platform builds (amd64/arm64) Advanced security scanning Comprehensive error handling Modern Docker features Detailed monitoring and alerting For more advanced scenarios or customizations, refer to: - GitHub Actions Documentation - SonarQube Documentation - Harbor Documentation Next Steps Consider integrating this workflow with ArgoCD for continuous deployment to your Kubernetes cluster. See our ArgoCD Setup Guide for more information.","title":"GitHub Actions with SonarQube & Harbor"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#github-actions-sonarqube-scanning-docker-image-deployment-to-harbor","text":"","title":"GitHub Actions: SonarQube Scanning &amp; Docker Image Deployment to Harbor"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#overview","text":"Guide Information Difficulty : Intermediate Time Required : ~30 minutes for setup Last Updated : March 2025 GitHub Actions Version : v4.2 (2025) SonarQube Version : 11.2 LTS Harbor Version : 3.2 Docker Version : 26.0 This guide provides comprehensive instructions for setting up a GitHub Actions workflow that: Scans your code using SonarQube for quality and security issues Builds a Docker image from your codebase Pushes the image to a private Harbor registry","title":"Overview"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#workflow-architecture","text":"graph TD A[Git Push] -->|Trigger| B[GitHub Actions] B --> C{Run Jobs} C -->|Job 1| D[Code Quality] C -->|Job 2| E[Build & Push] D --> D1[Checkout Code] D --> D2[SonarQube Scan] D --> D3[Quality Gate] E --> E1[Checkout Code] E --> E2[Docker Build] E --> E3[Security Scan] E --> E4[Push to Harbor] D3 -->|Pass| E D3 -->|Fail| F[Notify Team] E4 --> G[Deployment Ready] style A fill:#f9f,stroke:#333,stroke-width:2px style B fill:#bbf,stroke:#333,stroke-width:2px style D fill:#dfd,stroke:#333,stroke-width:2px style E fill:#dfd,stroke:#333,stroke-width:2px style F fill:#fdd,stroke:#333,stroke-width:2px style G fill:#dfd,stroke:#333,stroke-width:2px","title":"Workflow Architecture"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#cicd-process-flow","text":"sequenceDiagram participant Dev as Developer participant GH as GitHub participant GA as GitHub Actions participant SQ as SonarQube participant DR as Docker Registry participant HR as Harbor Registry Dev->>GH: Push Code GH->>GA: Trigger Workflow GA->>SQ: Run Code Analysis SQ-->>GA: Quality Report alt Quality Gate Passed GA->>DR: Pull Base Image GA->>GA: Build Docker Image GA->>GA: Run Security Scan GA->>HR: Push Image HR-->>GA: Confirm Push GA-->>GH: Update Status else Quality Gate Failed GA-->>GH: Mark Check as Failed GH-->>Dev: Notify Failure end","title":"CI/CD Process Flow"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#prerequisites","text":"Before you begin, ensure you have: A GitHub account with a private repository containing your application code Admin access to a SonarQube instance (self-hosted or SonarCloud) Access to a Harbor registry instance A Dockerfile in your repository that defines your application image Basic understanding of YAML and GitHub Actions concepts","title":"Prerequisites"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#repository-structure","text":"graph TD A[Repository Root] --> B[.github] B --> C[workflows] C --> D[build-scan-deploy.yml] A --> E[src] E --> E1[application code] A --> F[tests] F --> F1[test files] A --> G[Dockerfile] A --> H[sonar-project.properties] A --> I[README.md] style A fill:#f9f,stroke:#333,stroke-width:2px style D fill:#bbf,stroke:#333,stroke-width:2px style G fill:#bfb,stroke:#333,stroke-width:2px style H fill:#bfb,stroke:#333,stroke-width:2px","title":"Repository Structure"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#step-by-step-implementation","text":"","title":"Step-by-Step Implementation"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#1-create-github-secrets","text":"First, you'll need to add the following secrets to your GitHub repository: Navigate to your GitHub repository Go to Settings > Secrets and variables > Actions Click New repository secret and add the following: Secret Name Description SONAR_TOKEN Authentication token for SonarQube SONAR_HOST_URL URL of your SonarQube instance HARBOR_USERNAME Username for Harbor registry HARBOR_PASSWORD Password or access token for Harbor registry HARBOR_URL URL of your Harbor registry HARBOR_PROJECT Project name in Harbor IMAGE_NAME Name of the Docker image","title":"1. Create GitHub Secrets"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#2-create-the-github-actions-workflow-file","text":"Create a new file at .github/workflows/build-scan-deploy.yml : name : Build, Scan and Deploy on : push : branches : [ main , develop ] pull_request : branches : [ main , develop ] workflow_dispatch : env : IMAGE_TAG : ${{ github.sha }} jobs : code-quality : name : Code Quality Scan runs-on : ubuntu-latest permissions : security-events : write actions : read contents : read steps : - name : Checkout code uses : actions/checkout@v5 with : fetch-depth : 0 - name : SonarQube Scan uses : SonarSource/sonarqube-scan-action@v4 env : SONAR_TOKEN : ${{ secrets.SONAR_TOKEN }} SONAR_HOST_URL : ${{ secrets.SONAR_HOST_URL }} with : args : > -Dsonar.projectKey=${{ github.repository_owner }}_${{ github.event.repository.name }} -Dsonar.projectName=${{ github.repository_owner }}_${{ github.event.repository.name }} -Dsonar.python.version=3.12 -Dsonar.qualitygate.wait=true - name : SonarQube Quality Gate uses : SonarSource/sonarqube-quality-gate-action@v3 timeout-minutes : 5 env : SONAR_TOKEN : ${{ secrets.SONAR_TOKEN }} SONAR_HOST_URL : ${{ secrets.SONAR_HOST_URL }} build-and-push : name : Build and Push Docker Image needs : code-quality runs-on : ubuntu-latest permissions : packages : write contents : read steps : - name : Checkout code uses : actions/checkout@v5 - name : Set up Docker Buildx uses : docker/setup-buildx-action@v3 with : version : v0.12.0 buildkitd-flags : --debug - name : Login to Harbor uses : docker/login-action@v3 with : registry : ${{ secrets.HARBOR_URL }} username : ${{ secrets.HARBOR_USERNAME }} password : ${{ secrets.HARBOR_PASSWORD }} - name : Generate Image Tags id : meta uses : docker/metadata-action@v5 with : images : ${{ secrets.HARBOR_URL }}/${{ secrets.HARBOR_PROJECT }}/${{ secrets.IMAGE_NAME }} tags : | type=raw,value=latest,enable=${{ github.ref == 'refs/heads/main' }} type=sha,format=short type=ref,event=branch type=semver,pattern={{version}} type=semver,pattern={{major}}.{{minor}} - name : Build and Push Docker Image uses : docker/build-push-action@v5 with : context : . push : true tags : ${{ steps.meta.outputs.tags }} labels : ${{ steps.meta.outputs.labels }} cache-from : type=registry,ref=${{ secrets.HARBOR_URL }}/${{ secrets.HARBOR_PROJECT }}/${{ secrets.IMAGE_NAME }}:buildcache cache-to : type=registry,ref=${{ secrets.HARBOR_URL }}/${{ secrets.HARBOR_PROJECT }}/${{ secrets.IMAGE_NAME }}:buildcache,mode=max platforms : linux/amd64,linux/arm64 build-args : | BUILD_DATE=$(date -u +'%Y-%m-%dT%H:%M:%SZ') VCS_REF=${{ github.sha }} VERSION=${{ steps.meta.outputs.version }} - name : Run Trivy vulnerability scanner uses : aquasecurity/trivy-action@master with : image-ref : ${{ secrets.HARBOR_URL }}/${{ secrets.HARBOR_PROJECT }}/${{ secrets.IMAGE_NAME }}:${{ github.sha }} format : 'sarif' output : 'trivy-results.sarif' severity : 'CRITICAL,HIGH' timeout : '10m0s' - name : Upload Trivy scan results to GitHub Security tab uses : github/codeql-action/upload-sarif@v3 if : always() with : sarif_file : 'trivy-results.sarif'","title":"2. Create the GitHub Actions Workflow File"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#3-configure-sonarqube-properties","text":"Create a sonar-project.properties file: sonar.projectKey = ${PROJECT_KEY} sonar.projectName = ${PROJECT_NAME} sonar.projectVersion = 1.0 sonar.sources = src sonar.tests = tests sonar.sourceEncoding = UTF-8 sonar.exclusions = **/node_modules/**,**/*.spec.ts,**/dist/**,**/coverage/** sonar.javascript.lcov.reportPaths = coverage/lcov.info sonar.python.version = 3.12 sonar.python.coverage.reportPaths = coverage.xml sonar.python.flake8.reportPaths = flake8-report.txt sonar.security.config.path = .sonarqube/security-config.json","title":"3. Configure SonarQube Properties"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#4-modern-dockerfile-example","text":"# Syntax version for better caching and security # syntax=docker/dockerfile:1.6 # Build stage FROM node:20-alpine AS builder # Use non-root user for security USER node WORKDIR /app # Copy package files COPY --chown = node:node package*.json ./ # Install dependencies with exact versions RUN npm ci --only = production # Copy source code COPY --chown = node:node . . # Build application RUN npm run build # Production stage FROM node:20-alpine AS production # Set security headers LABEL org.opencontainers.image.security.policy = \"policy.json\" # Use non-root user USER node WORKDIR /app # Copy only production files COPY --from = builder --chown = node:node /app/package*.json ./ COPY --from = builder --chown = node:node /app/node_modules ./node_modules COPY --from = builder --chown = node:node /app/dist ./dist # Set environment ENV NODE_ENV = production \\ PORT = 3000 # Expose port EXPOSE 3000 # Health check HEALTHCHECK --interval = 30s --timeout = 3s \\ CMD wget --no-verbose --tries = 1 --spider http://localhost:3000/health || exit 1 # Start application CMD [ \"node\" , \"dist/main.js\" ]","title":"4. Modern Dockerfile Example"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#security-best-practices","text":"%%{init: {'theme': 'base', 'themeVariables': { 'fontSize': '16px', 'fontFamily': 'arial' }, 'securityLevel': 'loose', 'zoom': 1.5 }}%% graph TD A[Security Best Practices] --> B[Secrets Management] A --> C[Container Security] A --> D[Code Security] A --> E[Access Control] A --> F[Image Security] B --> B1[Use GitHub Secrets] B --> B2[Rotate Regularly] B --> B3[Limit Access Scope] C --> C1[Non-root User] C --> C2[Multi-stage Builds] C --> C3[Minimal Base Images] C --> C4[Regular Updates] D --> D1[SonarQube Scanning] D --> D2[Dependency Scanning] D --> D3[SAST/DAST] D --> D4[Quality Gates] E --> E1[RBAC] E --> E2[Least Privilege] E --> E3[Token-based Auth] E --> E4[Regular Audits] F --> F1[Version Tagging] F --> F2[Image Signing] F --> F3[Vulnerability Scanning] F --> F4[Harbor Security] style A fill:#f9f,stroke:#333,stroke-width:3px,text-align:center,width:200px,height:60px style B fill:#bbf,stroke:#333,stroke-width:2px,width:180px style C fill:#bbf,stroke:#333,stroke-width:2px,width:180px style D fill:#bbf,stroke:#333,stroke-width:2px,width:180px style E fill:#bbf,stroke:#333,stroke-width:2px,width:180px style F fill:#bbf,stroke:#333,stroke-width:2px,width:180px","title":"Security Best Practices"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#troubleshooting-guide","text":"flowchart TD A[Issue Detected] --> B{Error Type} B -->|Build Error| C[Check Build Logs] B -->|Push Error| D[Check Registry] B -->|Scan Error| E[Check SonarQube] C --> C1[Check Dockerfile] C --> C2[Check Dependencies] C --> C3[Check Build Context] D --> D1[Check Credentials] D --> D2[Check Network] D --> D3[Check Permissions] E --> E1[Check Token] E --> E2[Check Config] E --> E3[Check Coverage] style A fill:#f96,stroke:#333,stroke-width:2px style B fill:#69f,stroke:#333,stroke-width:2px style C fill:#9f6,stroke:#333,stroke-width:2px style D fill:#9f6,stroke:#333,stroke-width:2px style E fill:#9f6,stroke:#333,stroke-width:2px","title":"Troubleshooting Guide"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#common-issues-and-solutions","text":"","title":"Common Issues and Solutions"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#sonarqube-analysis-fails","text":"Issue : SonarQube scan fails with authentication errors. Solution : - Verify SONAR_TOKEN is valid and not expired - Check SonarQube instance accessibility - Ensure correct project permissions","title":"SonarQube Analysis Fails"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#docker-build-fails","text":"Issue : Docker build fails with errors. Solution : - Check Dockerfile syntax - Verify build context - Check base image availability - Validate multi-stage build steps","title":"Docker Build Fails"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#harbor-push-fails","text":"Issue : Cannot push to Harbor registry. Solution : - Verify Harbor credentials - Check project exists and permissions - Validate network connectivity - Ensure image tag format is correct","title":"Harbor Push Fails"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#monitoring-and-alerts","text":"graph LR A[GitHub Actions] -->|Status| B[GitHub Status Checks] A -->|Logs| C[GitHub Actions Logs] A -->|Alerts| D[GitHub Notifications] B -->|Success/Failure| E[Team Notification] C -->|Error Analysis| F[Troubleshooting] D -->|Security Issues| G[Security Team] style A fill:#f9f,stroke:#333,stroke-width:2px style E fill:#9f9,stroke:#333,stroke-width:2px style G fill:#f99,stroke:#333,stroke-width:2px","title":"Monitoring and Alerts"},{"location":"documentation/cicd/github-actions-sonarqube-docker/#conclusion","text":"This GitHub Actions workflow provides a modern, secure CI/CD pipeline that ensures code quality through SonarQube scanning and delivers secure, versioned Docker images to your Harbor registry. The workflow includes best practices for 2025, including: Multi-platform builds (amd64/arm64) Advanced security scanning Comprehensive error handling Modern Docker features Detailed monitoring and alerting For more advanced scenarios or customizations, refer to: - GitHub Actions Documentation - SonarQube Documentation - Harbor Documentation Next Steps Consider integrating this workflow with ArgoCD for continuous deployment to your Kubernetes cluster. See our ArgoCD Setup Guide for more information.","title":"Conclusion"},{"location":"documentation/infrastructure/","text":"Infrastructure Coming soon! This section will contain guides and tutorials about infrastructure management and cloud services.","title":"Overview"},{"location":"documentation/infrastructure/#infrastructure","text":"Coming soon! This section will contain guides and tutorials about infrastructure management and cloud services.","title":"Infrastructure"},{"location":"documentation/infrastructure/ansible-guide/","text":"Ansible: Automation at Scale Guide Information Difficulty : Intermediate Time Required : ~45 minutes for setup Last Updated : March 2025 Ansible Version : 9.2.0 Ansible Galaxy Version : 4.5.0 Python Version : 3.12 This comprehensive guide covers Ansible installation, configuration, and practical use cases for DevOps automation. You'll learn how to use Ansible for server preparation, package management, and integrating with GitHub Actions for automated deployments. Prerequisites Before starting, ensure you have: - Python 3.12 or newer installed - Access to target servers (SSH access) - Sudo privileges on both control and target machines - Basic understanding of YAML syntax - GitHub account (for GitHub Actions integration) Installation Installing Ansible # Update package list sudo apt update # Install Python 3 and pip sudo apt install -y python3 python3-pip # Install Ansible using pip python3 -m pip install --user ansible == 9 .2.0 # Verify installation ansible --version Installing Ansible Galaxy # Install Ansible Galaxy python3 -m pip install --user ansible-galaxy # Verify installation ansible-galaxy --version Basic Configuration 1. Configure Inventory Create an inventory file at /etc/ansible/hosts or in your project directory: [webservers] web1 ansible_host = 192.168.1.101 web2 ansible_host = 192.168.1.102 [dbservers] db1 ansible_host = 192.168.1.201 [kubernetes_nodes] master ansible_host = 192.168.1.10 worker1 ansible_host = 192.168.1.11 worker2 ansible_host = 192.168.1.12 [all:vars] ansible_python_interpreter = /usr/bin/python3 ansible_user = your_ssh_user 2. Configure SSH Keys # Generate SSH key ssh-keygen -t ed25519 -C \"ansible\" # Copy key to remote servers ssh-copy-id -i ~/.ssh/ansible your_user@remote_host Common Tasks and Playbooks Server Preparation Playbook # server_prep.yml --- - name : Prepare servers for Kubernetes hosts : kubernetes_nodes become : yes tasks : - name : Update apt cache apt : update_cache : yes cache_valid_time : 3600 - name : Install common packages apt : name : - neovim - vim - docker.io - docker-compose - ncdu - gdu - logrotate - nginx - certbot - python3-certbot-nginx state : present - name : Configure UFW ufw : rule : allow port : \"{{ item }}\" proto : tcp loop : - 22 - 80 - 443 - 6443 # Kubernetes API - 2379 # etcd - 2380 # etcd peer - 10250 # Kubelet API - name : Enable UFW ufw : state : enabled policy : deny - name : Configure logrotate template : src : templates/logrotate.j2 dest : /etc/logrotate.d/custom mode : '0644' Docker Deployment Playbook # docker_deploy.yml --- - name : Deploy Docker Applications hosts : webservers become : yes vars : harbor_registry : harbor.example.com harbor_project : myproject harbor_username : \"{{ lookup('env', 'HARBOR_USERNAME') }}\" harbor_password : \"{{ lookup('env', 'HARBOR_PASSWORD') }}\" tasks : - name : Log into Harbor registry docker_login : registry_url : \"{{ harbor_registry }}\" username : \"{{ harbor_username }}\" password : \"{{ harbor_password }}\" reauthorize : yes - name : Create Docker Compose directory file : path : /opt/docker-compose state : directory mode : '0755' - name : Copy Docker Compose file template : src : templates/docker-compose.yml.j2 dest : /opt/docker-compose/docker-compose.yml mode : '0644' - name : Deploy using Docker Compose docker_compose : project_src : /opt/docker-compose pull : yes recreate : always GitHub Actions Integration Workflow Example # .github/workflows/ansible-deploy.yml name : Build and Deploy with Ansible on : push : branches : [ main ] workflow_dispatch : jobs : build-and-deploy : runs-on : ubuntu-latest steps : - uses : actions/checkout@v5 - name : Set up Docker Buildx uses : docker/setup-buildx-action@v3 - name : Login to Harbor uses : docker/login-action@v3 with : registry : ${{ secrets.HARBOR_URL }} username : ${{ secrets.HARBOR_USERNAME }} password : ${{ secrets.HARBOR_PASSWORD }} - name : Build and push uses : docker/build-push-action@v5 with : push : true tags : ${{ secrets.HARBOR_URL }}/${{ secrets.HARBOR_PROJECT }}/myapp:${{ github.sha }} - name : Install Ansible run : | python -m pip install --user ansible==9.2.0 python -m pip install --user ansible-galaxy - name : Set up SSH key run : | mkdir -p ~/.ssh echo \"${{ secrets.SSH_PRIVATE_KEY }}\" > ~/.ssh/id_ed25519 chmod 600 ~/.ssh/id_ed25519 ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts - name : Run Ansible playbook env : HARBOR_USERNAME : ${{ secrets.HARBOR_USERNAME }} HARBOR_PASSWORD : ${{ secrets.HARBOR_PASSWORD }} run : | ansible-playbook -i inventory.yml docker_deploy.yml Environment Variables Management Using Ansible Vault # Create encrypted variables file ansible-vault create vars/secrets.yml # Edit encrypted file ansible-vault edit vars/secrets.yml # Run playbook with vault ansible-playbook playbook.yml --ask-vault-pass Example encrypted variables: # vars/secrets.yml (encrypted) harbor_credentials : username : myuser password : mypassword db_credentials : user : dbuser password : dbpassword Troubleshooting Common Issues and Solutions flowchart TD A[Issue Detected] --> B{Error Type} B -->|Connection| C[SSH Issues] B -->|Permission| D[Sudo Access] B -->|Task| E[Playbook Error] C --> C1[Check SSH Keys] C --> C2[Verify Connectivity] C --> C3[Check Inventory] D --> D1[Check sudoers] D --> D2[Verify become:] D --> D3[Check Privileges] E --> E1[Check Syntax] E --> E2[Run with -vvv] E --> E3[Check Variables] style A fill:#f96,stroke:#333,stroke-width:2px style B fill:#69f,stroke:#333,stroke-width:2px Debugging Tips Verbose Output ansible-playbook playbook.yml -vvv Check Syntax ansible-playbook playbook.yml --syntax-check Dry Run ansible-playbook playbook.yml --check Best Practices Role Organization Use roles for reusable configurations Keep roles focused and modular Use Galaxy for common tasks Variable Management Use group_vars and host_vars Encrypt sensitive data with Vault Use meaningful variable names Playbook Structure Keep playbooks simple and focused Use tags for selective execution Document roles and variables Security Use SSH keys instead of passwords Regularly rotate credentials Limit sudo access to required commands Conclusion This guide covered the essential aspects of using Ansible for automation, from basic setup to advanced integration with GitHub Actions and Harbor. For more information, refer to: Ansible Documentation Ansible Galaxy GitHub Actions Documentation Next Steps Consider exploring more advanced topics like: - Dynamic inventory management - Custom module development - Advanced role development - Integration with other CI/CD tools","title":"Ansible Guide"},{"location":"documentation/infrastructure/ansible-guide/#ansible-automation-at-scale","text":"Guide Information Difficulty : Intermediate Time Required : ~45 minutes for setup Last Updated : March 2025 Ansible Version : 9.2.0 Ansible Galaxy Version : 4.5.0 Python Version : 3.12 This comprehensive guide covers Ansible installation, configuration, and practical use cases for DevOps automation. You'll learn how to use Ansible for server preparation, package management, and integrating with GitHub Actions for automated deployments.","title":"Ansible: Automation at Scale"},{"location":"documentation/infrastructure/ansible-guide/#prerequisites","text":"Before starting, ensure you have: - Python 3.12 or newer installed - Access to target servers (SSH access) - Sudo privileges on both control and target machines - Basic understanding of YAML syntax - GitHub account (for GitHub Actions integration)","title":"Prerequisites"},{"location":"documentation/infrastructure/ansible-guide/#installation","text":"","title":"Installation"},{"location":"documentation/infrastructure/ansible-guide/#installing-ansible","text":"# Update package list sudo apt update # Install Python 3 and pip sudo apt install -y python3 python3-pip # Install Ansible using pip python3 -m pip install --user ansible == 9 .2.0 # Verify installation ansible --version","title":"Installing Ansible"},{"location":"documentation/infrastructure/ansible-guide/#installing-ansible-galaxy","text":"# Install Ansible Galaxy python3 -m pip install --user ansible-galaxy # Verify installation ansible-galaxy --version","title":"Installing Ansible Galaxy"},{"location":"documentation/infrastructure/ansible-guide/#basic-configuration","text":"","title":"Basic Configuration"},{"location":"documentation/infrastructure/ansible-guide/#1-configure-inventory","text":"Create an inventory file at /etc/ansible/hosts or in your project directory: [webservers] web1 ansible_host = 192.168.1.101 web2 ansible_host = 192.168.1.102 [dbservers] db1 ansible_host = 192.168.1.201 [kubernetes_nodes] master ansible_host = 192.168.1.10 worker1 ansible_host = 192.168.1.11 worker2 ansible_host = 192.168.1.12 [all:vars] ansible_python_interpreter = /usr/bin/python3 ansible_user = your_ssh_user","title":"1. Configure Inventory"},{"location":"documentation/infrastructure/ansible-guide/#2-configure-ssh-keys","text":"# Generate SSH key ssh-keygen -t ed25519 -C \"ansible\" # Copy key to remote servers ssh-copy-id -i ~/.ssh/ansible your_user@remote_host","title":"2. Configure SSH Keys"},{"location":"documentation/infrastructure/ansible-guide/#common-tasks-and-playbooks","text":"","title":"Common Tasks and Playbooks"},{"location":"documentation/infrastructure/ansible-guide/#server-preparation-playbook","text":"# server_prep.yml --- - name : Prepare servers for Kubernetes hosts : kubernetes_nodes become : yes tasks : - name : Update apt cache apt : update_cache : yes cache_valid_time : 3600 - name : Install common packages apt : name : - neovim - vim - docker.io - docker-compose - ncdu - gdu - logrotate - nginx - certbot - python3-certbot-nginx state : present - name : Configure UFW ufw : rule : allow port : \"{{ item }}\" proto : tcp loop : - 22 - 80 - 443 - 6443 # Kubernetes API - 2379 # etcd - 2380 # etcd peer - 10250 # Kubelet API - name : Enable UFW ufw : state : enabled policy : deny - name : Configure logrotate template : src : templates/logrotate.j2 dest : /etc/logrotate.d/custom mode : '0644'","title":"Server Preparation Playbook"},{"location":"documentation/infrastructure/ansible-guide/#docker-deployment-playbook","text":"# docker_deploy.yml --- - name : Deploy Docker Applications hosts : webservers become : yes vars : harbor_registry : harbor.example.com harbor_project : myproject harbor_username : \"{{ lookup('env', 'HARBOR_USERNAME') }}\" harbor_password : \"{{ lookup('env', 'HARBOR_PASSWORD') }}\" tasks : - name : Log into Harbor registry docker_login : registry_url : \"{{ harbor_registry }}\" username : \"{{ harbor_username }}\" password : \"{{ harbor_password }}\" reauthorize : yes - name : Create Docker Compose directory file : path : /opt/docker-compose state : directory mode : '0755' - name : Copy Docker Compose file template : src : templates/docker-compose.yml.j2 dest : /opt/docker-compose/docker-compose.yml mode : '0644' - name : Deploy using Docker Compose docker_compose : project_src : /opt/docker-compose pull : yes recreate : always","title":"Docker Deployment Playbook"},{"location":"documentation/infrastructure/ansible-guide/#github-actions-integration","text":"","title":"GitHub Actions Integration"},{"location":"documentation/infrastructure/ansible-guide/#workflow-example","text":"# .github/workflows/ansible-deploy.yml name : Build and Deploy with Ansible on : push : branches : [ main ] workflow_dispatch : jobs : build-and-deploy : runs-on : ubuntu-latest steps : - uses : actions/checkout@v5 - name : Set up Docker Buildx uses : docker/setup-buildx-action@v3 - name : Login to Harbor uses : docker/login-action@v3 with : registry : ${{ secrets.HARBOR_URL }} username : ${{ secrets.HARBOR_USERNAME }} password : ${{ secrets.HARBOR_PASSWORD }} - name : Build and push uses : docker/build-push-action@v5 with : push : true tags : ${{ secrets.HARBOR_URL }}/${{ secrets.HARBOR_PROJECT }}/myapp:${{ github.sha }} - name : Install Ansible run : | python -m pip install --user ansible==9.2.0 python -m pip install --user ansible-galaxy - name : Set up SSH key run : | mkdir -p ~/.ssh echo \"${{ secrets.SSH_PRIVATE_KEY }}\" > ~/.ssh/id_ed25519 chmod 600 ~/.ssh/id_ed25519 ssh-keyscan -H ${{ secrets.SERVER_HOST }} >> ~/.ssh/known_hosts - name : Run Ansible playbook env : HARBOR_USERNAME : ${{ secrets.HARBOR_USERNAME }} HARBOR_PASSWORD : ${{ secrets.HARBOR_PASSWORD }} run : | ansible-playbook -i inventory.yml docker_deploy.yml","title":"Workflow Example"},{"location":"documentation/infrastructure/ansible-guide/#environment-variables-management","text":"","title":"Environment Variables Management"},{"location":"documentation/infrastructure/ansible-guide/#using-ansible-vault","text":"# Create encrypted variables file ansible-vault create vars/secrets.yml # Edit encrypted file ansible-vault edit vars/secrets.yml # Run playbook with vault ansible-playbook playbook.yml --ask-vault-pass Example encrypted variables: # vars/secrets.yml (encrypted) harbor_credentials : username : myuser password : mypassword db_credentials : user : dbuser password : dbpassword","title":"Using Ansible Vault"},{"location":"documentation/infrastructure/ansible-guide/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"documentation/infrastructure/ansible-guide/#common-issues-and-solutions","text":"flowchart TD A[Issue Detected] --> B{Error Type} B -->|Connection| C[SSH Issues] B -->|Permission| D[Sudo Access] B -->|Task| E[Playbook Error] C --> C1[Check SSH Keys] C --> C2[Verify Connectivity] C --> C3[Check Inventory] D --> D1[Check sudoers] D --> D2[Verify become:] D --> D3[Check Privileges] E --> E1[Check Syntax] E --> E2[Run with -vvv] E --> E3[Check Variables] style A fill:#f96,stroke:#333,stroke-width:2px style B fill:#69f,stroke:#333,stroke-width:2px","title":"Common Issues and Solutions"},{"location":"documentation/infrastructure/ansible-guide/#debugging-tips","text":"Verbose Output ansible-playbook playbook.yml -vvv Check Syntax ansible-playbook playbook.yml --syntax-check Dry Run ansible-playbook playbook.yml --check","title":"Debugging Tips"},{"location":"documentation/infrastructure/ansible-guide/#best-practices","text":"Role Organization Use roles for reusable configurations Keep roles focused and modular Use Galaxy for common tasks Variable Management Use group_vars and host_vars Encrypt sensitive data with Vault Use meaningful variable names Playbook Structure Keep playbooks simple and focused Use tags for selective execution Document roles and variables Security Use SSH keys instead of passwords Regularly rotate credentials Limit sudo access to required commands","title":"Best Practices"},{"location":"documentation/infrastructure/ansible-guide/#conclusion","text":"This guide covered the essential aspects of using Ansible for automation, from basic setup to advanced integration with GitHub Actions and Harbor. For more information, refer to: Ansible Documentation Ansible Galaxy GitHub Actions Documentation Next Steps Consider exploring more advanced topics like: - Dynamic inventory management - Custom module development - Advanced role development - Integration with other CI/CD tools","title":"Conclusion"},{"location":"documentation/kubernetes/k9s-guide/","text":"K9s: Terminal UI for Kubernetes K9s is a powerful terminal-based UI for interacting with Kubernetes clusters. It provides an intuitive interface to navigate, observe, and manage your applications and resources. Why Use K9s? K9s offers several advantages over traditional kubectl commands: Real-time Monitoring : Live updates of cluster resources Intuitive Navigation : Easy keyboard shortcuts and commands Resource Management : Quick access to logs, shell, and resource editing Cluster Context Switching : Seamless switching between multiple clusters Resource Filtering : Powerful search and filtering capabilities Plugin System : Extensible with custom plugins Resource Metrics : CPU/Memory usage visualization Installation Using Package Managers Debian/Ubuntu RedHat/Fedora Arch Linux # Using apt (Recommended) curl -fsSL https://pkgs.k9s.dev/key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/k9s-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/k9s-archive-keyring.gpg] https://pkgs.k9s.dev/deb stable main\" | sudo tee /etc/apt/sources.list.d/k9s.list sudo apt update sudo apt install k9s # Using snap sudo snap install k9s # Using dnf sudo dnf config-manager --add-repo https://pkgs.k9s.dev/rpm/k9s.repo sudo dnf install k9s # Alternative: Direct RPM installation VERSION = $( curl -s https://api.github.com/repos/derailed/k9s/releases/latest | grep tag_name | cut -d '\"' -f 4 ) curl -LO https://github.com/derailed/k9s/releases/download/ ${ VERSION } /k9s_Linux_amd64.rpm sudo rpm -ivh k9s_Linux_amd64.rpm # Using pacman sudo pacman -S k9s # Using yay (AUR) yay -S k9s-bin Using Binary Release # Download latest binary VERSION = $( curl -s https://api.github.com/repos/derailed/k9s/releases/latest | grep tag_name | cut -d '\"' -f 4 ) curl -LO https://github.com/derailed/k9s/releases/download/ ${ VERSION } /k9s_Linux_amd64.tar.gz # Extract and install tar xzf k9s_Linux_amd64.tar.gz sudo install -o root -g root -m 0755 k9s /usr/local/bin/k9s # Verify installation k9s version Using Container Image # Run using Docker docker run --rm -it -v ~/.kube/config:/root/.kube/config ghcr.io/derailed/k9s # Run using Podman podman run --rm -it -v ~/.kube/config:/root/.kube/config ghcr.io/derailed/k9s Installation Best Practices Always verify GPG keys and signatures when installing via package managers Keep K9s updated to benefit from the latest features and security fixes Consider using the container image in restricted environments Back up your K9s configuration before upgrading Basic Usage Starting K9s # Start with default context k9s # Start with specific context k9s --context my-cluster # Start in a specific namespace k9s -n kube-system Key Features and Commands Navigation 0-9 : Switch to namespace : : Enter command mode / : Enter filter mode esc : Go back/Clear filter ctrl-a : Show all resources ctrl-d : Delete resource ctrl-k : Kill pod ? : Show help Resource Views # Access different resources using commands :pods # View pods :deployments # View deployments :services # View services :nodes # View nodes :configmaps # View configmaps :secrets # View secrets Pod Management # While in pod view l # View logs s # Shell into container d # Describe pod e # Edit pod y # YAML view ctrl-k # Kill pod Cluster Context Management # Switch contexts :context # List available contexts ctrl-c # Open context switch menu Multi-Cluster Configuration Using Different Kubeconfig Files Default Configuration : # K9s uses default kubeconfig at ~/.kube/config k9s Specific Kubeconfig : # Use a specific kubeconfig file k9s --kubeconfig = /path/to/kubeconfig # Set environment variable export KUBECONFIG = /path/to/kubeconfig k9s Multiple Kubeconfig Files : # Merge multiple kubeconfig files export KUBECONFIG = ~/.kube/config-cluster1:~/.kube/config-cluster2 k9s Creating Cluster-Specific Aliases # Add to your ~/.bashrc or ~/.zshrc alias k9s-prod = \"k9s --kubeconfig=/path/to/prod-kubeconfig\" alias k9s-dev = \"k9s --kubeconfig=/path/to/dev-kubeconfig\" alias k9s-staging = \"k9s --kubeconfig=/path/to/staging-kubeconfig\" Customization Configuration File K9s uses a configuration file located at ~/.k9s/config.yml : k9s : refreshRate : 2 maxConnRetry : 5 enableMouse : false headless : false logoless : false crumbsless : false readOnly : false noIcons : false logger : tail : 100 buffer : 5000 sinceSeconds : 60 fullScreenLogs : false textWrap : false showTime : false currentContext : minikube currentCluster : minikube clusters : minikube : namespace : active : default favorites : - default - kube-system view : active : pods featureGates : nodeShell : false shellPod : image : busybox:1.31 command : [] args : [] namespace : default Skin Customization Create a custom skin at ~/.k9s/skin.yml : k9s : body : fgColor : dodgerblue bgColor : black logoColor : orange prompt : fgColor : cadetblue bgColor : black suggestColor : dodgerblue info : fgColor : orange sectionColor : white Best Practices 1. Resource Organization Use namespaces as logical groupings for resources Create custom resource views for different workflows Utilize labels and filters for efficient resource management Set up port-forwarding for frequently accessed services 2. Performance Optimization Increase refresh rate in large clusters (>1000 pods) Filter by namespace when working in specific areas Use readonly mode for monitoring to reduce API calls Limit resource views to relevant namespaces 3. Security Considerations Maintain separate kubeconfig files for each environment Enable readonly mode for audit and review tasks Follow the principle of least privilege with RBAC Regularly rotate cluster credentials Use context-specific configurations 4. Workflow Efficiency Master essential keyboard shortcuts ( : , / , ctrl+a ) Configure custom skins for better visibility Set up command aliases for common operations Use hotkeys for frequent namespace switches Configure persistent views for regular tasks 5. Cluster Management Monitor resource utilization with built-in metrics Set up custom resource views for different roles Use benchmarks for performance monitoring Configure alerts for critical resources 6. Troubleshooting Enable logging for debugging sessions Use describe and events for detailed resource info Set up custom views for error tracking Configure proper log buffer sizes Troubleshooting Common Issues Connection Problems : # Check kubeconfig k9s info # Verify cluster access kubectl cluster-info Performance Issues : # Adjust refresh rate in ~/.k9s/config.yml k9s : refreshRate : 5 # Increase refresh interval Resource Access : # Check RBAC permissions kubectl auth can-i --list Additional Resources Official K9s Documentation GitHub Repository K9s Release Notes K9s Plugins","title":"K9s CLI Guide"},{"location":"documentation/kubernetes/k9s-guide/#k9s-terminal-ui-for-kubernetes","text":"K9s is a powerful terminal-based UI for interacting with Kubernetes clusters. It provides an intuitive interface to navigate, observe, and manage your applications and resources.","title":"K9s: Terminal UI for Kubernetes"},{"location":"documentation/kubernetes/k9s-guide/#why-use-k9s","text":"K9s offers several advantages over traditional kubectl commands: Real-time Monitoring : Live updates of cluster resources Intuitive Navigation : Easy keyboard shortcuts and commands Resource Management : Quick access to logs, shell, and resource editing Cluster Context Switching : Seamless switching between multiple clusters Resource Filtering : Powerful search and filtering capabilities Plugin System : Extensible with custom plugins Resource Metrics : CPU/Memory usage visualization","title":"Why Use K9s?"},{"location":"documentation/kubernetes/k9s-guide/#installation","text":"","title":"Installation"},{"location":"documentation/kubernetes/k9s-guide/#using-package-managers","text":"Debian/Ubuntu RedHat/Fedora Arch Linux # Using apt (Recommended) curl -fsSL https://pkgs.k9s.dev/key.gpg | sudo gpg --dearmor -o /usr/share/keyrings/k9s-archive-keyring.gpg echo \"deb [signed-by=/usr/share/keyrings/k9s-archive-keyring.gpg] https://pkgs.k9s.dev/deb stable main\" | sudo tee /etc/apt/sources.list.d/k9s.list sudo apt update sudo apt install k9s # Using snap sudo snap install k9s # Using dnf sudo dnf config-manager --add-repo https://pkgs.k9s.dev/rpm/k9s.repo sudo dnf install k9s # Alternative: Direct RPM installation VERSION = $( curl -s https://api.github.com/repos/derailed/k9s/releases/latest | grep tag_name | cut -d '\"' -f 4 ) curl -LO https://github.com/derailed/k9s/releases/download/ ${ VERSION } /k9s_Linux_amd64.rpm sudo rpm -ivh k9s_Linux_amd64.rpm # Using pacman sudo pacman -S k9s # Using yay (AUR) yay -S k9s-bin","title":"Using Package Managers"},{"location":"documentation/kubernetes/k9s-guide/#using-binary-release","text":"# Download latest binary VERSION = $( curl -s https://api.github.com/repos/derailed/k9s/releases/latest | grep tag_name | cut -d '\"' -f 4 ) curl -LO https://github.com/derailed/k9s/releases/download/ ${ VERSION } /k9s_Linux_amd64.tar.gz # Extract and install tar xzf k9s_Linux_amd64.tar.gz sudo install -o root -g root -m 0755 k9s /usr/local/bin/k9s # Verify installation k9s version","title":"Using Binary Release"},{"location":"documentation/kubernetes/k9s-guide/#using-container-image","text":"# Run using Docker docker run --rm -it -v ~/.kube/config:/root/.kube/config ghcr.io/derailed/k9s # Run using Podman podman run --rm -it -v ~/.kube/config:/root/.kube/config ghcr.io/derailed/k9s Installation Best Practices Always verify GPG keys and signatures when installing via package managers Keep K9s updated to benefit from the latest features and security fixes Consider using the container image in restricted environments Back up your K9s configuration before upgrading","title":"Using Container Image"},{"location":"documentation/kubernetes/k9s-guide/#basic-usage","text":"","title":"Basic Usage"},{"location":"documentation/kubernetes/k9s-guide/#starting-k9s","text":"# Start with default context k9s # Start with specific context k9s --context my-cluster # Start in a specific namespace k9s -n kube-system","title":"Starting K9s"},{"location":"documentation/kubernetes/k9s-guide/#key-features-and-commands","text":"","title":"Key Features and Commands"},{"location":"documentation/kubernetes/k9s-guide/#navigation","text":"0-9 : Switch to namespace : : Enter command mode / : Enter filter mode esc : Go back/Clear filter ctrl-a : Show all resources ctrl-d : Delete resource ctrl-k : Kill pod ? : Show help","title":"Navigation"},{"location":"documentation/kubernetes/k9s-guide/#resource-views","text":"# Access different resources using commands :pods # View pods :deployments # View deployments :services # View services :nodes # View nodes :configmaps # View configmaps :secrets # View secrets","title":"Resource Views"},{"location":"documentation/kubernetes/k9s-guide/#pod-management","text":"# While in pod view l # View logs s # Shell into container d # Describe pod e # Edit pod y # YAML view ctrl-k # Kill pod","title":"Pod Management"},{"location":"documentation/kubernetes/k9s-guide/#cluster-context-management","text":"# Switch contexts :context # List available contexts ctrl-c # Open context switch menu","title":"Cluster Context Management"},{"location":"documentation/kubernetes/k9s-guide/#multi-cluster-configuration","text":"","title":"Multi-Cluster Configuration"},{"location":"documentation/kubernetes/k9s-guide/#using-different-kubeconfig-files","text":"Default Configuration : # K9s uses default kubeconfig at ~/.kube/config k9s Specific Kubeconfig : # Use a specific kubeconfig file k9s --kubeconfig = /path/to/kubeconfig # Set environment variable export KUBECONFIG = /path/to/kubeconfig k9s Multiple Kubeconfig Files : # Merge multiple kubeconfig files export KUBECONFIG = ~/.kube/config-cluster1:~/.kube/config-cluster2 k9s","title":"Using Different Kubeconfig Files"},{"location":"documentation/kubernetes/k9s-guide/#creating-cluster-specific-aliases","text":"# Add to your ~/.bashrc or ~/.zshrc alias k9s-prod = \"k9s --kubeconfig=/path/to/prod-kubeconfig\" alias k9s-dev = \"k9s --kubeconfig=/path/to/dev-kubeconfig\" alias k9s-staging = \"k9s --kubeconfig=/path/to/staging-kubeconfig\"","title":"Creating Cluster-Specific Aliases"},{"location":"documentation/kubernetes/k9s-guide/#customization","text":"","title":"Customization"},{"location":"documentation/kubernetes/k9s-guide/#configuration-file","text":"K9s uses a configuration file located at ~/.k9s/config.yml : k9s : refreshRate : 2 maxConnRetry : 5 enableMouse : false headless : false logoless : false crumbsless : false readOnly : false noIcons : false logger : tail : 100 buffer : 5000 sinceSeconds : 60 fullScreenLogs : false textWrap : false showTime : false currentContext : minikube currentCluster : minikube clusters : minikube : namespace : active : default favorites : - default - kube-system view : active : pods featureGates : nodeShell : false shellPod : image : busybox:1.31 command : [] args : [] namespace : default","title":"Configuration File"},{"location":"documentation/kubernetes/k9s-guide/#skin-customization","text":"Create a custom skin at ~/.k9s/skin.yml : k9s : body : fgColor : dodgerblue bgColor : black logoColor : orange prompt : fgColor : cadetblue bgColor : black suggestColor : dodgerblue info : fgColor : orange sectionColor : white","title":"Skin Customization"},{"location":"documentation/kubernetes/k9s-guide/#best-practices","text":"","title":"Best Practices"},{"location":"documentation/kubernetes/k9s-guide/#1-resource-organization","text":"Use namespaces as logical groupings for resources Create custom resource views for different workflows Utilize labels and filters for efficient resource management Set up port-forwarding for frequently accessed services","title":"1. Resource Organization"},{"location":"documentation/kubernetes/k9s-guide/#2-performance-optimization","text":"Increase refresh rate in large clusters (>1000 pods) Filter by namespace when working in specific areas Use readonly mode for monitoring to reduce API calls Limit resource views to relevant namespaces","title":"2. Performance Optimization"},{"location":"documentation/kubernetes/k9s-guide/#3-security-considerations","text":"Maintain separate kubeconfig files for each environment Enable readonly mode for audit and review tasks Follow the principle of least privilege with RBAC Regularly rotate cluster credentials Use context-specific configurations","title":"3. Security Considerations"},{"location":"documentation/kubernetes/k9s-guide/#4-workflow-efficiency","text":"Master essential keyboard shortcuts ( : , / , ctrl+a ) Configure custom skins for better visibility Set up command aliases for common operations Use hotkeys for frequent namespace switches Configure persistent views for regular tasks","title":"4. Workflow Efficiency"},{"location":"documentation/kubernetes/k9s-guide/#5-cluster-management","text":"Monitor resource utilization with built-in metrics Set up custom resource views for different roles Use benchmarks for performance monitoring Configure alerts for critical resources","title":"5. Cluster Management"},{"location":"documentation/kubernetes/k9s-guide/#6-troubleshooting","text":"Enable logging for debugging sessions Use describe and events for detailed resource info Set up custom views for error tracking Configure proper log buffer sizes","title":"6. Troubleshooting"},{"location":"documentation/kubernetes/k9s-guide/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"documentation/kubernetes/k9s-guide/#common-issues","text":"Connection Problems : # Check kubeconfig k9s info # Verify cluster access kubectl cluster-info Performance Issues : # Adjust refresh rate in ~/.k9s/config.yml k9s : refreshRate : 5 # Increase refresh interval Resource Access : # Check RBAC permissions kubectl auth can-i --list","title":"Common Issues"},{"location":"documentation/kubernetes/k9s-guide/#additional-resources","text":"Official K9s Documentation GitHub Repository K9s Release Notes K9s Plugins","title":"Additional Resources"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/","text":"Kubectl Command Cheat Sheet This comprehensive cheat sheet provides a quick reference for commonly used kubectl commands, organized by category for easy access. Cluster Management Cluster Information # View cluster information kubectl cluster-info # Check the health status of cluster components kubectl get componentstatuses # View nodes in the cluster kubectl get nodes kubectl get nodes -o wide # Detailed view # Get API versions available kubectl api-versions Context and Configuration # View kubeconfig settings kubectl config view # Show current context kubectl config current-context # List all contexts kubectl config get-contexts # Switch to a different context kubectl config use-context <context-name> Resource Management Pods # List all pods kubectl get pods kubectl get pods -A # across all namespaces kubectl get pods -n <namespace> # in specific namespace # Get detailed pod information kubectl describe pod <pod-name> # Delete a pod kubectl delete pod <pod-name> kubectl delete pod <pod-name> --force # Force deletion # Execute command in pod kubectl exec -it <pod-name> -- /bin/bash Deployments # List deployments kubectl get deployments # Create deployment kubectl create deployment <name> --image = <image> # Scale deployment kubectl scale deployment <name> --replicas = <count> # Update image kubectl set image deployment/<name> <container> = <image> # Rollout commands kubectl rollout status deployment/<name> kubectl rollout history deployment/<name> kubectl rollout undo deployment/<name> Services # List services kubectl get services # Create service kubectl expose deployment <name> --port = <port> --type = <type> # Delete service kubectl delete service <name> Monitoring and Debugging Logs and Events # View pod logs kubectl logs <pod-name> kubectl logs -f <pod-name> # Follow logs kubectl logs <pod-name> -c <container-name> # Specific container # View cluster events kubectl get events kubectl get events --sort-by = '.metadata.creationTimestamp' Resource Usage # Node resource usage kubectl top node # Pod resource usage kubectl top pod kubectl top pod -n <namespace> Troubleshooting # Check pod status kubectl get pod <pod-name> -o yaml # Port forwarding kubectl port-forward <pod-name> <local-port>:<pod-port> # Proxy to cluster kubectl proxy Configuration and Security ConfigMaps and Secrets # Create ConfigMap kubectl create configmap <name> --from-file = <path> kubectl create configmap <name> --from-literal = key = value # Create Secret kubectl create secret generic <name> --from-literal = key = value kubectl create secret tls <name> --cert = <path/to/cert> --key = <path/to/key> RBAC # View roles kubectl get roles kubectl get clusterroles # View role bindings kubectl get rolebindings kubectl get clusterrolebindings Advanced Operations Resource Quotas and Limits # View resource quotas kubectl get resourcequota kubectl describe resourcequota # View limit ranges kubectl get limitrange kubectl describe limitrange Namespace Operations # Create namespace kubectl create namespace <name> # Set default namespace kubectl config set-context --current --namespace = <name> # Delete namespace kubectl delete namespace <name> Tips and Tricks Useful Aliases # Common aliases for kubectl alias k = 'kubectl' alias kg = 'kubectl get' alias kd = 'kubectl describe' alias krm = 'kubectl delete' Output Formatting # Different output formats kubectl get pods -o wide # Additional details kubectl get pods -o yaml # YAML format kubectl get pods -o json # JSON format kubectl get pods -o jsonpath = '{.items[*].metadata.name}' # Custom format Pro Tips Use kubectl explain <resource> to get documentation about resources Use --dry-run=client -o yaml to generate resource YAML without creating it Use labels and selectors for efficient resource management Always use namespaces to organize resources Set up auto-completion for easier command line usage Best Practices Always use resource limits for containers Implement RBAC policies for security Regularly backup etcd data Use readiness and liveness probes Keep your kubectl version in sync with cluster version Additional Resources Official Kubernetes Documentation Kubectl Reference Docs Kubernetes Cheat Sheet","title":"Kubectl Cheat Sheet"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#kubectl-command-cheat-sheet","text":"This comprehensive cheat sheet provides a quick reference for commonly used kubectl commands, organized by category for easy access.","title":"Kubectl Command Cheat Sheet"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#cluster-management","text":"","title":"Cluster Management"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#cluster-information","text":"# View cluster information kubectl cluster-info # Check the health status of cluster components kubectl get componentstatuses # View nodes in the cluster kubectl get nodes kubectl get nodes -o wide # Detailed view # Get API versions available kubectl api-versions","title":"Cluster Information"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#context-and-configuration","text":"# View kubeconfig settings kubectl config view # Show current context kubectl config current-context # List all contexts kubectl config get-contexts # Switch to a different context kubectl config use-context <context-name>","title":"Context and Configuration"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#resource-management","text":"","title":"Resource Management"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#pods","text":"# List all pods kubectl get pods kubectl get pods -A # across all namespaces kubectl get pods -n <namespace> # in specific namespace # Get detailed pod information kubectl describe pod <pod-name> # Delete a pod kubectl delete pod <pod-name> kubectl delete pod <pod-name> --force # Force deletion # Execute command in pod kubectl exec -it <pod-name> -- /bin/bash","title":"Pods"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#deployments","text":"# List deployments kubectl get deployments # Create deployment kubectl create deployment <name> --image = <image> # Scale deployment kubectl scale deployment <name> --replicas = <count> # Update image kubectl set image deployment/<name> <container> = <image> # Rollout commands kubectl rollout status deployment/<name> kubectl rollout history deployment/<name> kubectl rollout undo deployment/<name>","title":"Deployments"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#services","text":"# List services kubectl get services # Create service kubectl expose deployment <name> --port = <port> --type = <type> # Delete service kubectl delete service <name>","title":"Services"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#monitoring-and-debugging","text":"","title":"Monitoring and Debugging"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#logs-and-events","text":"# View pod logs kubectl logs <pod-name> kubectl logs -f <pod-name> # Follow logs kubectl logs <pod-name> -c <container-name> # Specific container # View cluster events kubectl get events kubectl get events --sort-by = '.metadata.creationTimestamp'","title":"Logs and Events"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#resource-usage","text":"# Node resource usage kubectl top node # Pod resource usage kubectl top pod kubectl top pod -n <namespace>","title":"Resource Usage"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#troubleshooting","text":"# Check pod status kubectl get pod <pod-name> -o yaml # Port forwarding kubectl port-forward <pod-name> <local-port>:<pod-port> # Proxy to cluster kubectl proxy","title":"Troubleshooting"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#configuration-and-security","text":"","title":"Configuration and Security"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#configmaps-and-secrets","text":"# Create ConfigMap kubectl create configmap <name> --from-file = <path> kubectl create configmap <name> --from-literal = key = value # Create Secret kubectl create secret generic <name> --from-literal = key = value kubectl create secret tls <name> --cert = <path/to/cert> --key = <path/to/key>","title":"ConfigMaps and Secrets"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#rbac","text":"# View roles kubectl get roles kubectl get clusterroles # View role bindings kubectl get rolebindings kubectl get clusterrolebindings","title":"RBAC"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#advanced-operations","text":"","title":"Advanced Operations"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#resource-quotas-and-limits","text":"# View resource quotas kubectl get resourcequota kubectl describe resourcequota # View limit ranges kubectl get limitrange kubectl describe limitrange","title":"Resource Quotas and Limits"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#namespace-operations","text":"# Create namespace kubectl create namespace <name> # Set default namespace kubectl config set-context --current --namespace = <name> # Delete namespace kubectl delete namespace <name>","title":"Namespace Operations"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#tips-and-tricks","text":"","title":"Tips and Tricks"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#useful-aliases","text":"# Common aliases for kubectl alias k = 'kubectl' alias kg = 'kubectl get' alias kd = 'kubectl describe' alias krm = 'kubectl delete'","title":"Useful Aliases"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#output-formatting","text":"# Different output formats kubectl get pods -o wide # Additional details kubectl get pods -o yaml # YAML format kubectl get pods -o json # JSON format kubectl get pods -o jsonpath = '{.items[*].metadata.name}' # Custom format Pro Tips Use kubectl explain <resource> to get documentation about resources Use --dry-run=client -o yaml to generate resource YAML without creating it Use labels and selectors for efficient resource management Always use namespaces to organize resources Set up auto-completion for easier command line usage Best Practices Always use resource limits for containers Implement RBAC policies for security Regularly backup etcd data Use readiness and liveness probes Keep your kubectl version in sync with cluster version","title":"Output Formatting"},{"location":"documentation/kubernetes/kubectl-cheat-sheet/#additional-resources","text":"Official Kubernetes Documentation Kubectl Reference Docs Kubernetes Cheat Sheet","title":"Additional Resources"},{"location":"documentation/kubernetes/kubernetes-networking/","text":"Understanding Kubernetes Network Backends Introduction Kubernetes networking can be complex, but choosing the right network backend (CNI plugin) is crucial for your cluster's success. This guide helps you understand common networking backends and when to use them. Network Backend Basics graph TD A[Kubernetes Cluster] --> B[Network Backend/CNI] B --> C[Pod-to-Pod Communication] B --> D[Network Policies] B --> E[Service Networking] style B fill:#f96,stroke:#333 What is CNI? Container Network Interface (CNI) is a standard that defines how container networking should be configured. Network backends implement this standard to provide: Pod-to-pod communication Network policy enforcement Service networking External access Common Network Backends 1. Flannel: The Simple Solution Best For Development environments Small clusters Learning Kubernetes Simple requirements Key Features Easy to set up Minimal configuration Low overhead Layer 3 networking Installation kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml When to Use Flannel You're new to Kubernetes You need a simple development environment Your cluster has basic networking needs You want minimal configuration overhead When Not to Use You need network policies You require advanced security features You have performance-critical applications You need cross-cluster networking 2. Calico: The Production Standard Best For Production environments Security-focused deployments Large clusters Multi-tenant environments Key Features Network policy support BGP routing Performance optimized Security controls Installation kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml When to Use Calico You need network policy enforcement You want BGP routing capabilities You have a large-scale deployment You require performance monitoring When Not to Use You need a simple setup for development You have limited resources You don't need advanced networking features 3. Cilium: The Modern Choice Best For Microservices architectures High-performance requirements Security-critical applications Observability needs Key Features eBPF-based networking Application-aware policies Advanced observability Load balancing Installation helm repo add cilium https://helm.cilium.io/ helm install cilium cilium/cilium --namespace kube-system When to Use Cilium You need application-layer visibility You want modern security features You require high performance You need detailed networking metrics When Not to Use You have older kernel versions (<4.9) You need a simple networking solution You have limited system resources Choosing the Right Backend Decision Matrix Requirement Flannel Calico Cilium Ease of Setup \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605 \u2605\u2605 Performance \u2605\u2605\u2605 \u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 Security Features \u2605 \u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 Resource Usage \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605 \u2605\u2605 Enterprise Support \u2605 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605 Learning Curve \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605 \u2605\u2605 Common Scenarios 1. Development Environment Recommendation : Flannel Reasons : - Simple setup - Minimal resources - Quick start - Sufficient for development 2. Production Microservices Recommendation : Cilium Reasons : - Application-aware policies - High performance - Advanced observability - Modern security features 3. Enterprise Cluster Recommendation : Calico Reasons : - Proven track record - Enterprise support - Network policies - BGP routing Network Policy Examples Basic Policy with Calico apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : allow-frontend spec : podSelector : matchLabels : app : backend ingress : - from : - podSelector : matchLabels : app : frontend ports : - protocol : TCP port : 80 Application-Layer Policy with Cilium apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : http-policy spec : endpointSelector : matchLabels : app : api ingress : - fromEndpoints : - matchLabels : app : frontend toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : GET path : \"/api/v1\" Troubleshooting Tips 1. Pod Communication Issues # Test pod connectivity kubectl run test-pod --image = busybox -it --rm -- ping <target-pod-ip> # Check CNI configuration kubectl get pods -n kube-system | grep cni 2. Network Policy Problems # Verify policy kubectl describe networkpolicy <policy-name> # Check CNI logs kubectl logs -n kube-system -l k8s-app = calico-node # For Calico kubectl logs -n kube-system -l k8s-app = cilium # For Cilium 3. Performance Issues # Check CNI pod status kubectl get pods -n kube-system -o wide | grep cni # View metrics (Cilium) cilium status Best Practices 1. General Guidelines Start with Flannel for learning Use Calico for standard production workloads Choose Cilium for modern microservices 2. Production Setup Always enable network policies Monitor CNI pod health Keep CNI version updated Document your network architecture 3. Resource Planning Consider node resources Plan IP address ranges Account for future growth Monitor network performance Additional Resources CNI Specification Flannel Documentation Calico Documentation Cilium Documentation","title":"Kubernetes Networking"},{"location":"documentation/kubernetes/kubernetes-networking/#understanding-kubernetes-network-backends","text":"","title":"Understanding Kubernetes Network Backends"},{"location":"documentation/kubernetes/kubernetes-networking/#introduction","text":"Kubernetes networking can be complex, but choosing the right network backend (CNI plugin) is crucial for your cluster's success. This guide helps you understand common networking backends and when to use them.","title":"Introduction"},{"location":"documentation/kubernetes/kubernetes-networking/#network-backend-basics","text":"graph TD A[Kubernetes Cluster] --> B[Network Backend/CNI] B --> C[Pod-to-Pod Communication] B --> D[Network Policies] B --> E[Service Networking] style B fill:#f96,stroke:#333","title":"Network Backend Basics"},{"location":"documentation/kubernetes/kubernetes-networking/#what-is-cni","text":"Container Network Interface (CNI) is a standard that defines how container networking should be configured. Network backends implement this standard to provide: Pod-to-pod communication Network policy enforcement Service networking External access","title":"What is CNI?"},{"location":"documentation/kubernetes/kubernetes-networking/#common-network-backends","text":"","title":"Common Network Backends"},{"location":"documentation/kubernetes/kubernetes-networking/#1-flannel-the-simple-solution","text":"Best For Development environments Small clusters Learning Kubernetes Simple requirements","title":"1. Flannel: The Simple Solution"},{"location":"documentation/kubernetes/kubernetes-networking/#key-features","text":"Easy to set up Minimal configuration Low overhead Layer 3 networking","title":"Key Features"},{"location":"documentation/kubernetes/kubernetes-networking/#installation","text":"kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml","title":"Installation"},{"location":"documentation/kubernetes/kubernetes-networking/#when-to-use-flannel","text":"You're new to Kubernetes You need a simple development environment Your cluster has basic networking needs You want minimal configuration overhead","title":"When to Use Flannel"},{"location":"documentation/kubernetes/kubernetes-networking/#when-not-to-use","text":"You need network policies You require advanced security features You have performance-critical applications You need cross-cluster networking","title":"When Not to Use"},{"location":"documentation/kubernetes/kubernetes-networking/#2-calico-the-production-standard","text":"Best For Production environments Security-focused deployments Large clusters Multi-tenant environments","title":"2. Calico: The Production Standard"},{"location":"documentation/kubernetes/kubernetes-networking/#key-features_1","text":"Network policy support BGP routing Performance optimized Security controls","title":"Key Features"},{"location":"documentation/kubernetes/kubernetes-networking/#installation_1","text":"kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml","title":"Installation"},{"location":"documentation/kubernetes/kubernetes-networking/#when-to-use-calico","text":"You need network policy enforcement You want BGP routing capabilities You have a large-scale deployment You require performance monitoring","title":"When to Use Calico"},{"location":"documentation/kubernetes/kubernetes-networking/#when-not-to-use_1","text":"You need a simple setup for development You have limited resources You don't need advanced networking features","title":"When Not to Use"},{"location":"documentation/kubernetes/kubernetes-networking/#3-cilium-the-modern-choice","text":"Best For Microservices architectures High-performance requirements Security-critical applications Observability needs","title":"3. Cilium: The Modern Choice"},{"location":"documentation/kubernetes/kubernetes-networking/#key-features_2","text":"eBPF-based networking Application-aware policies Advanced observability Load balancing","title":"Key Features"},{"location":"documentation/kubernetes/kubernetes-networking/#installation_2","text":"helm repo add cilium https://helm.cilium.io/ helm install cilium cilium/cilium --namespace kube-system","title":"Installation"},{"location":"documentation/kubernetes/kubernetes-networking/#when-to-use-cilium","text":"You need application-layer visibility You want modern security features You require high performance You need detailed networking metrics","title":"When to Use Cilium"},{"location":"documentation/kubernetes/kubernetes-networking/#when-not-to-use_2","text":"You have older kernel versions (<4.9) You need a simple networking solution You have limited system resources","title":"When Not to Use"},{"location":"documentation/kubernetes/kubernetes-networking/#choosing-the-right-backend","text":"","title":"Choosing the Right Backend"},{"location":"documentation/kubernetes/kubernetes-networking/#decision-matrix","text":"Requirement Flannel Calico Cilium Ease of Setup \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605 \u2605\u2605 Performance \u2605\u2605\u2605 \u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 Security Features \u2605 \u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605\u2605 Resource Usage \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605 \u2605\u2605 Enterprise Support \u2605 \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605\u2605 Learning Curve \u2605\u2605\u2605\u2605\u2605 \u2605\u2605\u2605 \u2605\u2605","title":"Decision Matrix"},{"location":"documentation/kubernetes/kubernetes-networking/#common-scenarios","text":"","title":"Common Scenarios"},{"location":"documentation/kubernetes/kubernetes-networking/#1-development-environment","text":"Recommendation : Flannel Reasons : - Simple setup - Minimal resources - Quick start - Sufficient for development","title":"1. Development Environment"},{"location":"documentation/kubernetes/kubernetes-networking/#2-production-microservices","text":"Recommendation : Cilium Reasons : - Application-aware policies - High performance - Advanced observability - Modern security features","title":"2. Production Microservices"},{"location":"documentation/kubernetes/kubernetes-networking/#3-enterprise-cluster","text":"Recommendation : Calico Reasons : - Proven track record - Enterprise support - Network policies - BGP routing","title":"3. Enterprise Cluster"},{"location":"documentation/kubernetes/kubernetes-networking/#network-policy-examples","text":"","title":"Network Policy Examples"},{"location":"documentation/kubernetes/kubernetes-networking/#basic-policy-with-calico","text":"apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : allow-frontend spec : podSelector : matchLabels : app : backend ingress : - from : - podSelector : matchLabels : app : frontend ports : - protocol : TCP port : 80","title":"Basic Policy with Calico"},{"location":"documentation/kubernetes/kubernetes-networking/#application-layer-policy-with-cilium","text":"apiVersion : cilium.io/v2 kind : CiliumNetworkPolicy metadata : name : http-policy spec : endpointSelector : matchLabels : app : api ingress : - fromEndpoints : - matchLabels : app : frontend toPorts : - ports : - port : \"80\" protocol : TCP rules : http : - method : GET path : \"/api/v1\"","title":"Application-Layer Policy with Cilium"},{"location":"documentation/kubernetes/kubernetes-networking/#troubleshooting-tips","text":"","title":"Troubleshooting Tips"},{"location":"documentation/kubernetes/kubernetes-networking/#1-pod-communication-issues","text":"# Test pod connectivity kubectl run test-pod --image = busybox -it --rm -- ping <target-pod-ip> # Check CNI configuration kubectl get pods -n kube-system | grep cni","title":"1. Pod Communication Issues"},{"location":"documentation/kubernetes/kubernetes-networking/#2-network-policy-problems","text":"# Verify policy kubectl describe networkpolicy <policy-name> # Check CNI logs kubectl logs -n kube-system -l k8s-app = calico-node # For Calico kubectl logs -n kube-system -l k8s-app = cilium # For Cilium","title":"2. Network Policy Problems"},{"location":"documentation/kubernetes/kubernetes-networking/#3-performance-issues","text":"# Check CNI pod status kubectl get pods -n kube-system -o wide | grep cni # View metrics (Cilium) cilium status","title":"3. Performance Issues"},{"location":"documentation/kubernetes/kubernetes-networking/#best-practices","text":"","title":"Best Practices"},{"location":"documentation/kubernetes/kubernetes-networking/#1-general-guidelines","text":"Start with Flannel for learning Use Calico for standard production workloads Choose Cilium for modern microservices","title":"1. General Guidelines"},{"location":"documentation/kubernetes/kubernetes-networking/#2-production-setup","text":"Always enable network policies Monitor CNI pod health Keep CNI version updated Document your network architecture","title":"2. Production Setup"},{"location":"documentation/kubernetes/kubernetes-networking/#3-resource-planning","text":"Consider node resources Plan IP address ranges Account for future growth Monitor network performance","title":"3. Resource Planning"},{"location":"documentation/kubernetes/kubernetes-networking/#additional-resources","text":"CNI Specification Flannel Documentation Calico Documentation Cilium Documentation","title":"Additional Resources"},{"location":"documentation/linux/","text":"Linux Administration Coming soon! This section will contain guides and tutorials about Linux system administration and best practices.","title":"Linux"},{"location":"documentation/linux/#linux-administration","text":"Coming soon! This section will contain guides and tutorials about Linux system administration and best practices.","title":"Linux Administration"},{"location":"documentation/miscellaneous/","text":"Miscellaneous Coming soon! This section will contain various other DevOps-related guides and tutorials that don't fit into other categories.","title":"Miscellaneous"},{"location":"documentation/miscellaneous/#miscellaneous","text":"Coming soon! This section will contain various other DevOps-related guides and tutorials that don't fit into other categories.","title":"Miscellaneous"},{"location":"documentation/networking/","text":"Networking Coming soon! This section will contain guides and tutorials about networking concepts and configurations.","title":"Networking"},{"location":"documentation/networking/#networking","text":"Coming soon! This section will contain guides and tutorials about networking concepts and configurations.","title":"Networking"},{"location":"documentation/selfhosting/","text":"Self-Hosting Guide Coming soon! This section will contain guides and tutorials about self-hosting various services and applications.","title":"Overview"},{"location":"documentation/selfhosting/#self-hosting-guide","text":"Coming soon! This section will contain guides and tutorials about self-hosting various services and applications.","title":"Self-Hosting Guide"},{"location":"documentation/selfhosting/why-selfhost/","text":"Why Self-Host? A Philosophy of Digital Independence Self-hosting is more than just running services on your own hardware\u2014it's a philosophy of digital independence, privacy, and skill development. This guide explores why you might want to self-host, what benefits it brings, and how to get started on your self-hosting journey. Quick Overview Take control of your digital life Learn valuable technical skills Reduce dependency on third-party services Save money in the long run Why Self-Host? 1. Privacy and Data Sovereignty Control Over Your Data : Your data resides on your own infrastructure. You know exactly where it is, how it's stored, and who has access to it. No Third-Party Access : Your personal information isn't being mined for advertising or analytics by big tech companies. Compliance : Easier compliance with data protection regulations (GDPR, CCPA) as you control the entire data lifecycle. 2. Learning and Skill Development Practical Experience : Linux system administration Network configuration and security Docker and containerization Reverse proxies and SSL certificates Backup strategies and disaster recovery Career Growth : These skills are highly valuable in DevOps and System Administration roles. Understanding Modern Tech : Better understanding of how modern cloud services work \"under the hood\". 3. Customization and Freedom Full Control : Customize every aspect of your services to your exact needs. No Vendor Lock-in : Freedom to switch technologies or migrate services as needed. Feature Independence : Not waiting for providers to implement features you need. 4. Cost Effectiveness Long-term Savings : While initial setup requires investment, running services on your own hardware can be more cost-effective long-term. Resource Optimization : Utilize the same hardware for multiple services. No Subscription Fees : Eliminate monthly fees for various cloud services. What Can You Self-Host? Popular Self-Hosted Services Here are some of the most common services people choose to self-host: Essential Services Communication Email servers Chat platforms (Matrix, XMPP) VoIP solutions Storage and Sync File sync (Nextcloud, Seafile) Password managers (Vaultwarden) Note-taking apps (Joplin Server) Media Media servers (Jellyfin, Plex) Music streaming (Navidrome) Photo galleries (PhotoPrism) Productivity Office suites (Collabora Online) Project management (Wekan) Wiki systems (BookStack) Getting Started Start Your Journey Begin with simple services and gradually expand as you gain confidence. 1. Start Small Begin with a single, simple service Focus on learning the basics: Server setup and maintenance Basic security practices Backup procedures 2. Essential Knowledge Areas Linux Basics Command line navigation Package management User management File permissions Networking DNS configuration Port forwarding Firewall setup Basic network security Docker Container concepts Docker Compose Image management Volume persistence 3. Hardware Considerations Start with : Old PC/laptop Raspberry Pi Small form factor PC Consider : Power consumption Noise levels Reliability Expansion possibilities 4. Security Best Practices Regular system updates Strong password policies Network segmentation Regular backups SSL certificate management Firewall configuration Common Challenges and Solutions Be Prepared Self-hosting comes with responsibilities. Here are common challenges and how to address them: 1. Reliability Challenge : Ensuring 24/7 service availability Solutions : UPS for power backup Monitoring systems Automated restart procedures Regular maintenance schedules 2. Security Challenge : Protecting services from attacks Solutions : Regular security audits VPN for remote access Intrusion detection systems Automated update systems 3. Maintenance Challenge : Managing regular upkeep Solutions : Automation scripts Docker for easy updates Documentation of procedures Monitoring and alerting Resources for Learning Communities r/selfhosted r/homelab Self-hosted Discord communities Linux user groups Documentation Docker Documentation Linux Documentation Service-specific wikis Security best practices guides Conclusion Self-hosting is a journey of learning, independence, and technical growth. While it requires initial investment in time and learning, the benefits of privacy, control, and skill development make it a rewarding endeavor. Start small, focus on security and reliability, and gradually expand your self-hosted infrastructure as your knowledge and confidence grow. Remember The goal isn't to self-host everything immediately, but to thoughtfully choose which services make sense for your needs and gradually build your digital independence.","title":"Why Self-Host?"},{"location":"documentation/selfhosting/why-selfhost/#why-self-host-a-philosophy-of-digital-independence","text":"Self-hosting is more than just running services on your own hardware\u2014it's a philosophy of digital independence, privacy, and skill development. This guide explores why you might want to self-host, what benefits it brings, and how to get started on your self-hosting journey. Quick Overview Take control of your digital life Learn valuable technical skills Reduce dependency on third-party services Save money in the long run","title":"Why Self-Host? A Philosophy of Digital Independence"},{"location":"documentation/selfhosting/why-selfhost/#why-self-host","text":"","title":"Why Self-Host?"},{"location":"documentation/selfhosting/why-selfhost/#1-privacy-and-data-sovereignty","text":"Control Over Your Data : Your data resides on your own infrastructure. You know exactly where it is, how it's stored, and who has access to it. No Third-Party Access : Your personal information isn't being mined for advertising or analytics by big tech companies. Compliance : Easier compliance with data protection regulations (GDPR, CCPA) as you control the entire data lifecycle.","title":"1. Privacy and Data Sovereignty"},{"location":"documentation/selfhosting/why-selfhost/#2-learning-and-skill-development","text":"Practical Experience : Linux system administration Network configuration and security Docker and containerization Reverse proxies and SSL certificates Backup strategies and disaster recovery Career Growth : These skills are highly valuable in DevOps and System Administration roles. Understanding Modern Tech : Better understanding of how modern cloud services work \"under the hood\".","title":"2. Learning and Skill Development"},{"location":"documentation/selfhosting/why-selfhost/#3-customization-and-freedom","text":"Full Control : Customize every aspect of your services to your exact needs. No Vendor Lock-in : Freedom to switch technologies or migrate services as needed. Feature Independence : Not waiting for providers to implement features you need.","title":"3. Customization and Freedom"},{"location":"documentation/selfhosting/why-selfhost/#4-cost-effectiveness","text":"Long-term Savings : While initial setup requires investment, running services on your own hardware can be more cost-effective long-term. Resource Optimization : Utilize the same hardware for multiple services. No Subscription Fees : Eliminate monthly fees for various cloud services.","title":"4. Cost Effectiveness"},{"location":"documentation/selfhosting/why-selfhost/#what-can-you-self-host","text":"Popular Self-Hosted Services Here are some of the most common services people choose to self-host:","title":"What Can You Self-Host?"},{"location":"documentation/selfhosting/why-selfhost/#essential-services","text":"Communication Email servers Chat platforms (Matrix, XMPP) VoIP solutions Storage and Sync File sync (Nextcloud, Seafile) Password managers (Vaultwarden) Note-taking apps (Joplin Server) Media Media servers (Jellyfin, Plex) Music streaming (Navidrome) Photo galleries (PhotoPrism) Productivity Office suites (Collabora Online) Project management (Wekan) Wiki systems (BookStack)","title":"Essential Services"},{"location":"documentation/selfhosting/why-selfhost/#getting-started","text":"Start Your Journey Begin with simple services and gradually expand as you gain confidence.","title":"Getting Started"},{"location":"documentation/selfhosting/why-selfhost/#1-start-small","text":"Begin with a single, simple service Focus on learning the basics: Server setup and maintenance Basic security practices Backup procedures","title":"1. Start Small"},{"location":"documentation/selfhosting/why-selfhost/#2-essential-knowledge-areas","text":"Linux Basics Command line navigation Package management User management File permissions Networking DNS configuration Port forwarding Firewall setup Basic network security Docker Container concepts Docker Compose Image management Volume persistence","title":"2. Essential Knowledge Areas"},{"location":"documentation/selfhosting/why-selfhost/#3-hardware-considerations","text":"Start with : Old PC/laptop Raspberry Pi Small form factor PC Consider : Power consumption Noise levels Reliability Expansion possibilities","title":"3. Hardware Considerations"},{"location":"documentation/selfhosting/why-selfhost/#4-security-best-practices","text":"Regular system updates Strong password policies Network segmentation Regular backups SSL certificate management Firewall configuration","title":"4. Security Best Practices"},{"location":"documentation/selfhosting/why-selfhost/#common-challenges-and-solutions","text":"Be Prepared Self-hosting comes with responsibilities. Here are common challenges and how to address them:","title":"Common Challenges and Solutions"},{"location":"documentation/selfhosting/why-selfhost/#1-reliability","text":"Challenge : Ensuring 24/7 service availability Solutions : UPS for power backup Monitoring systems Automated restart procedures Regular maintenance schedules","title":"1. Reliability"},{"location":"documentation/selfhosting/why-selfhost/#2-security","text":"Challenge : Protecting services from attacks Solutions : Regular security audits VPN for remote access Intrusion detection systems Automated update systems","title":"2. Security"},{"location":"documentation/selfhosting/why-selfhost/#3-maintenance","text":"Challenge : Managing regular upkeep Solutions : Automation scripts Docker for easy updates Documentation of procedures Monitoring and alerting","title":"3. Maintenance"},{"location":"documentation/selfhosting/why-selfhost/#resources-for-learning","text":"","title":"Resources for Learning"},{"location":"documentation/selfhosting/why-selfhost/#communities","text":"r/selfhosted r/homelab Self-hosted Discord communities Linux user groups","title":"Communities"},{"location":"documentation/selfhosting/why-selfhost/#documentation","text":"Docker Documentation Linux Documentation Service-specific wikis Security best practices guides","title":"Documentation"},{"location":"documentation/selfhosting/why-selfhost/#conclusion","text":"Self-hosting is a journey of learning, independence, and technical growth. While it requires initial investment in time and learning, the benefits of privacy, control, and skill development make it a rewarding endeavor. Start small, focus on security and reliability, and gradually expand your self-hosted infrastructure as your knowledge and confidence grow. Remember The goal isn't to self-host everything immediately, but to thoughtfully choose which services make sense for your needs and gradually build your digital independence.","title":"Conclusion"}]}