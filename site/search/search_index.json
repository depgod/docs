{"config":{"indexing":"full","lang":["en"],"min_search_length":2,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Welcome to My DevOps Journey \u00b6 About Me I'm a DevOps Architect and Systems Administrator based in New Delhi, India, with a strong background in Kubernetes, CI/CD, and infrastructure management. My expertise lies in building and maintaining scalable, secure, and automated cloud-native environments. Professional Focus \u00b6 graph TD A[DevOps] --> B[Containers] A --> C[Infrastructure] A --> D[Automation] A --> E[Security] A --> F[Monitoring] B --> B1[Kubernetes] B --> B2[Docker] B --> B3[Container Security] C --> C1[High Availability] C --> C2[Distributed Storage] C --> C3[Load Balancing] D --> D1[CI/CD Pipelines] D --> D2[IaC] D --> D3[GitOps] E --> E1[SSO Integration] E --> E2[Certificate Management] E --> E3[Access Control] F --> F1[Prometheus] F --> F2[Grafana] F --> F3[UptimeKuma] style A fill:#f9f,stroke:#333,stroke-width:2px style B fill:#bbf,stroke:#333 style C fill:#bbf,stroke:#333 style D fill:#bbf,stroke:#333 style E fill:#bbf,stroke:#333 style F fill:#bbf,stroke:#333 Core Competencies \u00b6 Infrastructure & Orchestration DevOps & Automation Security & Networking \u26f4\ufe0f Kubernetes Cluster Management High Availability setups Custom resource management Multi-environment deployments \ud83d\udd04 Container Orchestration Docker containerization Harbor private registry Image security scanning \ud83d\udcbe Storage Solutions Distributed storage (Longhorn) HA database clusters Backup management \ud83d\udd04 CI/CD Implementation GitHub Actions ArgoCD Automated deployments \ud83d\udee0\ufe0f Infrastructure as Code Ansible automation Configuration management Version control \ud83d\udd0d Monitoring & Logging Prometheus metrics Grafana dashboards Log aggregation \ud83d\udd12 Security Management SSL/TLS implementation SSO integration Access control \ud83c\udf10 Network Administration Traefik ingress control DNS management Cloudflare integration Current Projects \u00b6 HomeLab Environment A sophisticated home laboratory environment featuring: K3s Kubernetes cluster (1 master, 2 workers) Self-hosted services: Documentation server Nextcloud Bitwarden Jellyfin media server Secured with: Cloudflare tunnels SSO authentication Custom domain management Technical Stack \u00b6 Primary Technologies \u00b6 Category Technologies Container Platforms Kubernetes, Docker, Proxmox CI/CD Tools GitHub Actions, ArgoCD, Harbor Infrastructure Ansible, Terraform Databases PostgreSQL, MariaDB, Redis Monitoring Grafana, Prometheus, UptimeKuma Web Servers Nginx, Traefik Programming Python, Bash Version Control Git, GitHub Cloud & Hosting Experience \u00b6 Digital Ocean AWS Hetzner Traditional hosting (Plesk, cPanel) Areas of Interest \u00b6 I'm particularly passionate about: \ud83c\udfd7\ufe0f Building scalable infrastructure \ud83d\udd10 Implementing security best practices \ud83d\udcda Documentation and knowledge sharing \ud83e\udd1d Mentoring and team collaboration \ud83d\udd04 Process automation and optimization Let's Connect \u00b6 Feel free to reach out if you want to: Discuss DevOps practices Collaborate on projects Share knowledge and experiences Explore infrastructure solutions You can find my detailed technical documentation under the Documentation section, or connect with me on LinkedIn . \ud83c\udf10 darkmode.dev","title":"Welcome"},{"location":"#welcome-to-my-devops-journey","text":"About Me I'm a DevOps Architect and Systems Administrator based in New Delhi, India, with a strong background in Kubernetes, CI/CD, and infrastructure management. My expertise lies in building and maintaining scalable, secure, and automated cloud-native environments.","title":"Welcome to My DevOps Journey"},{"location":"#professional-focus","text":"graph TD A[DevOps] --> B[Containers] A --> C[Infrastructure] A --> D[Automation] A --> E[Security] A --> F[Monitoring] B --> B1[Kubernetes] B --> B2[Docker] B --> B3[Container Security] C --> C1[High Availability] C --> C2[Distributed Storage] C --> C3[Load Balancing] D --> D1[CI/CD Pipelines] D --> D2[IaC] D --> D3[GitOps] E --> E1[SSO Integration] E --> E2[Certificate Management] E --> E3[Access Control] F --> F1[Prometheus] F --> F2[Grafana] F --> F3[UptimeKuma] style A fill:#f9f,stroke:#333,stroke-width:2px style B fill:#bbf,stroke:#333 style C fill:#bbf,stroke:#333 style D fill:#bbf,stroke:#333 style E fill:#bbf,stroke:#333 style F fill:#bbf,stroke:#333","title":"Professional Focus"},{"location":"#core-competencies","text":"Infrastructure & Orchestration DevOps & Automation Security & Networking \u26f4\ufe0f Kubernetes Cluster Management High Availability setups Custom resource management Multi-environment deployments \ud83d\udd04 Container Orchestration Docker containerization Harbor private registry Image security scanning \ud83d\udcbe Storage Solutions Distributed storage (Longhorn) HA database clusters Backup management \ud83d\udd04 CI/CD Implementation GitHub Actions ArgoCD Automated deployments \ud83d\udee0\ufe0f Infrastructure as Code Ansible automation Configuration management Version control \ud83d\udd0d Monitoring & Logging Prometheus metrics Grafana dashboards Log aggregation \ud83d\udd12 Security Management SSL/TLS implementation SSO integration Access control \ud83c\udf10 Network Administration Traefik ingress control DNS management Cloudflare integration","title":"Core Competencies"},{"location":"#current-projects","text":"HomeLab Environment A sophisticated home laboratory environment featuring: K3s Kubernetes cluster (1 master, 2 workers) Self-hosted services: Documentation server Nextcloud Bitwarden Jellyfin media server Secured with: Cloudflare tunnels SSO authentication Custom domain management","title":"Current Projects"},{"location":"#technical-stack","text":"","title":"Technical Stack"},{"location":"#primary-technologies","text":"Category Technologies Container Platforms Kubernetes, Docker, Proxmox CI/CD Tools GitHub Actions, ArgoCD, Harbor Infrastructure Ansible, Terraform Databases PostgreSQL, MariaDB, Redis Monitoring Grafana, Prometheus, UptimeKuma Web Servers Nginx, Traefik Programming Python, Bash Version Control Git, GitHub","title":"Primary Technologies"},{"location":"#cloud-hosting-experience","text":"Digital Ocean AWS Hetzner Traditional hosting (Plesk, cPanel)","title":"Cloud &amp; Hosting Experience"},{"location":"#areas-of-interest","text":"I'm particularly passionate about: \ud83c\udfd7\ufe0f Building scalable infrastructure \ud83d\udd10 Implementing security best practices \ud83d\udcda Documentation and knowledge sharing \ud83e\udd1d Mentoring and team collaboration \ud83d\udd04 Process automation and optimization","title":"Areas of Interest"},{"location":"#lets-connect","text":"Feel free to reach out if you want to: Discuss DevOps practices Collaborate on projects Share knowledge and experiences Explore infrastructure solutions You can find my detailed technical documentation under the Documentation section, or connect with me on LinkedIn . \ud83c\udf10 darkmode.dev","title":"Let's Connect"},{"location":"about/","text":"About Me \u00b6 Welcome to My DevOps Documentation \u00b6 I'm a DevOps enthusiast passionate about automation, infrastructure as code, and continuous integration/deployment practices. This documentation site serves as both a personal knowledge base and a resource for others in the DevOps community. What I Do \u00b6 Infrastructure Automation : Expertise in tools like Terraform, Ansible, and CloudFormation Container Orchestration : Working with Docker and Kubernetes CI/CD Implementation : Setting up and optimizing deployment pipelines Cloud Architecture : Experience with major cloud providers (AWS, GCP, Azure) Purpose of This Site \u00b6 This documentation site aims to: Share knowledge and best practices in DevOps Provide practical guides and tutorials Document common solutions to technical challenges Create a reliable reference for DevOps tools and practices Skills & Expertise \u00b6 Languages : Python, Bash, Go Infrastructure : Docker, Kubernetes, Terraform CI/CD : Jenkins, GitHub Actions, GitLab CI Monitoring : Prometheus, Grafana Cloud Platforms : AWS, GCP, Azure Get in Touch \u00b6 Feel free to reach out if you: - Have questions about any of the documentation - Want to contribute to the content - Need consulting or professional advice - Just want to connect and discuss DevOps You can find me on GitHub or through the contact page.","title":"About Me"},{"location":"about/#about-me","text":"","title":"About Me"},{"location":"about/#welcome-to-my-devops-documentation","text":"I'm a DevOps enthusiast passionate about automation, infrastructure as code, and continuous integration/deployment practices. This documentation site serves as both a personal knowledge base and a resource for others in the DevOps community.","title":"Welcome to My DevOps Documentation"},{"location":"about/#what-i-do","text":"Infrastructure Automation : Expertise in tools like Terraform, Ansible, and CloudFormation Container Orchestration : Working with Docker and Kubernetes CI/CD Implementation : Setting up and optimizing deployment pipelines Cloud Architecture : Experience with major cloud providers (AWS, GCP, Azure)","title":"What I Do"},{"location":"about/#purpose-of-this-site","text":"This documentation site aims to: Share knowledge and best practices in DevOps Provide practical guides and tutorials Document common solutions to technical challenges Create a reliable reference for DevOps tools and practices","title":"Purpose of This Site"},{"location":"about/#skills-expertise","text":"Languages : Python, Bash, Go Infrastructure : Docker, Kubernetes, Terraform CI/CD : Jenkins, GitHub Actions, GitLab CI Monitoring : Prometheus, Grafana Cloud Platforms : AWS, GCP, Azure","title":"Skills &amp; Expertise"},{"location":"about/#get-in-touch","text":"Feel free to reach out if you: - Have questions about any of the documentation - Want to contribute to the content - Need consulting or professional advice - Just want to connect and discuss DevOps You can find me on GitHub or through the contact page.","title":"Get in Touch"},{"location":"argocd-setup/","text":"ArgoCD Setup Guide for Kubernetes \u00b6 Overview \u00b6 Guide Information Difficulty : Intermediate Time Required : ~45 minutes Last Updated : March 2024 ArgoCD Version : v2.9.3 Kubernetes Compatibility : K3s, K8s 1.24+ OS : Debian 12 This guide provides comprehensive instructions for setting up ArgoCD in a Kubernetes or K3s cluster, configuring Traefik ingress, and securing it with cert-manager for automatic SSL certificate renewal. What is ArgoCD? \u00b6 ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It automates the deployment of applications to Kubernetes clusters by monitoring Git repositories and applying changes when they occur. graph LR A[Git Repository] -->|Contains manifests| B[ArgoCD] B -->|Syncs to| C[Kubernetes Cluster] D[Developers] -->|Push changes| A E[ArgoCD UI/CLI] -->|Manage| B style B fill:#f9f,stroke:#333,stroke-width:2px Prerequisites \u00b6 Requirements A running Kubernetes or K3s cluster kubectl installed and configured helm v3.x installed A domain name for ArgoCD access DNS record pointing to your cluster's IP Administrative access to your cluster Installation Steps \u00b6 1. Prepare Your Cluster \u00b6 For Standard Kubernetes For K3s # Verify cluster access kubectl cluster-info # Create namespace for ArgoCD kubectl create namespace argocd # Verify K3s is running sudo systemctl status k3s # Create namespace for ArgoCD kubectl create namespace argocd 2. Install ArgoCD \u00b6 There are two methods to install ArgoCD: using manifests directly or using Helm. Using Manifests Using Helm # Apply the ArgoCD installation manifest kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.9.3/manifests/install.yaml # Verify pods are running kubectl get pods -n argocd # Add ArgoCD Helm repository helm repo add argo https://argoproj.github.io/argo-helm helm repo update # Install ArgoCD helm install argocd argo/argo-cd \\ --namespace argocd \\ --create-namespace \\ --version 5 .51.4 \\ --set server.extraArgs = \"{--insecure}\" \\ --set controller.metrics.enabled = true \\ --set server.metrics.enabled = true Resource Requirements ArgoCD is relatively lightweight, but for production use, consider allocating: - At least 2 CPU cores and 4GB RAM for the cluster - 1GB RAM for the ArgoCD controller - 512MB RAM for the ArgoCD server 3. Install cert-manager \u00b6 cert-manager is required to automatically provision and manage TLS certificates. # Add Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io helm repo update # Install cert-manager with CRDs helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.13.2 \\ --set installCRDs = true # Verify cert-manager pods are running kubectl get pods -n cert-manager 4. Configure ClusterIssuer for Let's Encrypt \u00b6 Create a ClusterIssuer to obtain certificates from Let's Encrypt: letsencrypt-issuer.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : server : https://acme-v02.api.letsencrypt.org/directory email : your-email@example.com # Replace with your email privateKeySecretRef : name : letsencrypt-prod solvers : - http01 : ingress : class : traefik Apply the ClusterIssuer: # Apply the ClusterIssuer kubectl apply -f letsencrypt-issuer.yaml # Verify the ClusterIssuer is ready kubectl get clusterissuer letsencrypt-prod -o wide Rate Limits Let's Encrypt has rate limits: 50 certificates per domain per week. Use the staging server ( https://acme-staging-v02.api.letsencrypt.org/directory ) for testing. 5. Configure Traefik Ingress for ArgoCD \u00b6 Create an Ingress resource for ArgoCD: argocd-ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : argocd-server-ingress namespace : argocd annotations : cert-manager.io/cluster-issuer : \"letsencrypt-prod\" traefik.ingress.kubernetes.io/router.entrypoints : \"websecure\" traefik.ingress.kubernetes.io/router.tls : \"true\" traefik.ingress.kubernetes.io/router.middlewares : \"argocd-argocd-middleware@kubernetescrd\" spec : ingressClassName : traefik tls : - hosts : - argocd.example.com # Replace with your domain secretName : argocd-server-tls rules : - host : argocd.example.com # Replace with your domain http : paths : - path : / pathType : Prefix backend : service : name : argocd-server port : number : 80 Create a middleware to handle gRPC and HTTP traffic: argocd-middleware.yaml 1 2 3 4 5 6 7 8 9 apiVersion : traefik.containo.us/v1alpha1 kind : Middleware metadata : name : argocd-middleware namespace : argocd spec : headers : customRequestHeaders : X-Forwarded-Proto : \"https\" Apply the configurations: # Apply the middleware kubectl apply -f argocd-middleware.yaml # Apply the ingress kubectl apply -f argocd-ingress.yaml # Check the status of the ingress kubectl get ingress -n argocd Accessing ArgoCD \u00b6 Initial Login \u00b6 Once ArgoCD is installed and the ingress is configured, you can access it via your domain (e.g., https://argocd.example.com ). Get Initial Password Login via CLI # For manifest installation kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath = \"{.data.password}\" | base64 -d # For Helm installation (if using default values) kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath = \"{.data.password}\" | base64 -d # Install ArgoCD CLI curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/download/v2.9.3/argocd-linux-amd64 sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd rm argocd-linux-amd64 # Login using CLI argocd login argocd.example.com # Change the default password argocd account update-password Security Note Always change the default admin password immediately after the first login! Setting Up Your First Application \u00b6 After logging in, you can deploy your first application: Click on \"+ New App\" in the UI Fill in the application details: Name: example-app Project: default Sync Policy: Automatic Repository URL: Your Git repository URL Path: Path to your Kubernetes manifests Cluster: https://kubernetes.default.svc (for in-cluster deployment) Namespace: Your target namespace Security Hardening \u00b6 Security Best Practices RBAC Configuration : Limit access to ArgoCD SSO Integration : Connect to your identity provider Network Policies : Restrict pod communication Secrets Management : Use external secret stores Configure RBAC \u00b6 Create a custom RBAC policy: argocd-rbac-cm.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : ConfigMap metadata : name : argocd-rbac-cm namespace : argocd data : policy.csv : | p, role:readonly, applications, get, */*, allow p, role:readonly, clusters, get, *, allow p, role:developer, applications, create, */*, allow p, role:developer, applications, update, */*, allow p, role:developer, applications, delete, */*, allow g, developer@example.com, role:developer g, viewer@example.com, role:readonly policy.default : role:readonly Apply the ConfigMap: kubectl apply -f argocd-rbac-cm.yaml Configure SSO (GitHub Example) \u00b6 Update the ArgoCD ConfigMap: argocd-cm.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd data : url : https://argocd.example.com dex.config : | connectors: - type: github id: github name: GitHub config: clientID: your-github-client-id clientSecret: $dex.github.clientSecret orgs: - name: your-github-org Create a secret for GitHub OAuth: kubectl -n argocd create secret generic github-secret \\ --from-literal = clientSecret = your-github-client-secret Apply the ConfigMap: kubectl apply -f argocd-cm.yaml Network Policies \u00b6 Restrict network traffic to ArgoCD: argocd-network-policy.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : argocd-server-network-policy namespace : argocd spec : podSelector : matchLabels : app.kubernetes.io/name : argocd-server policyTypes : - Ingress ingress : - from : - namespaceSelector : {} ports : - protocol : TCP port : 80 - protocol : TCP port : 443 Apply the network policy: kubectl apply -f argocd-network-policy.yaml Troubleshooting \u00b6 Common Issues and Solutions \u00b6 Common Problems Ingress Not Working Certificate Issues ArgoCD Server Crashes Symptoms : Unable to access ArgoCD through the domain Solutions : 1. Check if the certificate is issued correctly: kubectl get certificate -n argocd 2. Verify Traefik is properly configured: kubectl get ingressroute -A 3. Check the Traefik logs: kubectl logs -n kube-system -l app.kubernetes.io/name = traefik Symptoms : SSL errors or certificate not issuing Solutions : 1. Check cert-manager logs: kubectl logs -n cert-manager -l app = cert-manager 2. Verify the ClusterIssuer status: kubectl describe clusterissuer letsencrypt-prod 3. Check certificate request status: kubectl get certificaterequest -n argocd Symptoms : ArgoCD UI unavailable, server pods restarting Solutions : 1. Check server logs: kubectl logs -n argocd -l app.kubernetes.io/name = argocd-server 2. Verify resource allocation: kubectl top pods -n argocd 3. Check for eviction events: kubectl get events -n argocd Diagnostic Commands \u00b6 Here are some useful commands for diagnosing issues: Diagnostic Commands # Check all ArgoCD components kubectl get pods -n argocd # Check ArgoCD server logs kubectl logs -n argocd -l app.kubernetes.io/name = argocd-server # Check ArgoCD application controller logs kubectl logs -n argocd -l app.kubernetes.io/name = argocd-application-controller # Check certificate status kubectl get certificate -n argocd # Check ingress status kubectl describe ingress argocd-server-ingress -n argocd Maintenance \u00b6 Upgrading ArgoCD \u00b6 Using Manifests Using Helm # Update to a new version kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.9.3/manifests/install.yaml # Update Helm repositories helm repo update # Upgrade ArgoCD helm upgrade argocd argo/argo-cd \\ --namespace argocd \\ --version 5 .51.4 Backup Before Upgrading Always backup your ArgoCD settings before upgrading: kubectl get -n argocd -o yaml configmap,secret,application > argocd-backup.yaml Monitoring ArgoCD \u00b6 ArgoCD exposes Prometheus metrics that can be scraped for monitoring: argocd-prometheus-servicemonitor.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-metrics namespace : monitoring spec : selector : matchLabels : app.kubernetes.io/name : argocd-metrics endpoints : - port : metrics Advanced Configuration \u00b6 GitOps Workflow Example \u00b6 sequenceDiagram participant Dev as Developer participant Git as Git Repository participant CI as CI Pipeline participant Argo as ArgoCD participant K8s as Kubernetes Dev->>Git: Push code changes Git->>CI: Trigger CI pipeline CI->>Git: Update manifests Git->>Argo: Detect changes Argo->>K8s: Apply changes K8s->>Argo: Report status Argo->>Git: Update deployment status Multi-Cluster Setup \u00b6 For managing multiple clusters with ArgoCD: Register the external cluster: argocd cluster add context-name Create applications targeting the external cluster: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : multi-cluster-app namespace : argocd spec : destination : namespace : default server : https://external-cluster-api-url project : default source : path : path/to/manifests repoURL : https://github.com/your-org/your-repo.git targetRevision : HEAD syncPolicy : automated : prune : true selfHeal : true Conclusion \u00b6 You now have a fully functional ArgoCD setup with: Secure access via HTTPS Automatic certificate management Traefik ingress integration Basic security hardening Next Steps Configure notifications Set up project templates Integrate with your CI pipeline Explore ApplicationSets for multi-cluster management References \u00b6 ArgoCD Documentation cert-manager Documentation Traefik Documentation Kubernetes Documentation","title":"ArgoCD Setup"},{"location":"argocd-setup/#argocd-setup-guide-for-kubernetes","text":"","title":"ArgoCD Setup Guide for Kubernetes"},{"location":"argocd-setup/#overview","text":"Guide Information Difficulty : Intermediate Time Required : ~45 minutes Last Updated : March 2024 ArgoCD Version : v2.9.3 Kubernetes Compatibility : K3s, K8s 1.24+ OS : Debian 12 This guide provides comprehensive instructions for setting up ArgoCD in a Kubernetes or K3s cluster, configuring Traefik ingress, and securing it with cert-manager for automatic SSL certificate renewal.","title":"Overview"},{"location":"argocd-setup/#what-is-argocd","text":"ArgoCD is a declarative, GitOps continuous delivery tool for Kubernetes. It automates the deployment of applications to Kubernetes clusters by monitoring Git repositories and applying changes when they occur. graph LR A[Git Repository] -->|Contains manifests| B[ArgoCD] B -->|Syncs to| C[Kubernetes Cluster] D[Developers] -->|Push changes| A E[ArgoCD UI/CLI] -->|Manage| B style B fill:#f9f,stroke:#333,stroke-width:2px","title":"What is ArgoCD?"},{"location":"argocd-setup/#prerequisites","text":"Requirements A running Kubernetes or K3s cluster kubectl installed and configured helm v3.x installed A domain name for ArgoCD access DNS record pointing to your cluster's IP Administrative access to your cluster","title":"Prerequisites"},{"location":"argocd-setup/#installation-steps","text":"","title":"Installation Steps"},{"location":"argocd-setup/#1-prepare-your-cluster","text":"For Standard Kubernetes For K3s # Verify cluster access kubectl cluster-info # Create namespace for ArgoCD kubectl create namespace argocd # Verify K3s is running sudo systemctl status k3s # Create namespace for ArgoCD kubectl create namespace argocd","title":"1. Prepare Your Cluster"},{"location":"argocd-setup/#2-install-argocd","text":"There are two methods to install ArgoCD: using manifests directly or using Helm. Using Manifests Using Helm # Apply the ArgoCD installation manifest kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.9.3/manifests/install.yaml # Verify pods are running kubectl get pods -n argocd # Add ArgoCD Helm repository helm repo add argo https://argoproj.github.io/argo-helm helm repo update # Install ArgoCD helm install argocd argo/argo-cd \\ --namespace argocd \\ --create-namespace \\ --version 5 .51.4 \\ --set server.extraArgs = \"{--insecure}\" \\ --set controller.metrics.enabled = true \\ --set server.metrics.enabled = true Resource Requirements ArgoCD is relatively lightweight, but for production use, consider allocating: - At least 2 CPU cores and 4GB RAM for the cluster - 1GB RAM for the ArgoCD controller - 512MB RAM for the ArgoCD server","title":"2. Install ArgoCD"},{"location":"argocd-setup/#3-install-cert-manager","text":"cert-manager is required to automatically provision and manage TLS certificates. # Add Jetstack Helm repository helm repo add jetstack https://charts.jetstack.io helm repo update # Install cert-manager with CRDs helm install cert-manager jetstack/cert-manager \\ --namespace cert-manager \\ --create-namespace \\ --version v1.13.2 \\ --set installCRDs = true # Verify cert-manager pods are running kubectl get pods -n cert-manager","title":"3. Install cert-manager"},{"location":"argocd-setup/#4-configure-clusterissuer-for-lets-encrypt","text":"Create a ClusterIssuer to obtain certificates from Let's Encrypt: letsencrypt-issuer.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion : cert-manager.io/v1 kind : ClusterIssuer metadata : name : letsencrypt-prod spec : acme : server : https://acme-v02.api.letsencrypt.org/directory email : your-email@example.com # Replace with your email privateKeySecretRef : name : letsencrypt-prod solvers : - http01 : ingress : class : traefik Apply the ClusterIssuer: # Apply the ClusterIssuer kubectl apply -f letsencrypt-issuer.yaml # Verify the ClusterIssuer is ready kubectl get clusterissuer letsencrypt-prod -o wide Rate Limits Let's Encrypt has rate limits: 50 certificates per domain per week. Use the staging server ( https://acme-staging-v02.api.letsencrypt.org/directory ) for testing.","title":"4. Configure ClusterIssuer for Let's Encrypt"},{"location":"argocd-setup/#5-configure-traefik-ingress-for-argocd","text":"Create an Ingress resource for ArgoCD: argocd-ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 apiVersion : networking.k8s.io/v1 kind : Ingress metadata : name : argocd-server-ingress namespace : argocd annotations : cert-manager.io/cluster-issuer : \"letsencrypt-prod\" traefik.ingress.kubernetes.io/router.entrypoints : \"websecure\" traefik.ingress.kubernetes.io/router.tls : \"true\" traefik.ingress.kubernetes.io/router.middlewares : \"argocd-argocd-middleware@kubernetescrd\" spec : ingressClassName : traefik tls : - hosts : - argocd.example.com # Replace with your domain secretName : argocd-server-tls rules : - host : argocd.example.com # Replace with your domain http : paths : - path : / pathType : Prefix backend : service : name : argocd-server port : number : 80 Create a middleware to handle gRPC and HTTP traffic: argocd-middleware.yaml 1 2 3 4 5 6 7 8 9 apiVersion : traefik.containo.us/v1alpha1 kind : Middleware metadata : name : argocd-middleware namespace : argocd spec : headers : customRequestHeaders : X-Forwarded-Proto : \"https\" Apply the configurations: # Apply the middleware kubectl apply -f argocd-middleware.yaml # Apply the ingress kubectl apply -f argocd-ingress.yaml # Check the status of the ingress kubectl get ingress -n argocd","title":"5. Configure Traefik Ingress for ArgoCD"},{"location":"argocd-setup/#accessing-argocd","text":"","title":"Accessing ArgoCD"},{"location":"argocd-setup/#initial-login","text":"Once ArgoCD is installed and the ingress is configured, you can access it via your domain (e.g., https://argocd.example.com ). Get Initial Password Login via CLI # For manifest installation kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath = \"{.data.password}\" | base64 -d # For Helm installation (if using default values) kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath = \"{.data.password}\" | base64 -d # Install ArgoCD CLI curl -sSL -o argocd-linux-amd64 https://github.com/argoproj/argo-cd/releases/download/v2.9.3/argocd-linux-amd64 sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd rm argocd-linux-amd64 # Login using CLI argocd login argocd.example.com # Change the default password argocd account update-password Security Note Always change the default admin password immediately after the first login!","title":"Initial Login"},{"location":"argocd-setup/#setting-up-your-first-application","text":"After logging in, you can deploy your first application: Click on \"+ New App\" in the UI Fill in the application details: Name: example-app Project: default Sync Policy: Automatic Repository URL: Your Git repository URL Path: Path to your Kubernetes manifests Cluster: https://kubernetes.default.svc (for in-cluster deployment) Namespace: Your target namespace","title":"Setting Up Your First Application"},{"location":"argocd-setup/#security-hardening","text":"Security Best Practices RBAC Configuration : Limit access to ArgoCD SSO Integration : Connect to your identity provider Network Policies : Restrict pod communication Secrets Management : Use external secret stores","title":"Security Hardening"},{"location":"argocd-setup/#configure-rbac","text":"Create a custom RBAC policy: argocd-rbac-cm.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 apiVersion : v1 kind : ConfigMap metadata : name : argocd-rbac-cm namespace : argocd data : policy.csv : | p, role:readonly, applications, get, */*, allow p, role:readonly, clusters, get, *, allow p, role:developer, applications, create, */*, allow p, role:developer, applications, update, */*, allow p, role:developer, applications, delete, */*, allow g, developer@example.com, role:developer g, viewer@example.com, role:readonly policy.default : role:readonly Apply the ConfigMap: kubectl apply -f argocd-rbac-cm.yaml","title":"Configure RBAC"},{"location":"argocd-setup/#configure-sso-github-example","text":"Update the ArgoCD ConfigMap: argocd-cm.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 apiVersion : v1 kind : ConfigMap metadata : name : argocd-cm namespace : argocd data : url : https://argocd.example.com dex.config : | connectors: - type: github id: github name: GitHub config: clientID: your-github-client-id clientSecret: $dex.github.clientSecret orgs: - name: your-github-org Create a secret for GitHub OAuth: kubectl -n argocd create secret generic github-secret \\ --from-literal = clientSecret = your-github-client-secret Apply the ConfigMap: kubectl apply -f argocd-cm.yaml","title":"Configure SSO (GitHub Example)"},{"location":"argocd-setup/#network-policies","text":"Restrict network traffic to ArgoCD: argocd-network-policy.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : argocd-server-network-policy namespace : argocd spec : podSelector : matchLabels : app.kubernetes.io/name : argocd-server policyTypes : - Ingress ingress : - from : - namespaceSelector : {} ports : - protocol : TCP port : 80 - protocol : TCP port : 443 Apply the network policy: kubectl apply -f argocd-network-policy.yaml","title":"Network Policies"},{"location":"argocd-setup/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"argocd-setup/#common-issues-and-solutions","text":"Common Problems Ingress Not Working Certificate Issues ArgoCD Server Crashes Symptoms : Unable to access ArgoCD through the domain Solutions : 1. Check if the certificate is issued correctly: kubectl get certificate -n argocd 2. Verify Traefik is properly configured: kubectl get ingressroute -A 3. Check the Traefik logs: kubectl logs -n kube-system -l app.kubernetes.io/name = traefik Symptoms : SSL errors or certificate not issuing Solutions : 1. Check cert-manager logs: kubectl logs -n cert-manager -l app = cert-manager 2. Verify the ClusterIssuer status: kubectl describe clusterissuer letsencrypt-prod 3. Check certificate request status: kubectl get certificaterequest -n argocd Symptoms : ArgoCD UI unavailable, server pods restarting Solutions : 1. Check server logs: kubectl logs -n argocd -l app.kubernetes.io/name = argocd-server 2. Verify resource allocation: kubectl top pods -n argocd 3. Check for eviction events: kubectl get events -n argocd","title":"Common Issues and Solutions"},{"location":"argocd-setup/#diagnostic-commands","text":"Here are some useful commands for diagnosing issues: Diagnostic Commands # Check all ArgoCD components kubectl get pods -n argocd # Check ArgoCD server logs kubectl logs -n argocd -l app.kubernetes.io/name = argocd-server # Check ArgoCD application controller logs kubectl logs -n argocd -l app.kubernetes.io/name = argocd-application-controller # Check certificate status kubectl get certificate -n argocd # Check ingress status kubectl describe ingress argocd-server-ingress -n argocd","title":"Diagnostic Commands"},{"location":"argocd-setup/#maintenance","text":"","title":"Maintenance"},{"location":"argocd-setup/#upgrading-argocd","text":"Using Manifests Using Helm # Update to a new version kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.9.3/manifests/install.yaml # Update Helm repositories helm repo update # Upgrade ArgoCD helm upgrade argocd argo/argo-cd \\ --namespace argocd \\ --version 5 .51.4 Backup Before Upgrading Always backup your ArgoCD settings before upgrading: kubectl get -n argocd -o yaml configmap,secret,application > argocd-backup.yaml","title":"Upgrading ArgoCD"},{"location":"argocd-setup/#monitoring-argocd","text":"ArgoCD exposes Prometheus metrics that can be scraped for monitoring: argocd-prometheus-servicemonitor.yaml 1 2 3 4 5 6 7 8 9 10 11 apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : name : argocd-metrics namespace : monitoring spec : selector : matchLabels : app.kubernetes.io/name : argocd-metrics endpoints : - port : metrics","title":"Monitoring ArgoCD"},{"location":"argocd-setup/#advanced-configuration","text":"","title":"Advanced Configuration"},{"location":"argocd-setup/#gitops-workflow-example","text":"sequenceDiagram participant Dev as Developer participant Git as Git Repository participant CI as CI Pipeline participant Argo as ArgoCD participant K8s as Kubernetes Dev->>Git: Push code changes Git->>CI: Trigger CI pipeline CI->>Git: Update manifests Git->>Argo: Detect changes Argo->>K8s: Apply changes K8s->>Argo: Report status Argo->>Git: Update deployment status","title":"GitOps Workflow Example"},{"location":"argocd-setup/#multi-cluster-setup","text":"For managing multiple clusters with ArgoCD: Register the external cluster: argocd cluster add context-name Create applications targeting the external cluster: apiVersion : argoproj.io/v1alpha1 kind : Application metadata : name : multi-cluster-app namespace : argocd spec : destination : namespace : default server : https://external-cluster-api-url project : default source : path : path/to/manifests repoURL : https://github.com/your-org/your-repo.git targetRevision : HEAD syncPolicy : automated : prune : true selfHeal : true","title":"Multi-Cluster Setup"},{"location":"argocd-setup/#conclusion","text":"You now have a fully functional ArgoCD setup with: Secure access via HTTPS Automatic certificate management Traefik ingress integration Basic security hardening Next Steps Configure notifications Set up project templates Integrate with your CI pipeline Explore ApplicationSets for multi-cluster management","title":"Conclusion"},{"location":"argocd-setup/#references","text":"ArgoCD Documentation cert-manager Documentation Traefik Documentation Kubernetes Documentation","title":"References"},{"location":"contact/","text":"Get in Touch \u00b6 Connect With Me \u00b6 I'm always interested in connecting with fellow DevOps practitioners, developers, and technology enthusiasts. Feel free to reach out through any of the following channels: Social Media \u00b6 GitHub : github.com/depgod Twitter : @myuser LinkedIn : LinkedIn Profile Professional Inquiries \u00b6 For professional inquiries regarding: - Consulting opportunities - Technical collaboration - Speaking engagements - Training sessions Please reach out via email or connect on LinkedIn. Contributing \u00b6 Interested in contributing to this documentation? Here's how you can help: Submit Issues : Found a mistake or have a suggestion? Open an issue on GitHub. Pull Requests : Want to contribute directly? Submit a pull request with your changes. Feedback : Share your thoughts on the documentation structure and content. Community \u00b6 Join our community discussions: - GitHub Discussions - Technical Forums - DevOps Communities Response Time \u00b6 I typically respond to inquiries within 24-48 hours. For urgent matters, please indicate so in your message. Looking forward to connecting with you!","title":"Get in Touch"},{"location":"contact/#get-in-touch","text":"","title":"Get in Touch"},{"location":"contact/#connect-with-me","text":"I'm always interested in connecting with fellow DevOps practitioners, developers, and technology enthusiasts. Feel free to reach out through any of the following channels:","title":"Connect With Me"},{"location":"contact/#social-media","text":"GitHub : github.com/depgod Twitter : @myuser LinkedIn : LinkedIn Profile","title":"Social Media"},{"location":"contact/#professional-inquiries","text":"For professional inquiries regarding: - Consulting opportunities - Technical collaboration - Speaking engagements - Training sessions Please reach out via email or connect on LinkedIn.","title":"Professional Inquiries"},{"location":"contact/#contributing","text":"Interested in contributing to this documentation? Here's how you can help: Submit Issues : Found a mistake or have a suggestion? Open an issue on GitHub. Pull Requests : Want to contribute directly? Submit a pull request with your changes. Feedback : Share your thoughts on the documentation structure and content.","title":"Contributing"},{"location":"contact/#community","text":"Join our community discussions: - GitHub Discussions - Technical Forums - DevOps Communities","title":"Community"},{"location":"contact/#response-time","text":"I typically respond to inquiries within 24-48 hours. For urgent matters, please indicate so in your message. Looking forward to connecting with you!","title":"Response Time"},{"location":"k3s-ha-cluster/","text":"High Availability K3s Cluster Setup Guide \u00b6 Overview \u00b6 Guide Information Difficulty : Advanced Time Required : ~1 hour Last Updated : March 2024 K3s Version : v1.29.1+k3s2 Longhorn Version : v1.6.0 Architecture Overview \u00b6 graph TD subgraph \"Control Plane\" A[Master Node<br/>etcd + K3s Server] end subgraph \"Worker Nodes\" B[Worker Node 1<br/>K3s Agent] C[Worker Node 2<br/>K3s Agent] D[Worker Node 3<br/>K3s Agent] end subgraph \"Storage Layer\" E[Longhorn<br/>Distributed Block Storage] end A --> B A --> C A --> D B --> E C --> E D --> E style A fill:#f9f,stroke:#333 style E fill:#bbf,stroke:#333 Prerequisites \u00b6 System Requirements Master Node Worker Nodes Network Requirements 2 CPU cores 4GB RAM 40GB disk space Ubuntu 22.04 LTS Static IP address 2 CPU cores 8GB RAM 100GB disk space Ubuntu 22.04 LTS Static IP addresses All nodes must have: Unrestricted connectivity between nodes Internet access for package installation Firewall ports open: TCP/6443 (K3s API) TCP/2379-2380 (etcd) UDP/8472 (VXLAN) TCP/10250 (kubelet) Server Preparation \u00b6 Important Execute these steps on ALL nodes unless specified otherwise. System Updates \u00b6 # Update package list and upgrade system sudo apt update && sudo apt upgrade -y # Install required packages sudo apt install -y \\ curl \\ gnupg \\ nfs-common \\ open-iscsi \\ jq \\ logrotate Configure Log Rotation \u00b6 Create a logrotate configuration for K3s: /etc/logrotate.d/k3s 1 2 3 4 5 6 7 8 9 10 11 12 13 /var/log/k3s/*.log { daily rotate 7 compress delaycompress missingok notifempty create 0640 root root postrotate systemctl restart k3s-server 2 >/dev/null || true systemctl restart k3s-agent 2 >/dev/null || true endscript } System Configuration \u00b6 System Settings 1 2 3 4 5 6 7 8 9 10 11 # Enable and start open-iscsi for Longhorn sudo systemctl enable --now iscsid # Configure sysctl settings cat << EOF | sudo tee /etc/sysctl.d/k3s.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.max_map_count = 262144 EOF sudo sysctl --system K3s Installation \u00b6 Master Node Setup \u00b6 Master Node Only Run these commands ONLY on the master node. Install K3s Server # Download K3s installation script curl -sfL https://get.k3s.io > k3s-install.sh # Install K3s server with HA etcd and VXLAN sudo INSTALL_K3S_VERSION = \"v1.29.1+k3s2\" bash k3s-install.sh server \\ --cluster-init \\ --flannel-backend = vxlan \\ --disable traefik \\ --disable servicelb \\ --disable local-storage \\ --tls-san $( hostname -f ) \\ --write-kubeconfig-mode 644 # Get node token for workers sudo cat /var/lib/rancher/k3s/server/node-token Worker Nodes Setup \u00b6 Worker Nodes Only Replace MASTER_IP and NODE_TOKEN with your actual values. Install K3s Agent # Download K3s installation script curl -sfL https://get.k3s.io > k3s-install.sh # Install K3s agent sudo INSTALL_K3S_VERSION = \"v1.29.1+k3s2\" K3S_URL = \"https://MASTER_IP:6443\" \\ K3S_TOKEN = \"NODE_TOKEN\" bash k3s-install.sh agent Verify Cluster Status \u00b6 Check Nodes Check Pods Check etcd Health kubectl get nodes -o wide kubectl get pods -A kubectl -n kube-system exec -it etcd-master -- etcdctl endpoint health Longhorn Installation \u00b6 Prerequisites Check \u00b6 Run this on all nodes to verify Longhorn requirements: Verify Requirements curl -sSfL https://raw.githubusercontent.com/longhorn/longhorn/v1.6.0/scripts/environment_check.sh | bash Install Longhorn \u00b6 Installation Steps Execute these commands on the master node. Deploy Longhorn # Add Longhorn Helm repository helm repo add longhorn https://charts.longhorn.io helm repo update # Install Longhorn helm install longhorn longhorn/longhorn \\ --namespace longhorn-system \\ --create-namespace \\ --version 1 .6.0 \\ --set defaultSettings.defaultDataPath = \"/var/lib/longhorn\" \\ --set defaultSettings.guaranteedEngineManagerCPU = 5 \\ --set defaultSettings.guaranteedReplicaManagerCPU = 5 Verify Longhorn Installation \u00b6 Check Pods Check StorageClass Access Dashboard kubectl -n longhorn-system get pods kubectl get sc # Port forward Longhorn UI kubectl -n longhorn-system port-forward svc/longhorn-frontend 8000 :80 Access via: http://localhost:8000 Troubleshooting \u00b6 Common Issues \u00b6 Known Problems and Solutions Node Not Ready etcd Issues Longhorn Volume Issues Check K3s service status: sudo systemctl status k3s View K3s logs: sudo journalctl -u k3s Check etcd cluster health: sudo k3s etcd-snapshot ls Verify etcd member list: kubectl -n kube-system exec -it etcd-master -- etcdctl member list Check volume status: kubectl -n longhorn-system get volumes View instance manager logs: kubectl -n longhorn-system logs -l app = longhorn-manager Maintenance \u00b6 Backup Procedures \u00b6 etcd Backup Longhorn Backup # Create etcd snapshot sudo k3s etcd-snapshot save --name etcd-backup- $( date +%Y%m%d ) # Create backup settings kubectl -n longhorn-system apply -f - <<EOF apiVersion: longhorn.io/v1beta1 kind: BackupTarget metadata: name: default spec: backupTargetURL: s3://your-bucket-name@region/path credentialSecret: aws-secret EOF Monitoring Setup \u00b6 Monitoring Stack Consider installing: - Prometheus for metrics collection - Grafana for visualization - Alertmanager for notifications Security Recommendations \u00b6 Network Policies apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny-all spec : podSelector : {} policyTypes : - Ingress - Egress Pod Security Standards apiVersion : constraints.gatekeeper.sh/v1beta1 kind : K8sPSPPrivilegedContainer metadata : name : no-privileged-containers spec : enforcementAction : deny Next Steps \u00b6 [ ] Configure external load balancer [ ] Set up monitoring and logging [ ] Implement backup strategy [ ] Configure disaster recovery [ ] Set up CI/CD pipelines Need Help? If you encounter any issues: - Check the K3s documentation - Visit the Longhorn documentation - Join the K3s Slack channel","title":"HA K3s Cluster"},{"location":"k3s-ha-cluster/#high-availability-k3s-cluster-setup-guide","text":"","title":"High Availability K3s Cluster Setup Guide"},{"location":"k3s-ha-cluster/#overview","text":"Guide Information Difficulty : Advanced Time Required : ~1 hour Last Updated : March 2024 K3s Version : v1.29.1+k3s2 Longhorn Version : v1.6.0","title":"Overview"},{"location":"k3s-ha-cluster/#architecture-overview","text":"graph TD subgraph \"Control Plane\" A[Master Node<br/>etcd + K3s Server] end subgraph \"Worker Nodes\" B[Worker Node 1<br/>K3s Agent] C[Worker Node 2<br/>K3s Agent] D[Worker Node 3<br/>K3s Agent] end subgraph \"Storage Layer\" E[Longhorn<br/>Distributed Block Storage] end A --> B A --> C A --> D B --> E C --> E D --> E style A fill:#f9f,stroke:#333 style E fill:#bbf,stroke:#333","title":"Architecture Overview"},{"location":"k3s-ha-cluster/#prerequisites","text":"System Requirements Master Node Worker Nodes Network Requirements 2 CPU cores 4GB RAM 40GB disk space Ubuntu 22.04 LTS Static IP address 2 CPU cores 8GB RAM 100GB disk space Ubuntu 22.04 LTS Static IP addresses All nodes must have: Unrestricted connectivity between nodes Internet access for package installation Firewall ports open: TCP/6443 (K3s API) TCP/2379-2380 (etcd) UDP/8472 (VXLAN) TCP/10250 (kubelet)","title":"Prerequisites"},{"location":"k3s-ha-cluster/#server-preparation","text":"Important Execute these steps on ALL nodes unless specified otherwise.","title":"Server Preparation"},{"location":"k3s-ha-cluster/#system-updates","text":"# Update package list and upgrade system sudo apt update && sudo apt upgrade -y # Install required packages sudo apt install -y \\ curl \\ gnupg \\ nfs-common \\ open-iscsi \\ jq \\ logrotate","title":"System Updates"},{"location":"k3s-ha-cluster/#configure-log-rotation","text":"Create a logrotate configuration for K3s: /etc/logrotate.d/k3s 1 2 3 4 5 6 7 8 9 10 11 12 13 /var/log/k3s/*.log { daily rotate 7 compress delaycompress missingok notifempty create 0640 root root postrotate systemctl restart k3s-server 2 >/dev/null || true systemctl restart k3s-agent 2 >/dev/null || true endscript }","title":"Configure Log Rotation"},{"location":"k3s-ha-cluster/#system-configuration","text":"System Settings 1 2 3 4 5 6 7 8 9 10 11 # Enable and start open-iscsi for Longhorn sudo systemctl enable --now iscsid # Configure sysctl settings cat << EOF | sudo tee /etc/sysctl.d/k3s.conf net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 vm.max_map_count = 262144 EOF sudo sysctl --system","title":"System Configuration"},{"location":"k3s-ha-cluster/#k3s-installation","text":"","title":"K3s Installation"},{"location":"k3s-ha-cluster/#master-node-setup","text":"Master Node Only Run these commands ONLY on the master node. Install K3s Server # Download K3s installation script curl -sfL https://get.k3s.io > k3s-install.sh # Install K3s server with HA etcd and VXLAN sudo INSTALL_K3S_VERSION = \"v1.29.1+k3s2\" bash k3s-install.sh server \\ --cluster-init \\ --flannel-backend = vxlan \\ --disable traefik \\ --disable servicelb \\ --disable local-storage \\ --tls-san $( hostname -f ) \\ --write-kubeconfig-mode 644 # Get node token for workers sudo cat /var/lib/rancher/k3s/server/node-token","title":"Master Node Setup"},{"location":"k3s-ha-cluster/#worker-nodes-setup","text":"Worker Nodes Only Replace MASTER_IP and NODE_TOKEN with your actual values. Install K3s Agent # Download K3s installation script curl -sfL https://get.k3s.io > k3s-install.sh # Install K3s agent sudo INSTALL_K3S_VERSION = \"v1.29.1+k3s2\" K3S_URL = \"https://MASTER_IP:6443\" \\ K3S_TOKEN = \"NODE_TOKEN\" bash k3s-install.sh agent","title":"Worker Nodes Setup"},{"location":"k3s-ha-cluster/#verify-cluster-status","text":"Check Nodes Check Pods Check etcd Health kubectl get nodes -o wide kubectl get pods -A kubectl -n kube-system exec -it etcd-master -- etcdctl endpoint health","title":"Verify Cluster Status"},{"location":"k3s-ha-cluster/#longhorn-installation","text":"","title":"Longhorn Installation"},{"location":"k3s-ha-cluster/#prerequisites-check","text":"Run this on all nodes to verify Longhorn requirements: Verify Requirements curl -sSfL https://raw.githubusercontent.com/longhorn/longhorn/v1.6.0/scripts/environment_check.sh | bash","title":"Prerequisites Check"},{"location":"k3s-ha-cluster/#install-longhorn","text":"Installation Steps Execute these commands on the master node. Deploy Longhorn # Add Longhorn Helm repository helm repo add longhorn https://charts.longhorn.io helm repo update # Install Longhorn helm install longhorn longhorn/longhorn \\ --namespace longhorn-system \\ --create-namespace \\ --version 1 .6.0 \\ --set defaultSettings.defaultDataPath = \"/var/lib/longhorn\" \\ --set defaultSettings.guaranteedEngineManagerCPU = 5 \\ --set defaultSettings.guaranteedReplicaManagerCPU = 5","title":"Install Longhorn"},{"location":"k3s-ha-cluster/#verify-longhorn-installation","text":"Check Pods Check StorageClass Access Dashboard kubectl -n longhorn-system get pods kubectl get sc # Port forward Longhorn UI kubectl -n longhorn-system port-forward svc/longhorn-frontend 8000 :80 Access via: http://localhost:8000","title":"Verify Longhorn Installation"},{"location":"k3s-ha-cluster/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"k3s-ha-cluster/#common-issues","text":"Known Problems and Solutions Node Not Ready etcd Issues Longhorn Volume Issues Check K3s service status: sudo systemctl status k3s View K3s logs: sudo journalctl -u k3s Check etcd cluster health: sudo k3s etcd-snapshot ls Verify etcd member list: kubectl -n kube-system exec -it etcd-master -- etcdctl member list Check volume status: kubectl -n longhorn-system get volumes View instance manager logs: kubectl -n longhorn-system logs -l app = longhorn-manager","title":"Common Issues"},{"location":"k3s-ha-cluster/#maintenance","text":"","title":"Maintenance"},{"location":"k3s-ha-cluster/#backup-procedures","text":"etcd Backup Longhorn Backup # Create etcd snapshot sudo k3s etcd-snapshot save --name etcd-backup- $( date +%Y%m%d ) # Create backup settings kubectl -n longhorn-system apply -f - <<EOF apiVersion: longhorn.io/v1beta1 kind: BackupTarget metadata: name: default spec: backupTargetURL: s3://your-bucket-name@region/path credentialSecret: aws-secret EOF","title":"Backup Procedures"},{"location":"k3s-ha-cluster/#monitoring-setup","text":"Monitoring Stack Consider installing: - Prometheus for metrics collection - Grafana for visualization - Alertmanager for notifications","title":"Monitoring Setup"},{"location":"k3s-ha-cluster/#security-recommendations","text":"Network Policies apiVersion : networking.k8s.io/v1 kind : NetworkPolicy metadata : name : default-deny-all spec : podSelector : {} policyTypes : - Ingress - Egress Pod Security Standards apiVersion : constraints.gatekeeper.sh/v1beta1 kind : K8sPSPPrivilegedContainer metadata : name : no-privileged-containers spec : enforcementAction : deny","title":"Security Recommendations"},{"location":"k3s-ha-cluster/#next-steps","text":"[ ] Configure external load balancer [ ] Set up monitoring and logging [ ] Implement backup strategy [ ] Configure disaster recovery [ ] Set up CI/CD pipelines Need Help? If you encounter any issues: - Check the K3s documentation - Visit the Longhorn documentation - Join the K3s Slack channel","title":"Next Steps"},{"location":"markdown_basics/","text":"Markdown Guide \u00b6 This comprehensive guide will help you master Markdown syntax for creating well-formatted documentation. Each section includes both the Markdown syntax and its rendered output. Basic Syntax \u00b6 1. Headings \u00b6 Markdown provides six levels of headings, using # symbols: # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 The rendered output looks like this: Heading 1 \u00b6 Heading 2 \u00b6 Heading 3 \u00b6 Heading 4 \u00b6 Heading 5 \u00b6 Heading 6 \u00b6 2. Text Formatting \u00b6 Bold Text \u00b6 **Bold text** or __Bold text__ Bold text or Bold text Italic Text \u00b6 *Italic text* or _Italic text_ Italic text or Italic text Bold and Italic \u00b6 ***Bold and italic*** or ___Bold and italic___ Bold and italic or Bold and italic Strikethrough \u00b6 ~~Strikethrough text~~ ~~Strikethrough text~~ 3. Lists \u00b6 Unordered Lists \u00b6 - First item - Second item - Indented item - Another indented item - Third item First item Second item Indented item Another indented item Third item Ordered Lists \u00b6 1. First item 2. Second item 1. Indented item 2. Another indented item 3. Third item First item Second item Indented item Another indented item Third item 4. Links \u00b6 Basic Links \u00b6 [ Visit GitHub ]( https://github.com ) Visit GitHub Links with Titles \u00b6 [ GitHub ]( https://github.com \"GitHub's Homepage\" ) GitHub Reference-style Links \u00b6 [ GitHub ][ 1 ] [ DevOps ][ 2 ] [ 1 ]: https://github.com [ 2 ]: https://en.wikipedia.org/wiki/DevOps GitHub DevOps 5. Images \u00b6 Basic Image \u00b6 ![ Alt text ]( https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png ) Image with Title \u00b6 ![ GitHub Logo ]( https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png \"GitHub Logo\" ) 6. Code \u00b6 Inline Code \u00b6 Use `git status` to list all changed files. Use git status to list all changed files. Code Blocks \u00b6 ```python def hello_world (): print ( \"Hello, World!\" ) ``` def hello_world (): print ( \"Hello, World!\" ) Syntax Highlighting \u00b6 ```javascript function greet ( name ) { console . log ( `Hello, ${ name } !` ); } ``` function greet ( name ) { console . log ( `Hello, ${ name } !` ); } 7. Tables \u00b6 | Left-aligned | Center-aligned | Right-aligned | |:-------------|:-------------:|-------------:| | Content | Content | Content | | Left | Center | Right | Left-aligned Center-aligned Right-aligned Content Content Content Left Center Right 8. Blockquotes \u00b6 Simple Blockquote \u00b6 > This is a blockquote This is a blockquote Nested Blockquotes \u00b6 > First level >> Second level >>> Third level First level Second level Third level 9. Task Lists \u00b6 - [x] Completed task - [ ] Incomplete task - [x] Completed subtask - [ ] Incomplete subtask [x] Completed task [ ] Incomplete task [x] Completed subtask [ ] Incomplete subtask 10. Horizontal Rules \u00b6 Any of these will create a horizontal rule: --- *** ___ 11. Escaping Characters \u00b6 Use backslash to escape special characters: \\* Not italic \\* \\` Not code \\` \\[ Not a link \\] * Not italic * ` Not code ` [ Not a link ] 12. Extended Syntax (with Material for MkDocs) \u00b6 Highlighting Text \u00b6 ==Highlighted text== ==Highlighted text== Footnotes \u00b6 Here's a sentence with a footnote[^1]. [ ^1 ]: This is the footnote. Here's a sentence with a footnote 1 . Definition Lists \u00b6 term : definition term definition Emoji \u00b6 :smile: :heart: :thumbsup: :smile: :heart: :thumbsup: Best Practices \u00b6 Consistency : Use consistent formatting throughout your document Spacing : Add blank lines before and after headings Headers : Use proper header hierarchy (don't skip levels) Lists : Keep them simple and nested no more than three levels Code Blocks : Always specify the language for syntax highlighting Links : Use descriptive text rather than \"click here\" Images : Always include alt text for accessibility Common Pitfalls to Avoid \u00b6 Forgetting to add two spaces for line breaks Incorrect nesting of lists Missing blank lines before and after lists and code blocks Improper escaping of special characters Inconsistent heading hierarchy Tools and Resources \u00b6 Markdown Editors : Visual Studio Code with Markdown extensions Typora StackEdit (web-based) Online Validators : MarkdownLint Dillinger Cheat Sheets : GitHub Markdown Guide Markdown Guide This is the footnote. \u21a9","title":"Markdown Guide"},{"location":"markdown_basics/#markdown-guide","text":"This comprehensive guide will help you master Markdown syntax for creating well-formatted documentation. Each section includes both the Markdown syntax and its rendered output.","title":"Markdown Guide"},{"location":"markdown_basics/#basic-syntax","text":"","title":"Basic Syntax"},{"location":"markdown_basics/#1-headings","text":"Markdown provides six levels of headings, using # symbols: # Heading 1 ## Heading 2 ### Heading 3 #### Heading 4 ##### Heading 5 ###### Heading 6 The rendered output looks like this:","title":"1. Headings"},{"location":"markdown_basics/#heading-1","text":"","title":"Heading 1"},{"location":"markdown_basics/#heading-2","text":"","title":"Heading 2"},{"location":"markdown_basics/#heading-3","text":"","title":"Heading 3"},{"location":"markdown_basics/#heading-4","text":"","title":"Heading 4"},{"location":"markdown_basics/#heading-5","text":"","title":"Heading 5"},{"location":"markdown_basics/#heading-6","text":"","title":"Heading 6"},{"location":"markdown_basics/#2-text-formatting","text":"","title":"2. Text Formatting"},{"location":"markdown_basics/#bold-text","text":"**Bold text** or __Bold text__ Bold text or Bold text","title":"Bold Text"},{"location":"markdown_basics/#italic-text","text":"*Italic text* or _Italic text_ Italic text or Italic text","title":"Italic Text"},{"location":"markdown_basics/#bold-and-italic","text":"***Bold and italic*** or ___Bold and italic___ Bold and italic or Bold and italic","title":"Bold and Italic"},{"location":"markdown_basics/#strikethrough","text":"~~Strikethrough text~~ ~~Strikethrough text~~","title":"Strikethrough"},{"location":"markdown_basics/#3-lists","text":"","title":"3. Lists"},{"location":"markdown_basics/#unordered-lists","text":"- First item - Second item - Indented item - Another indented item - Third item First item Second item Indented item Another indented item Third item","title":"Unordered Lists"},{"location":"markdown_basics/#ordered-lists","text":"1. First item 2. Second item 1. Indented item 2. Another indented item 3. Third item First item Second item Indented item Another indented item Third item","title":"Ordered Lists"},{"location":"markdown_basics/#4-links","text":"","title":"4. Links"},{"location":"markdown_basics/#basic-links","text":"[ Visit GitHub ]( https://github.com ) Visit GitHub","title":"Basic Links"},{"location":"markdown_basics/#links-with-titles","text":"[ GitHub ]( https://github.com \"GitHub's Homepage\" ) GitHub","title":"Links with Titles"},{"location":"markdown_basics/#reference-style-links","text":"[ GitHub ][ 1 ] [ DevOps ][ 2 ] [ 1 ]: https://github.com [ 2 ]: https://en.wikipedia.org/wiki/DevOps GitHub DevOps","title":"Reference-style Links"},{"location":"markdown_basics/#5-images","text":"","title":"5. Images"},{"location":"markdown_basics/#basic-image","text":"![ Alt text ]( https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png )","title":"Basic Image"},{"location":"markdown_basics/#image-with-title","text":"![ GitHub Logo ]( https://github.githubassets.com/images/modules/logos_page/GitHub-Mark.png \"GitHub Logo\" )","title":"Image with Title"},{"location":"markdown_basics/#6-code","text":"","title":"6. Code"},{"location":"markdown_basics/#inline-code","text":"Use `git status` to list all changed files. Use git status to list all changed files.","title":"Inline Code"},{"location":"markdown_basics/#code-blocks","text":"```python def hello_world (): print ( \"Hello, World!\" ) ``` def hello_world (): print ( \"Hello, World!\" )","title":"Code Blocks"},{"location":"markdown_basics/#syntax-highlighting","text":"```javascript function greet ( name ) { console . log ( `Hello, ${ name } !` ); } ``` function greet ( name ) { console . log ( `Hello, ${ name } !` ); }","title":"Syntax Highlighting"},{"location":"markdown_basics/#7-tables","text":"| Left-aligned | Center-aligned | Right-aligned | |:-------------|:-------------:|-------------:| | Content | Content | Content | | Left | Center | Right | Left-aligned Center-aligned Right-aligned Content Content Content Left Center Right","title":"7. Tables"},{"location":"markdown_basics/#8-blockquotes","text":"","title":"8. Blockquotes"},{"location":"markdown_basics/#simple-blockquote","text":"> This is a blockquote This is a blockquote","title":"Simple Blockquote"},{"location":"markdown_basics/#nested-blockquotes","text":"> First level >> Second level >>> Third level First level Second level Third level","title":"Nested Blockquotes"},{"location":"markdown_basics/#9-task-lists","text":"- [x] Completed task - [ ] Incomplete task - [x] Completed subtask - [ ] Incomplete subtask [x] Completed task [ ] Incomplete task [x] Completed subtask [ ] Incomplete subtask","title":"9. Task Lists"},{"location":"markdown_basics/#10-horizontal-rules","text":"Any of these will create a horizontal rule: --- *** ___","title":"10. Horizontal Rules"},{"location":"markdown_basics/#11-escaping-characters","text":"Use backslash to escape special characters: \\* Not italic \\* \\` Not code \\` \\[ Not a link \\] * Not italic * ` Not code ` [ Not a link ]","title":"11. Escaping Characters"},{"location":"markdown_basics/#12-extended-syntax-with-material-for-mkdocs","text":"","title":"12. Extended Syntax (with Material for MkDocs)"},{"location":"markdown_basics/#highlighting-text","text":"==Highlighted text== ==Highlighted text==","title":"Highlighting Text"},{"location":"markdown_basics/#footnotes","text":"Here's a sentence with a footnote[^1]. [ ^1 ]: This is the footnote. Here's a sentence with a footnote 1 .","title":"Footnotes"},{"location":"markdown_basics/#definition-lists","text":"term : definition term definition","title":"Definition Lists"},{"location":"markdown_basics/#emoji","text":":smile: :heart: :thumbsup: :smile: :heart: :thumbsup:","title":"Emoji"},{"location":"markdown_basics/#best-practices","text":"Consistency : Use consistent formatting throughout your document Spacing : Add blank lines before and after headings Headers : Use proper header hierarchy (don't skip levels) Lists : Keep them simple and nested no more than three levels Code Blocks : Always specify the language for syntax highlighting Links : Use descriptive text rather than \"click here\" Images : Always include alt text for accessibility","title":"Best Practices"},{"location":"markdown_basics/#common-pitfalls-to-avoid","text":"Forgetting to add two spaces for line breaks Incorrect nesting of lists Missing blank lines before and after lists and code blocks Improper escaping of special characters Inconsistent heading hierarchy","title":"Common Pitfalls to Avoid"},{"location":"markdown_basics/#tools-and-resources","text":"Markdown Editors : Visual Studio Code with Markdown extensions Typora StackEdit (web-based) Online Validators : MarkdownLint Dillinger Cheat Sheets : GitHub Markdown Guide Markdown Guide This is the footnote. \u21a9","title":"Tools and Resources"},{"location":"multipass-k8s-cluster/","text":"Local Kubernetes Cluster with Multipass \u00b6 Overview \u00b6 Guide Information Difficulty : Intermediate Time Required : ~30 minutes Last Updated : March 2024 Table of Contents \u00b6 Setup Multipass Provision Virtual Machines Server Preparation Configure System Settings Install Kubernetes Components Initialize Cluster Setup Network Interface Join Worker Nodes Verify Cluster Troubleshooting Cluster Maintenance Architecture \u00b6 graph TD A[Host Ubuntu 24.04] --> B[Multipass] B --> C[Master Node<br/>4GB RAM, 2 CPU] B --> D[Worker1<br/>4GB RAM, 2 CPU] B --> E[Worker2<br/>4GB RAM, 2 CPU] C --> F[Control Plane] F --> G[API Server] F --> H[etcd] F --> I[Controller Manager] F --> J[Scheduler] D --> K[kubelet] D --> L[containerd] E --> M[kubelet] E --> N[containerd] Setup Multipass \u00b6 Quick Setup Multipass provides a fast way to spin up Ubuntu VMs. It's lightweight and perfect for local Kubernetes clusters. Install Multipass Verify Installation List Available Images sudo snap install multipass multipass version multipass find Provision Virtual Machines \u00b6 Resource Allocation We'll create one master node and two worker nodes. Adjust the resources based on your system capabilities. Create Cluster Nodes # Create master node multipass launch --name master --cpus 2 --mem 4G --disk 20G # Create worker nodes multipass launch --name worker1 --cpus 2 --mem 4G --disk 20G multipass launch --name worker2 --cpus 2 --mem 4G --disk 20G Expected Output Launched: master Launched: worker1 Launched: worker2 Access Nodes \u00b6 Master Node Worker Node 1 Worker Node 2 multipass shell master multipass shell worker1 multipass shell worker2 Get Node IPs \u00b6 multipass list Server Preparation \u00b6 Important Run these commands on ALL nodes (master and workers). Update System Install Dependencies Setup Docker Repository sudo apt update && sudo apt upgrade -y sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo tee /etc/apt/keyrings/docker.asc > /dev/null echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null Install Containerd \u00b6 Install and Configure Containerd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Install containerd sudo apt update sudo apt install -y containerd.io # Enable and start containerd sudo systemctl enable containerd sudo systemctl start containerd # Generate default config sudo mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml # Modify containerd configuration sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml sudo sed -i 's/^disabled_plugins = \\[\"cri\"\\]/#disabled_plugins = \\[\"cri\"\\]/' /etc/containerd/config.toml # Restart containerd sudo systemctl restart containerd Configure System Settings \u00b6 Critical Step Skipping these configurations may result in cluster initialization failures. Disable Swap Load Kernel Modules Configure Sysctl sudo swapoff -a sudo sed -i '/swap/d' /etc/fstab cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF sudo sysctl --system Install Kubernetes Components \u00b6 Version Information This guide uses Kubernetes v1.32. Adjust version numbers as needed. Install Kubernetes Tools # Add Kubernetes repository curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list # Install required packages sudo apt update sudo apt install -y kubelet kubeadm kubectl # Prevent accidental upgrades sudo apt-mark hold kubelet kubeadm kubectl # Enable kubelet sudo systemctl enable kubelet sudo systemctl start kubelet Initialize Cluster \u00b6 Master Node Only Run these commands ONLY on the master node. Initialize Kubernetes Cluster # Initialize cluster sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 # Setup kubeconfig mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config Save the Join Command The initialization will output a kubeadm join command. Save this for joining worker nodes. Setup Network Interface \u00b6 Install Flannel CNI Verify Installation kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml kubectl get pods -n kube-flannel Join Worker Nodes \u00b6 Worker Nodes Only Run these commands on each worker node. Get Join Command (Master) Join Cluster (Workers) kubeadm token create --print-join-command sudo kubeadm join <master-ip>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash> Verify Cluster \u00b6 Check Node Status Check Pods kubectl get nodes kubectl get pods -A Troubleshooting \u00b6 Common Issues Node Not Ready Join Command Issues Pod Network Issues Check CNI pods: kubectl get pods -n kube-system Check kubelet status: systemctl status kubelet View kubelet logs: journalctl -xeu kubelet Generate new token: kubeadm token create Get discovery token CA cert hash: openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | \\ openssl rsa -pubin -outform der 2 >/dev/null | \\ openssl dgst -sha256 -hex | sed 's/^.* //' Check flannel pods: kubectl get pods -n kube-flannel Check flannel logs: kubectl logs -n kube-flannel <pod-name> Cluster Maintenance \u00b6 Backup Procedures \u00b6 Backup etcd sudo apt install etcd-client ETCDCTL_API = 3 etcdctl --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ snapshot save snapshot.db Scaling the Cluster \u00b6 To add more worker nodes: Create new VM using multipass Follow server preparation steps Join the cluster using the join command Cleanup \u00b6 Delete Node Delete VM # On master node kubectl drain <node-name> --ignore-daemonsets kubectl delete node <node-name> # On worker node sudo kubeadm reset multipass delete <vm-name> multipass purge Security Best Practices \u00b6 Keep Kubernetes version updated Use Network Policies Enable RBAC Regularly rotate certificates Monitor cluster with security tools Next Steps \u00b6 Deploy sample applications Setup monitoring with Prometheus and Grafana Configure persistent storage Implement high availability Need Help? If you encounter any issues, check the official Kubernetes documentation or open an issue in the repository.","title":"Multipass Local Cluster"},{"location":"multipass-k8s-cluster/#local-kubernetes-cluster-with-multipass","text":"","title":"Local Kubernetes Cluster with Multipass"},{"location":"multipass-k8s-cluster/#overview","text":"Guide Information Difficulty : Intermediate Time Required : ~30 minutes Last Updated : March 2024","title":"Overview"},{"location":"multipass-k8s-cluster/#table-of-contents","text":"Setup Multipass Provision Virtual Machines Server Preparation Configure System Settings Install Kubernetes Components Initialize Cluster Setup Network Interface Join Worker Nodes Verify Cluster Troubleshooting Cluster Maintenance","title":"Table of Contents"},{"location":"multipass-k8s-cluster/#architecture","text":"graph TD A[Host Ubuntu 24.04] --> B[Multipass] B --> C[Master Node<br/>4GB RAM, 2 CPU] B --> D[Worker1<br/>4GB RAM, 2 CPU] B --> E[Worker2<br/>4GB RAM, 2 CPU] C --> F[Control Plane] F --> G[API Server] F --> H[etcd] F --> I[Controller Manager] F --> J[Scheduler] D --> K[kubelet] D --> L[containerd] E --> M[kubelet] E --> N[containerd]","title":"Architecture"},{"location":"multipass-k8s-cluster/#setup-multipass","text":"Quick Setup Multipass provides a fast way to spin up Ubuntu VMs. It's lightweight and perfect for local Kubernetes clusters. Install Multipass Verify Installation List Available Images sudo snap install multipass multipass version multipass find","title":"Setup Multipass"},{"location":"multipass-k8s-cluster/#provision-virtual-machines","text":"Resource Allocation We'll create one master node and two worker nodes. Adjust the resources based on your system capabilities. Create Cluster Nodes # Create master node multipass launch --name master --cpus 2 --mem 4G --disk 20G # Create worker nodes multipass launch --name worker1 --cpus 2 --mem 4G --disk 20G multipass launch --name worker2 --cpus 2 --mem 4G --disk 20G Expected Output Launched: master Launched: worker1 Launched: worker2","title":"Provision Virtual Machines"},{"location":"multipass-k8s-cluster/#access-nodes","text":"Master Node Worker Node 1 Worker Node 2 multipass shell master multipass shell worker1 multipass shell worker2","title":"Access Nodes"},{"location":"multipass-k8s-cluster/#get-node-ips","text":"multipass list","title":"Get Node IPs"},{"location":"multipass-k8s-cluster/#server-preparation","text":"Important Run these commands on ALL nodes (master and workers). Update System Install Dependencies Setup Docker Repository sudo apt update && sudo apt upgrade -y sudo apt install -y apt-transport-https ca-certificates curl gnupg lsb-release sudo mkdir -p /etc/apt/keyrings curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo tee /etc/apt/keyrings/docker.asc > /dev/null echo \"deb [arch= $( dpkg --print-architecture ) signed-by=/etc/apt/keyrings/docker.asc] https://download.docker.com/linux/ubuntu $( lsb_release -cs ) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null","title":"Server Preparation"},{"location":"multipass-k8s-cluster/#install-containerd","text":"Install and Configure Containerd 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # Install containerd sudo apt update sudo apt install -y containerd.io # Enable and start containerd sudo systemctl enable containerd sudo systemctl start containerd # Generate default config sudo mkdir -p /etc/containerd containerd config default | sudo tee /etc/containerd/config.toml # Modify containerd configuration sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml sudo sed -i 's/^disabled_plugins = \\[\"cri\"\\]/#disabled_plugins = \\[\"cri\"\\]/' /etc/containerd/config.toml # Restart containerd sudo systemctl restart containerd","title":"Install Containerd"},{"location":"multipass-k8s-cluster/#configure-system-settings","text":"Critical Step Skipping these configurations may result in cluster initialization failures. Disable Swap Load Kernel Modules Configure Sysctl sudo swapoff -a sudo sed -i '/swap/d' /etc/fstab cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf overlay br_netfilter EOF sudo modprobe overlay sudo modprobe br_netfilter cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.ipv4.ip_forward = 1 EOF sudo sysctl --system","title":"Configure System Settings"},{"location":"multipass-k8s-cluster/#install-kubernetes-components","text":"Version Information This guide uses Kubernetes v1.32. Adjust version numbers as needed. Install Kubernetes Tools # Add Kubernetes repository curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.32/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.32/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list # Install required packages sudo apt update sudo apt install -y kubelet kubeadm kubectl # Prevent accidental upgrades sudo apt-mark hold kubelet kubeadm kubectl # Enable kubelet sudo systemctl enable kubelet sudo systemctl start kubelet","title":"Install Kubernetes Components"},{"location":"multipass-k8s-cluster/#initialize-cluster","text":"Master Node Only Run these commands ONLY on the master node. Initialize Kubernetes Cluster # Initialize cluster sudo kubeadm init --pod-network-cidr = 10 .244.0.0/16 # Setup kubeconfig mkdir -p $HOME /.kube sudo cp -i /etc/kubernetes/admin.conf $HOME /.kube/config sudo chown $( id -u ) : $( id -g ) $HOME /.kube/config Save the Join Command The initialization will output a kubeadm join command. Save this for joining worker nodes.","title":"Initialize Cluster"},{"location":"multipass-k8s-cluster/#setup-network-interface","text":"Install Flannel CNI Verify Installation kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml kubectl get pods -n kube-flannel","title":"Setup Network Interface"},{"location":"multipass-k8s-cluster/#join-worker-nodes","text":"Worker Nodes Only Run these commands on each worker node. Get Join Command (Master) Join Cluster (Workers) kubeadm token create --print-join-command sudo kubeadm join <master-ip>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>","title":"Join Worker Nodes"},{"location":"multipass-k8s-cluster/#verify-cluster","text":"Check Node Status Check Pods kubectl get nodes kubectl get pods -A","title":"Verify Cluster"},{"location":"multipass-k8s-cluster/#troubleshooting","text":"Common Issues Node Not Ready Join Command Issues Pod Network Issues Check CNI pods: kubectl get pods -n kube-system Check kubelet status: systemctl status kubelet View kubelet logs: journalctl -xeu kubelet Generate new token: kubeadm token create Get discovery token CA cert hash: openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | \\ openssl rsa -pubin -outform der 2 >/dev/null | \\ openssl dgst -sha256 -hex | sed 's/^.* //' Check flannel pods: kubectl get pods -n kube-flannel Check flannel logs: kubectl logs -n kube-flannel <pod-name>","title":"Troubleshooting"},{"location":"multipass-k8s-cluster/#cluster-maintenance","text":"","title":"Cluster Maintenance"},{"location":"multipass-k8s-cluster/#backup-procedures","text":"Backup etcd sudo apt install etcd-client ETCDCTL_API = 3 etcdctl --endpoints = https://127.0.0.1:2379 \\ --cacert = /etc/kubernetes/pki/etcd/ca.crt \\ --cert = /etc/kubernetes/pki/etcd/server.crt \\ --key = /etc/kubernetes/pki/etcd/server.key \\ snapshot save snapshot.db","title":"Backup Procedures"},{"location":"multipass-k8s-cluster/#scaling-the-cluster","text":"To add more worker nodes: Create new VM using multipass Follow server preparation steps Join the cluster using the join command","title":"Scaling the Cluster"},{"location":"multipass-k8s-cluster/#cleanup","text":"Delete Node Delete VM # On master node kubectl drain <node-name> --ignore-daemonsets kubectl delete node <node-name> # On worker node sudo kubeadm reset multipass delete <vm-name> multipass purge","title":"Cleanup"},{"location":"multipass-k8s-cluster/#security-best-practices","text":"Keep Kubernetes version updated Use Network Policies Enable RBAC Regularly rotate certificates Monitor cluster with security tools","title":"Security Best Practices"},{"location":"multipass-k8s-cluster/#next-steps","text":"Deploy sample applications Setup monitoring with Prometheus and Grafana Configure persistent storage Implement high availability Need Help? If you encounter any issues, check the official Kubernetes documentation or open an issue in the repository.","title":"Next Steps"},{"location":"versioning/","text":"1. Git SHA (Commit Hash) \u00b6 Format : image:git-sha Example : myapp:a1b2c3d Pros: \u00b6 Precise tracking to source code Immutable and unique Easy to debug and rollback Perfect for development environments Cons: \u00b6 Not human-readable Difficult to determine version order No immediate indication of stability level Detailed Explanation: \u00b6 Git SHA versioning uses the unique hash identifier that Git generates for each commit in your repository. When you build a Docker image using this approach, you take the first few characters (usually 7-8) of the commit hash and use it as your image tag. This method creates an unbreakable link between your source code and the Docker image, making it extremely useful for debugging and traceability. For instance, if you discover an issue in production, you can immediately identify the exact code commit that produced that image. However, these hashes are not human-friendly - you can't tell at a glance which version came first or what changes it contains. This makes it less ideal for release management but perfect for development and testing environments where precise code tracking is crucial. 2. Semantic Versioning \u00b6 Format : image:MAJOR.MINOR.PATCH Example : myapp:1.2.3 Pros: \u00b6 Clear indication of change magnitude Well understood by developers Good for stable releases Easy to automate with conventional commits Cons: \u00b6 Can be subjective (what constitutes a breaking change?) Multiple tags might point to same image Detailed Explanation: \u00b6 Semantic Versioning follows a structured numbering system with three components: MAJOR.MINOR.PATCH. Each component has a specific meaning - MAJOR versions indicate breaking changes that might require users to modify their code, MINOR versions add new features while maintaining backward compatibility, and PATCH versions represent bug fixes. This system is particularly valuable when your Docker image contains an application or service that other systems depend on. Users can quickly understand the impact of upgrading to a new version. For example, if you're currently using version 1.2.3 and see version 1.2.4, you know it's safe to upgrade since it's just a patch. However, if you see version 2.0.0, you know to carefully review the changes as it contains breaking changes. The main challenge with SemVer is maintaining discipline in version number assignment - teams need to consistently agree on what constitutes a breaking change versus a minor feature addition. 3. Git Tag Based \u00b6 Format : image:v1.2.3 Example : myapp:v1.2.3 Pros: \u00b6 Direct correlation with Git releases Good for release automation Clear release history Cons: \u00b6 Requires disciplined tag management May need additional CI/CD configuration Can be confusing with multiple release branches Detailed Explanation: \u00b6 Git tag based versioning aligns your Docker image versions with your Git repository's release tags. This approach creates a natural workflow where creating a Git tag automatically triggers a new Docker image build with the same version. It's particularly powerful when combined with semantic versioning - for example, tagging a release as v1.2.3 in Git automatically produces a Docker image tagged 1.2.3. This method works exceptionally well with automated release processes and provides clear documentation of your release history. The challenge comes when managing multiple release branches or when hotfixes need to be applied to older versions. You need robust processes to handle these scenarios and ensure tags are created consistently across branches. 4. Environment Based \u00b6 Format : image:env-timestamp Example : myapp:prod-20250302 Pros: \u00b6 Clear deployment target Easy to track when image was built Good for environment-specific configurations Cons: \u00b6 Less precise source tracking Potential confusion with multiple deployments per day Additional storage overhead Detailed Explanation: \u00b6 Environment based versioning adds context about where and when an image is intended to be used. This approach often combines an environment identifier with a timestamp or build number, such as prod-20250302 or staging-build123. This strategy is particularly useful in organizations with complex deployment pipelines involving multiple environments (development, staging, QA, production). It makes it immediately clear which images are approved for which environments and when they were built. The timestamp component helps track the age of deployments and can be crucial for compliance requirements. However, this approach can lead to image proliferation and doesn't inherently track the relationship between images across environments. You might need additional tooling to know that prod-20250302 and dev-20250301 contain the same code. 5. Latest Tag \u00b6 Format : image:latest Example : myapp:latest Pros: \u00b6 Simple to use Always points to newest version Good for development Cons: \u00b6 Unreliable for production Can lead to inconsistent deployments Hard to track actual version deployed Detailed Explanation: \u00b6 The 'latest' tag is a special convention in Docker that typically points to the most recent version of an image. While simple and convenient, especially during development, it's considered an anti-pattern for production use. The main issue is its mutability - the 'latest' tag can point to different images at different times, making it impossible to guarantee consistency across deployments. For example, if two developers pull 'latest' at different times, they might get different versions. This can lead to the \"it works on my machine\" problem and make debugging extremely difficult. Latest tags are best reserved for development environments or automated testing where having the newest version is more important than version stability. Best Practices \u00b6 For CI/CD Pipeline: \u00b6 Always use immutable tags Include build metadata (e.g., myapp:1.2.3-a1b2c3d ) Implement automated version bumping Use multi-stage builds to reduce image size For Production: \u00b6 Never use :latest tag Always specify exact version Implement image scanning Keep image history for rollbacks For ArgoCD: \u00b6 Use specific versions in manifests Implement automatic image updater Configure image pull policies Set up proper RBAC for image repositories Recommended Strategy \u00b6 For a robust production setup, combine multiple approaches: myapp:1.2.3-a1b2c3d-20250302 \u2502 \u2502 \u2502 \u2514\u2500 Build timestamp \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Git SHA (short) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Semantic version \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Image name This provides: - Clear version tracking - Easy rollbacks - Build traceability - Deployment history Combined Strategy Explanation: \u00b6 Many organizations adopt a hybrid approach that combines multiple versioning strategies to get the best of each. A common pattern is to tag each image with both a semantic version and a Git SHA (e.g., myapp:1.2.3-a1b2c3d). This provides both human-readable version information and precise code traceability. Some teams also add build metadata like timestamps or CI build numbers. While this approach provides comprehensive information, it requires more sophisticated build and deployment automation to manage the multiple tags correctly. Automation Tips \u00b6 Use GitHub Actions to automatically: Generate versions based on conventional commits Tag Docker images Push to registry Update deployment manifests Use ArgoCD to: Monitor image repositories Auto-sync new versions Maintain deployment history Enable easy rollbacks Key Stakeholder Considerations \u00b6 The key to successful Docker image versioning is choosing a strategy that balances the needs of different stakeholders: - Developers need to quickly identify and debug issues - Operations teams need stable, traceable deployments - Release managers need clear version progression - Security teams need audit capabilities - End users need clear upgrade paths","title":"Versioning Guide"},{"location":"versioning/#1-git-sha-commit-hash","text":"Format : image:git-sha Example : myapp:a1b2c3d","title":"1. Git SHA (Commit Hash)"},{"location":"versioning/#pros","text":"Precise tracking to source code Immutable and unique Easy to debug and rollback Perfect for development environments","title":"Pros:"},{"location":"versioning/#cons","text":"Not human-readable Difficult to determine version order No immediate indication of stability level","title":"Cons:"},{"location":"versioning/#detailed-explanation","text":"Git SHA versioning uses the unique hash identifier that Git generates for each commit in your repository. When you build a Docker image using this approach, you take the first few characters (usually 7-8) of the commit hash and use it as your image tag. This method creates an unbreakable link between your source code and the Docker image, making it extremely useful for debugging and traceability. For instance, if you discover an issue in production, you can immediately identify the exact code commit that produced that image. However, these hashes are not human-friendly - you can't tell at a glance which version came first or what changes it contains. This makes it less ideal for release management but perfect for development and testing environments where precise code tracking is crucial.","title":"Detailed Explanation:"},{"location":"versioning/#2-semantic-versioning","text":"Format : image:MAJOR.MINOR.PATCH Example : myapp:1.2.3","title":"2. Semantic Versioning"},{"location":"versioning/#pros_1","text":"Clear indication of change magnitude Well understood by developers Good for stable releases Easy to automate with conventional commits","title":"Pros:"},{"location":"versioning/#cons_1","text":"Can be subjective (what constitutes a breaking change?) Multiple tags might point to same image","title":"Cons:"},{"location":"versioning/#detailed-explanation_1","text":"Semantic Versioning follows a structured numbering system with three components: MAJOR.MINOR.PATCH. Each component has a specific meaning - MAJOR versions indicate breaking changes that might require users to modify their code, MINOR versions add new features while maintaining backward compatibility, and PATCH versions represent bug fixes. This system is particularly valuable when your Docker image contains an application or service that other systems depend on. Users can quickly understand the impact of upgrading to a new version. For example, if you're currently using version 1.2.3 and see version 1.2.4, you know it's safe to upgrade since it's just a patch. However, if you see version 2.0.0, you know to carefully review the changes as it contains breaking changes. The main challenge with SemVer is maintaining discipline in version number assignment - teams need to consistently agree on what constitutes a breaking change versus a minor feature addition.","title":"Detailed Explanation:"},{"location":"versioning/#3-git-tag-based","text":"Format : image:v1.2.3 Example : myapp:v1.2.3","title":"3. Git Tag Based"},{"location":"versioning/#pros_2","text":"Direct correlation with Git releases Good for release automation Clear release history","title":"Pros:"},{"location":"versioning/#cons_2","text":"Requires disciplined tag management May need additional CI/CD configuration Can be confusing with multiple release branches","title":"Cons:"},{"location":"versioning/#detailed-explanation_2","text":"Git tag based versioning aligns your Docker image versions with your Git repository's release tags. This approach creates a natural workflow where creating a Git tag automatically triggers a new Docker image build with the same version. It's particularly powerful when combined with semantic versioning - for example, tagging a release as v1.2.3 in Git automatically produces a Docker image tagged 1.2.3. This method works exceptionally well with automated release processes and provides clear documentation of your release history. The challenge comes when managing multiple release branches or when hotfixes need to be applied to older versions. You need robust processes to handle these scenarios and ensure tags are created consistently across branches.","title":"Detailed Explanation:"},{"location":"versioning/#4-environment-based","text":"Format : image:env-timestamp Example : myapp:prod-20250302","title":"4. Environment Based"},{"location":"versioning/#pros_3","text":"Clear deployment target Easy to track when image was built Good for environment-specific configurations","title":"Pros:"},{"location":"versioning/#cons_3","text":"Less precise source tracking Potential confusion with multiple deployments per day Additional storage overhead","title":"Cons:"},{"location":"versioning/#detailed-explanation_3","text":"Environment based versioning adds context about where and when an image is intended to be used. This approach often combines an environment identifier with a timestamp or build number, such as prod-20250302 or staging-build123. This strategy is particularly useful in organizations with complex deployment pipelines involving multiple environments (development, staging, QA, production). It makes it immediately clear which images are approved for which environments and when they were built. The timestamp component helps track the age of deployments and can be crucial for compliance requirements. However, this approach can lead to image proliferation and doesn't inherently track the relationship between images across environments. You might need additional tooling to know that prod-20250302 and dev-20250301 contain the same code.","title":"Detailed Explanation:"},{"location":"versioning/#5-latest-tag","text":"Format : image:latest Example : myapp:latest","title":"5. Latest Tag"},{"location":"versioning/#pros_4","text":"Simple to use Always points to newest version Good for development","title":"Pros:"},{"location":"versioning/#cons_4","text":"Unreliable for production Can lead to inconsistent deployments Hard to track actual version deployed","title":"Cons:"},{"location":"versioning/#detailed-explanation_4","text":"The 'latest' tag is a special convention in Docker that typically points to the most recent version of an image. While simple and convenient, especially during development, it's considered an anti-pattern for production use. The main issue is its mutability - the 'latest' tag can point to different images at different times, making it impossible to guarantee consistency across deployments. For example, if two developers pull 'latest' at different times, they might get different versions. This can lead to the \"it works on my machine\" problem and make debugging extremely difficult. Latest tags are best reserved for development environments or automated testing where having the newest version is more important than version stability.","title":"Detailed Explanation:"},{"location":"versioning/#best-practices","text":"","title":"Best Practices"},{"location":"versioning/#for-cicd-pipeline","text":"Always use immutable tags Include build metadata (e.g., myapp:1.2.3-a1b2c3d ) Implement automated version bumping Use multi-stage builds to reduce image size","title":"For CI/CD Pipeline:"},{"location":"versioning/#for-production","text":"Never use :latest tag Always specify exact version Implement image scanning Keep image history for rollbacks","title":"For Production:"},{"location":"versioning/#for-argocd","text":"Use specific versions in manifests Implement automatic image updater Configure image pull policies Set up proper RBAC for image repositories","title":"For ArgoCD:"},{"location":"versioning/#recommended-strategy","text":"For a robust production setup, combine multiple approaches: myapp:1.2.3-a1b2c3d-20250302 \u2502 \u2502 \u2502 \u2514\u2500 Build timestamp \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Git SHA (short) \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Semantic version \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Image name This provides: - Clear version tracking - Easy rollbacks - Build traceability - Deployment history","title":"Recommended Strategy"},{"location":"versioning/#combined-strategy-explanation","text":"Many organizations adopt a hybrid approach that combines multiple versioning strategies to get the best of each. A common pattern is to tag each image with both a semantic version and a Git SHA (e.g., myapp:1.2.3-a1b2c3d). This provides both human-readable version information and precise code traceability. Some teams also add build metadata like timestamps or CI build numbers. While this approach provides comprehensive information, it requires more sophisticated build and deployment automation to manage the multiple tags correctly.","title":"Combined Strategy Explanation:"},{"location":"versioning/#automation-tips","text":"Use GitHub Actions to automatically: Generate versions based on conventional commits Tag Docker images Push to registry Update deployment manifests Use ArgoCD to: Monitor image repositories Auto-sync new versions Maintain deployment history Enable easy rollbacks","title":"Automation Tips"},{"location":"versioning/#key-stakeholder-considerations","text":"The key to successful Docker image versioning is choosing a strategy that balances the needs of different stakeholders: - Developers need to quickly identify and debug issues - Operations teams need stable, traceable deployments - Release managers need clear version progression - Security teams need audit capabilities - End users need clear upgrade paths","title":"Key Stakeholder Considerations"},{"location":"documentation/","text":"Documentation \u00b6 Welcome to the documentation section! Here you'll find detailed guides and tutorials on various DevOps topics. Available Guides \u00b6 Markdown Guide : Learn markdown syntax for better documentation Versioning Guide : Understand different versioning strategies and best practices More guides coming soon!","title":"Overview"},{"location":"documentation/#documentation","text":"Welcome to the documentation section! Here you'll find detailed guides and tutorials on various DevOps topics.","title":"Documentation"},{"location":"documentation/#available-guides","text":"Markdown Guide : Learn markdown syntax for better documentation Versioning Guide : Understand different versioning strategies and best practices More guides coming soon!","title":"Available Guides"},{"location":"documentation/cicd/","text":"CI/CD Pipelines \u00b6 Coming soon! This section will contain guides and tutorials about Continuous Integration and Continuous Deployment.","title":"Overview"},{"location":"documentation/cicd/#cicd-pipelines","text":"Coming soon! This section will contain guides and tutorials about Continuous Integration and Continuous Deployment.","title":"CI/CD Pipelines"},{"location":"documentation/infrastructure/","text":"Infrastructure \u00b6 Coming soon! This section will contain guides and tutorials about infrastructure management and cloud services.","title":"Overview"},{"location":"documentation/infrastructure/#infrastructure","text":"Coming soon! This section will contain guides and tutorials about infrastructure management and cloud services.","title":"Infrastructure"},{"location":"documentation/linux/","text":"Linux Administration \u00b6 Coming soon! This section will contain guides and tutorials about Linux system administration and best practices.","title":"Linux Administration"},{"location":"documentation/linux/#linux-administration","text":"Coming soon! This section will contain guides and tutorials about Linux system administration and best practices.","title":"Linux Administration"},{"location":"documentation/miscellaneous/","text":"Miscellaneous \u00b6 Coming soon! This section will contain various other DevOps-related guides and tutorials that don't fit into other categories.","title":"Overview"},{"location":"documentation/miscellaneous/#miscellaneous","text":"Coming soon! This section will contain various other DevOps-related guides and tutorials that don't fit into other categories.","title":"Miscellaneous"},{"location":"documentation/networking/","text":"Networking \u00b6 Coming soon! This section will contain guides and tutorials about networking concepts and configurations.","title":"Overview"},{"location":"documentation/networking/#networking","text":"Coming soon! This section will contain guides and tutorials about networking concepts and configurations.","title":"Networking"},{"location":"documentation/selfhosting/","text":"Self-Hosting Guide \u00b6 Coming soon! This section will contain guides and tutorials about self-hosting various services and applications.","title":"Overview"},{"location":"documentation/selfhosting/#self-hosting-guide","text":"Coming soon! This section will contain guides and tutorials about self-hosting various services and applications.","title":"Self-Hosting Guide"}]}